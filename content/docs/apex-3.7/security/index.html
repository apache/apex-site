<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  
  <title>Security - Apache Apex Documentation</title>
  

  <link rel="shortcut icon" href="../favicon.ico">
  

  
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../css/highlight.css">

  
  <script>
    // Current page data
    var mkdocs_page_name = "Security";
    var mkdocs_page_input_path = "security.md";
    var mkdocs_page_url = "/security/";
  </script>
  
  <script src="../js/jquery-2.1.1.min.js"></script>
  <script src="../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../js/highlight.pack.js"></script>
  <script src="../js/theme.js"></script> 

  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> Apache Apex Documentation</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <ul class="current">
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="..">Apache Apex</a>
        
    </li>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Development</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../apex_development_setup/">Development Setup</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../application_development/">Applications</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../application_packages/">Packages</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../operator_development/">Operators</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../autometrics/">AutoMetric API</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../control_tuples/">Custom Control Tuples</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../development_best_practices/">Best Practices</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Operations</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../apex_cli/">Apex CLI</a>
        
    </li>

        
            
    <li class="toctree-l1 current">
        <a class="current" href="./">Security</a>
        
            <ul>
            
                <li class="toctree-l3"><a href="#security">Security</a></li>
                
            
                <li class="toctree-l3"><a href="#authentication">Authentication</a></li>
                
                    <li><a class="toctree-l4" href="#kerberos-authentication">Kerberos Authentication</a></li>
                
                    <li><a class="toctree-l4" href="#configuring-kerberos">Configuring Kerberos</a></li>
                
                    <li><a class="toctree-l4" href="#security-architecture">Security architecture</a></li>
                
            
                <li class="toctree-l3"><a href="#ssl-configuration">SSL Configuration</a></li>
                
                    <li><a class="toctree-l4" href="#approach-1-using-default-hadoop-yarn-ssl">Approach 1: Using Default Hadoop-YARN SSL</a></li>
                
                    <li><a class="toctree-l4" href="#approach-2-pre-installing-ssl-files-on-hadoop-cluster-nodes">Approach 2: Pre-installing SSL Files on Hadoop Cluster Nodes</a></li>
                
                    <li><a class="toctree-l4" href="#approach-3-using-apex-cli-to-copy-the-keystore">Approach 3: Using Apex CLI to Copy the Keystore</a></li>
                
                    <li><a class="toctree-l4" href="#updating-trust-store-for-the-app-proxy">Updating Trust-store for the App Proxy</a></li>
                
                    <li><a class="toctree-l4" href="#dependencies">Dependencies</a></li>
                
            
            </ul>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../compatibility/">Compatibility</a>
        
    </li>
<li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">Apache Apex Documentation</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
        
          <li>Operations &raquo;</li>
        
      
    
    <li>Security</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="security">Security</h1>
<p>Applications built on Apex run as native YARN applications on Hadoop. The security framework and apparatus in Hadoop apply to the applications. Both authentication and SSL aspects of the security framework are covered here.</p>
<h1 id="authentication">Authentication</h1>
<p>The default authentication mechanism in Hadoop is Kerberos.</p>
<h2 id="kerberos-authentication">Kerberos Authentication</h2>
<p>Kerberos is a ticket based authentication system that provides authentication in a distributed environment where authentication is needed between multiple users, hosts and services. It is the de-facto authentication mechanism supported in Hadoop. To use Kerberos authentication, the Hadoop installation must first be configured for secure mode with Kerberos. Please refer to the administration guide of your Hadoop distribution on how to do that. Once Hadoop is configured, some configuration is needed on the Apex side as well.</p>
<h2 id="configuring-kerberos">Configuring Kerberos</h2>
<p>The Apex command line interface (CLI) program, <code>apex</code>, is used to launch applications on the Hadoop cluster along with performing various other operations and administrative tasks on the applications. In a secure cluster additional configuration is needed for the CLI program <code>apex</code>.</p>
<h3 id="cli-configuration">CLI Configuration</h3>
<p>When Kerberos security is enabled in Hadoop, a Kerberos ticket granting ticket (TGT) or the Kerberos credentials of the user are needed by the CLI program <code>apex</code> to authenticate with Hadoop for any operation. Kerberos credentials are composed of a principal and either a <em>keytab</em> or a password. For security and operational reasons only keytabs are supported in Hadoop and by extension in Apex platform. When user credentials are specified, all operations including launching application are performed as that user.</p>
<h4 id="using-kinit">Using kinit</h4>
<p>A Kerberos ticket granting ticket (TGT) can be obtained by using the Kerberos command <code>kinit</code>. Detailed documentation for the command can be found online or in man pages. An sample usage of this command is</p>
<pre><code>kinit -k -t path-tokeytab-file kerberos-principal
</code></pre>
<p>If this command is successful, the TGT is obtained, cached and available for other programs. The CLI program <code>apex</code> can then be started to launch applications and perform other operations.</p>
<h4 id="using-kerberos-credentials">Using Kerberos credentials</h4>
<p>The CLI program <code>apex</code> can also use the Kerberos credentials directly without requiring a TGT to be obtained separately. This can be useful in batch mode where <code>apex</code> is not launched manually and also in scenarios where running another program like <code>kinit</code> is not feasible.</p>
<p>The credentials can be specified in the <code>dt-site.xml</code> configuration file. If only a single user is launching applications, the global <code>dt-site.xml</code> configuration file in the installation folder can be used. In a multi-user environment the users can use the <code>dt-site.xml</code> file in their
home directory. The location of this file will be <code>$HOME/.dt/dt-site.xml</code>. If this file does not exist, the user can create a new one.</p>
<p>The snippet below shows the how the credentials can be specified in the configuration file as properties.</p>
<pre><code class="xml">&lt;property&gt;
        &lt;name&gt;dt.authentication.principal&lt;/name&gt;
        &lt;value&gt;kerberos-principal-of-user&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
        &lt;name&gt;dt.authentication.keytab&lt;/name&gt;
        &lt;value&gt;absolute-path-to-keytab-file&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<p>The property <code>dt.authentication.principal</code> specifies the Kerberos user principal and <code>dt.authentication.keytab</code> specifies the absolute path to the keytab file for the user.</p>
<h3 id="web-services-security">Web Services security</h3>
<p>Alongside every Apex application, there is an application master process called Streaming Container Manager (STRAM) running. STRAM manages the application by handling the various control aspects of the application such as orchestrating the execution of the application on the cluster, playing a key role in scalability and fault tolerance, providing application insight by collecting statistics among other functionality.</p>
<p>STRAM provides a web service interface to introspect the state of the application and its various components and to make dynamic changes to the applications. Some examples of supported functionality are getting resource usage and partition information of various operators, getting operator statistics and changing properties of running operators.</p>
<p>Access to the web services can be secured to prevent unauthorized access. By default it is automatically enabled in Hadoop secure mode environments and not enabled in non-secure environments. How the security actually works is described in <code>Security architecture</code> section below.</p>
<p>There are additional options available for finer grained control on enabling it. This can be configured on a per-application basis using an application attribute. It can also be enabled or disabled based on Hadoop security configuration. The following security options are available</p>
<ul>
<li>Enable - Enable Authentication</li>
<li>Follow Hadoop Authentication - Enable authentication if secure mode is enabled in Hadoop, the default</li>
<li>Follow Hadoop HTTP Authentication - Enable authentication only if HTTP authentication is enabled in Hadoop and not just secure mode.</li>
<li>Disable - Disable Authentication</li>
</ul>
<p>To specify the security option for an application the following configuration can be specified in the <code>dt-site.xml</code> file</p>
<pre><code class="xml">&lt;property&gt;
        &lt;name&gt;dt.application.name.attr.STRAM_HTTP_AUTHENTICATION&lt;/name&gt;
        &lt;value&gt;security-option&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<p>The security option value can be <code>ENABLED</code>, <code>FOLLOW_HADOOP_AUTH</code>, <code>FOLLOW_HADOOP_HTTP_AUTH</code> or <code>DISABLE</code> for the four options above respectively.</p>
<p>The subsequent sections talk about how security works in Apex. This information is not needed by users but is intended for the inquisitive techical audience who want to know how security works.</p>
<h4 id="cli-setup">CLI setup</h4>
<p>The CLI program <code>apex</code> connects to the web service endpoint of the STRAM for a running application to query for information or to make changes to it. In order to do that, it has to first connect to the YARN proxy web service and get the necessary connection information and credentials to connect to STRAM. The proxy web service may have security enabled and in that case, the CLI program <code>apex</code> would first need to authenticate with the service before it can get any information.</p>
<p>Hadoop allows a lot of flexibility in the kind of security to use for the proxy. It allows the user to plug-in their own authentication provider. The authentication provider is specified as a JAVA class name. It also comes bundled with a provider for Kerberos SPNEGO authentication. Some distributions also include a provider for BASIC authentication via SASL.</p>
<p>The CLI <code>apex</code>, has built-in functionality for Kerberos SPNEGO, BASIC and DIGEST authentication mechanisms. Because of the way the authentication provider is configured for the proxy on the Hadoop side, there is no reliable way to determine before hand what kind of authentication is being used. Only at runtime, when the CLI connects to the proxy web service will it know the type of authentication that the service is using. For this reason, <code>apex</code> allows the user to configure credentials for multiple authentication mechanisms it supports and will pick the one that matches what the service expects.</p>
<p>If the authentication mechanism is Kerberos SPNEGO, the properties listed in the <a href="#using-kerberos-credentials">Using Kerberos credentials</a> section for general communication with Hadoop above are sufficient. No additional properties are needed.</p>
<p>For BASIC authentication, the credentials can be specified using the following properties</p>
<pre><code class="xml">&lt;property&gt;
        &lt;name&gt;dt.authentication.basic.username&lt;/name&gt;
        &lt;value&gt;username&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
        &lt;name&gt;dt.authentication.basic.password&lt;/name&gt;
        &lt;value&gt;password&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<p>For DIGEST authentication, the credentials can be specified using the following properties</p>
<pre><code class="xml">&lt;property&gt;
        &lt;name&gt;dt.authentication.digest.username&lt;/name&gt;
        &lt;value&gt;username&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
        &lt;name&gt;dt.authentication.digest.password&lt;/name&gt;
        &lt;value&gt;password&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<h3 id="token-refresh">Token Refresh</h3>
<p>Apex applications, at runtime, use delegation tokens to authenticate with Hadoop services when communicating with them as described in the security architecture section below. The delegation tokens are originally issued by these Hadoop services and have an expiry time period which is typically 7 days. The tokens become invalid beyond this time and the applications will no longer be able to communicate with the Hadoop services. For long running applications this presents a problem.</p>
<p>To solve this problem one of the two approaches can be used. The first approach is to change the Hadoop configuration itself to extend the token expiry time period. This may not be possible in all environments as it requires a change in the security policy as the tokens will now be valid for a longer period of time and the change also requires administrator privileges to Hadoop. The second approach is to use a feature available in apex to auto-refresh the tokens before they expire. Both the approaches are detailed below and the users can choose the one that works best for them.</p>
<h4 id="hadoop-configuration-approach">Hadoop configuration approach</h4>
<p>An Apex application uses delegation tokens to authenticate with Hadoop services, Resource Manager (YARN) and Name Node (HDFS), and these tokens are issued by those services respectively. Since the application is long-running, the tokens can expire while the application is still running. Hadoop uses configuration settings for the maximum lifetime of these tokens. </p>
<p>There are separate settings for ResourceManager and NameNode delegation tokens. In this approach the user increases the values of these settings to cover the lifetime of the application. Once these settings are changed, the YARN and HDFS services would have to be restarted. The values in these settings are of type <code>long</code> and has an upper limit so applications cannot run forever. This limitation is not present with the next approach described below.</p>
<p>The Resource Manager delegation token max lifetime is specified in <code>yarn-site.xml</code> and can be specified as follows for a lifetime of 1 year as an example</p>
<pre><code class="xml">&lt;property&gt;
  &lt;name&gt;yarn.resourcemanager.delegation.token.max-lifetime&lt;/name&gt;
  &lt;value&gt;31536000000&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<p>The Name Node delegation token max lifetime is specified in
hdfs-site.xml and can be specified as follows for a lifetime of 1 year as an example</p>
<pre><code class="xml">&lt;property&gt;
   &lt;name&gt;dfs.namenode.delegation.token.max-lifetime&lt;/name&gt;
   &lt;value&gt;31536000000&lt;/value&gt;
 &lt;/property&gt;
</code></pre>

<h4 id="auto-refresh-approach">Auto-refresh approach</h4>
<p>In this approach the application, in anticipation of a token expiring, obtains a new token to replace the current one. It keeps repeating the process whenever a token is close to expiry so that the application can continue to run indefinitely.</p>
<p>This requires the application having access to a keytab file at runtime because obtaining a new token requires a keytab. The keytab file should be present in HDFS so that the application can access it at runtime. The user can provide a HDFS location for the keytab file using a setting otherwise the keytab file specified for the <code>apex</code> CLI program above will be copied from the local filesystem into HDFS before the application is started and made available to the application. There are other optional settings available to configure the behavior of this feature. All the settings are described below.</p>
<p>The location of the keytab can be specified by using the following setting in <code>dt-site.xml</code>. If it is not specified then the file specified in <code>dt.authentication.keytab</code> is copied into HDFS and used.</p>
<pre><code class="xml">&lt;property&gt;
        &lt;name&gt;dt.authentication.store.keytab&lt;/name&gt;
        &lt;value&gt;hdfs-path-to-keytab-file&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<p>The expiry period of the Resource Manager and Name Node tokens needs to be known so that the application can renew them before they expire. These are automatically obtained using the <code>yarn.resourcemanager.delegation.token.max-lifetime</code> and <code>dfs.namenode.delegation.token.max-lifetime</code> properties from the hadoop configuration files. Sometimes however these properties are not available or kept up-to-date on the nodes running the applications. If that is the case then the following properties can be used to specify the expiry period, the values are in milliseconds. The example below shows how to specify these with values of 7 days.</p>
<pre><code class="xml">&lt;property&gt;
        &lt;name&gt;dt.resourcemanager.delegation.token.max-lifetime&lt;/name&gt;
        &lt;value&gt;604800000&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
        &lt;name&gt;dt.namenode.delegation.token.max-lifetime&lt;/name&gt;
        &lt;value&gt;604800000&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<p>As explained earlier new tokens are obtained before the old ones expire. How early the new tokens are obtained before expiry is controlled by a setting. This setting is specified as a factor of the token expiration with a value between 0.0 and 1.0. The default value is <code>0.7</code>. This factor is multipled with the expiration time to determine when to refresh the tokens. This setting can be changed by the user and the following example shows how this can be done</p>
<pre><code class="xml">&lt;property&gt;
        &lt;name&gt;dt.authentication.token.refresh.factor&lt;/name&gt;
        &lt;value&gt;0.7&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<h3 id="impersonation">Impersonation</h3>
<p>The CLI program <code>apex</code> supports Hadoop proxy user impersonation, in allowing applications to be launched and other operations to be performed as a different user than the one specified by the Kerberos credentials. The Kerberos credentials are still used for authentication. This is useful in scenarios where a system using <code>apex</code> has to support multiple users but only has a single set of Kerberos credentials, those of a system user.</p>
<h4 id="usage">Usage</h4>
<p>To use this feature, the following environment variable should be set to the user name of the user being impersonated, before running <code>apex</code> and the operations will be performed as that user. For example, if launching an application, the application will run as the specified user and not as the user specified by the Kerberos credentials.</p>
<pre><code>HADOOP_USER_NAME=&lt;username&gt;
</code></pre>

<h4 id="hadoop-configuration">Hadoop Configuration</h4>
<p>For this feature to work, additional configuration settings are needed in Hadoop. These settings would allow a specified user, such as a system user, to impersonate other users. The example snippet below shows these settings. In this example, the specified user can impersonate users belonging to any group and can do so running from any host. Note that the user specified here is different from the user specified above in usage, there it is the user that is being impersonated and here it is the impersonating user such as a system user.</p>
<pre><code class="xml">&lt;property&gt;
  &lt;name&gt;hadoop.proxyuser.&lt;username&gt;.groups&lt;/name&gt;
  &lt;value&gt;*&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;hadoop.proxyuser.&lt;username&gt;.hosts&lt;/name&gt;
  &lt;value&gt;*&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<h4 id="application-root-directory-under-hdfs">Application Root Directory under HDFS</h4>
<p>A running Apex application uses a 'root' directory under HDFS where runtime artifacts are saved or read from. For example, with default configuration an Apex application 
would use the HDFS path <code>/user/dtadmin/datatorrent</code> as the application root directory. If the application's Hadoop assigned application ID is <code>application_1487803614053_10222</code> 
then the full HDFS path for saving and reading application package contents,  events, checkpoint and recovery data is 
<code>/user/dtadmin/datatorrent/apps/application_1487803614053_10222</code>.</p>
<p>This root directory determination involves using the 'current' user, and in case of impersonation Apex treats either the impersonating user or the impersonated 
user as the 'current' user depending on the value of the configuration propery <code>dt.authentication.impersonation.path.enable</code>. You should set its value to <code>true</code> to 
treat the impersonated user as the current user, or <code>false</code> (the default value) to treat the impersonating user as the current user.</p>
<p>Apex also uses the configuration property <code>apex.app.dfsRootDirectory</code> to determine the application root directory when <code>dt.authentication.impersonation.path.enable</code>
is set to <code>true</code> i.e. treating the impersonated user as the 'current' user. When <code>dt.authentication.impersonation.path.enable</code> is set to <code>false</code>, Apex will use
the configuration property <code>dt.dfsRootDirectory</code> to determine the application root directory.</p>
<p>As an example, let's assume the following values:</p>
<ul>
<li>
<p><code>apex.app.dfsRootDirectory</code> is set to <code>/user/%USER_NAME%/apex</code>.</p>
</li>
<li>
<p><code>dt.dfsRootDirectory</code> is set to <code>/user/%USER_NAME%/datatorrent</code>.</p>
</li>
</ul>
<p>Also assume the impersonating user is <code>apexadmin</code> and the impersonated user is <code>peter</code> when the application is launched. In this scenario, when
<code>dt.authentication.impersonation.path.enable</code> is set to <code>true</code>, the application root directory is <code>/user/peter/apex</code>. When it is set to <code>false</code>, the application
root directory is <code>/user/apexadmin/datatorrent</code>. As you can see, Apex replaces the string <code>%USER_NAME%</code> with the 'current' user as described above. If you do not use
an absolute path (i.e. path starting with <code>/</code>) as the value of <code>apex.app.dfsRootDirectory</code> then the evaluated path is prepended with the current user's home directory.
For example, when the value of <code>apex.app.dfsRootDirectory</code> is set to <code>apex/%USER_NAME%/myDir</code> and <code>dt.authentication.impersonation.path.enable</code> is set to <code>true</code> then
the application root directory in the above example is <code>/user/peter/apex/peter/myDir</code>.</p>
<p>All of the configuration properties mentioned above can be set using any one of the methods mentioned <a href="../application_packages/#adding-configuration-properties">here</a>.
For example, you can use the <code>dt-site.xml</code> file, or use the <code>-D</code> option in the Apex CLI launch command as described <a href="../application_packages/#launching-an-application-package">here</a>.</p>
<h2 id="security-architecture">Security architecture</h2>
<p>In this section we will see how security works for applications built on Apex. We will look at the different methodologies involved in running the applications and in each case we will look into the different components that are involved. We will go into the architecture of these components and look at the different security mechanisms that are in play.</p>
<h3 id="application-launch">Application Launch</h3>
<p>To launch applications in Apache Apex the command line client <code>apex</code> can be used. The application artifacts such as binaries and properties are supplied as an application package. The client, during the various steps involved to launch the application needs to communicate with both the Resource Manager and the Name Node. The Resource Manager communication involves the client asking for new resources to run the application master and start the application launch process. The steps along with sample Java code are described in Writing YARN Applications. The Name Node communication includes the application artifacts being copied to HDFS so that they are available across the cluster for launching the different application containers.</p>
<p>In secure mode, the communications with both Resource Manager and Name Node requires authentication and the mechanism is Kerberos. Below is an illustration showing this.</p>
<p><img alt="" src="../images/security/image02.png" /></p>
<p>The client <code>apex</code> supports Kerberos authentication and will automatically enable it in a secure environment. To authenticate, some Kerberos configuration namely the Kerberos credentials, are needed by the client. There are two parameters, the Kerberos principal and keytab to use for the client. These can be specified in the dt-site.xml configuration file. The properties are shown below</p>
<pre><code>    &lt;property&gt;
            &lt;name&gt;dt.authentication.principal&lt;/name&gt;
            &lt;value&gt;kerberos-principal-of-user&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
            &lt;name&gt;dt.authentication.keytab&lt;/name&gt;
            &lt;value&gt;absolute-path-to-keytab-file&lt;/value&gt;
    &lt;/property&gt;
</code></pre>
<p>Refer to document Operation and Installation Guide section Multi Tenancy and Security subsection CLI Configuration in the documentation for more information. The document can also be accessed here client configuration</p>
<p>There is another important functionality that is performed by the client and that is to retrieve what are called delegation tokens from the Resource Manager and Name Node to seed the application master container that is to be launched. This is detailed in the next section. </p>
<h3 id="runtime-security">Runtime Security</h3>
<p>When the application is completely up and running, there are different components of the application running as separate processes possibly on different nodes in the cluster as it is a distributed application. These components interactwould be interacting with each other and the Hadoop services. In secure mode, all these interactions have to be authenticated before they can be successfully processed. The interactions are illustrated below in a diagram to give a complete overview. Each of them is explained in subsequent sections.</p>
<p><img alt="" src="../images/security/image00.png" /></p>
<h4 id="stram-and-hadoop">STRAM and Hadoop</h4>
<p>Every Apache Apex application has a master process akin to any YARN application. In our case it is called STRAM (Streaming Application Master). It is a master process that runs in its own container and manages the different distributed components of the application. Among other tasks it requests Resource Manager for new resources as they are needed and gives back resources that are no longer needed. STRAM also needs to communicate with Name Node from time-to-time to access the persistent HDFS file system. </p>
<p>In secure mode, STRAM has to authenticate with both Resource Manager and Name Node before it can send any requests and this is achieved using Delegation Tokens. Since STRAM runs as a managed application master, it runs in a Hadoop container. This container could have been allocated on any node based on what resources were available. Since there is no fixed node where STRAM runs, it does not have Kerberos credentials. Unlike launch client <code>apex</code>, it cannot authenticate with Hadoop services Resource Manager and Name Node using Kerberos. Instead, Delegation Tokens are used for authentication.</p>
<h5 id="delegation-tokens">Delegation Tokens</h5>
<p>Delegation tokens are tokens that are dynamically issued by the source and clients use them to authenticate with the source. The source stores the delegation tokens it has issued in a cache and checks the delegation token sent by a client against the cache. If a match is found, the authentication is successful else it fails. This is the second mode of authentication in secure Hadoop after Kerberos. More details can be found in the Hadoop security design document. In this case the delegation tokens are issued by Resource Manager and Name Node. STRAM would use these tokens to authenticate with them. But how does it get them in the first place? This is where the launch client <code>apex</code> comes in.</p>
<p>The client <code>apex</code>, since it possesses Kerberos credentials as explained in the Application Launch section, is able to authenticate with Resource Manager and Name Node using Kerberos. It then requests for delegation tokens over the Kerberos authenticated connection. The servers return the delegation tokens in the response payload. The client in requesting the resource manager for the start of the application master container for STRAM seeds it with these tokens so that when STRAM starts it has these tokens. It can then use these tokens to authenticate with the Hadoop services.</p>
<h4 id="streaming-container">Streaming Container</h4>
<p>A streaming container is a process that runs a part of the application business logic. It is a container deployed on a node in the cluster. The part of business logic is implemented in what we call an operator. Multiple operators connected together make up the complete application and hence there are multiple streaming containers in an application. The streaming containers have different types of communications going on as illustrated in the diagram above. They are described below.</p>
<h5 id="stram-delegation-token">STRAM Delegation Token</h5>
<p>The streaming containers periodically communicate with the application master STRAM. In the communication they send what are called heartbeats with information such as statistics and receive commands from STRAM such as deployment or un-deployment of operators, changing properties of operators etc. In secure mode, this communication cannot just occur without any authentication. To facilitate this authentication special tokens called STRAM Delegation Tokens are used. These tokens are created and managed by STRAM. When a new streaming container is being started, since STRAM is the one negotiating resources from Resource Manager for the container and requesting to start the container, it seeds the container with the STRAM delegation token necessary to communicate with it. Thus, a streaming container has the STRAM delegation token to successfully authenticate and communicate with STRAM.</p>
<h5 id="buffer-server-token">Buffer Server Token</h5>
<p>As mentioned earlier an operator implements a piece of the business logic of the application and multiple operators together complete the application. In creating the application the operators are assembled together in a direct acyclic graph, a pipeline, with output of operators becoming the input for other operators. At runtime the stream containers hosting the operators are connected to each other and sending data to each other. In secure mode these connections should be authenticated too, more importantly than others, as they are involved in transferring application data.</p>
<p>When operators are running there will be effective processing rate differences between them due to intrinsic reasons such as operator logic or external reasons such as different resource availability of CPU, memory, network bandwidth etc. as the operators are running in different containers. To maximize performance and utilization the data flow is handled asynchronous to the regular operator function and a buffer is used to intermediately store the data that is being produced by the operator. This buffered data is served by a buffer server over the network connection to the downstream streaming container containing the operator that is supposed to receive the data from this operator. This connection is secured by a token called the buffer server token. These tokens are also generated and seeded by STRAM when the streaming containers are deployed and started and it uses different tokens for different buffer servers to have better security.</p>
<h5 id="namenode-delegation-token">NameNode Delegation Token</h5>
<p>Like STRAM, streaming containers also need to communicate with NameNode to use HDFS persistence for reasons such as saving the state of the operators. In secure mode they also use NameNode delegation tokens for authentication. These tokens are also seeded by STRAM for the streaming containers.</p>
<h4 id="stram-web-services">Stram Web Services</h4>
<p>Clients connect to STRAM and make web service requests to obtain operational information about running applications. When security is enabled we want this connection to also be authenticated. In this mode the client passes a web service token in the request and STRAM checks this token. If the token is valid, then the request is processed else it is denied.</p>
<p>How does the client get the web service token in the first place? The client will have to first connect to STRAM via the Resource Manager Web Services Proxy which is a service run by Hadoop to proxy requests to application web services. This connection is authenticated by the proxy service using a protocol called SPNEGO when secure mode is enabled. SPNEGO is Kerberos over HTTP and the client also needs to support it. If the authentication is successful the proxy forwards the request to STRAM. STRAM in processing the request generates and sends back a web service token similar to a delegation token. This token is then used by the client in subsequent requests it makes directly to STRAM and STRAM is able to validate it since it generated the token in the first place.</p>
<p><img alt="" src="../images/security/image03.png" /></p>
<h1 id="ssl-configuration">SSL Configuration</h1>
<p>The STRAM Web services component described above can be configured with SSL to enable HTTPS. To achieve this you need to enable SSL in YARN as described <a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SecureMode.html#Data_Encryption_on_HTTP">here</a>, specifically set <code>yarn.http.policy</code> to <code>HTTPS_ONLY</code> in <code>yarn-site.xml</code>. You also need to make keystore and other SSL material available to the Web services component for HTTPS to work. There are 3 approaches to achieve this:</p>
<h4 id="approach-1-using-default-hadoop-yarn-ssl">Approach 1: Using Default Hadoop-YARN SSL</h4>
<p>If the default Hadoop-YARN SSL material is uniformly available in the local file system of each node in your Hadoop cluster, STRAM can use it provided the STRAM process has access to the files. Use your Hadoop/YARN configuration procedures to enable SSL in YARN to use this approach, so all YARN applications including Apex applications will be SSL-enabled. However if the SSL material is not present, or not accessible to the STRAM process (for example, if STRAM is not running as <code>root</code>), then you will have to use one of the following two approaches.</p>
<h4 id="approach-2-pre-installing-ssl-files-on-hadoop-cluster-nodes">Approach 2: Pre-installing SSL Files on Hadoop Cluster Nodes</h4>
<p>With this approach, your own SSL files are pre-installed on each node in the Hadoop cluster, so the STRAM can access them regardless of the node it runs on. Each node will have 2 files: <code>ssl-server.xml</code> is the master SSL configuration file and the SSL keystore file (a JKS file) whose name and location are specified in <code>ssl-server.xml</code>. Decide on a location for each of these files and follow the steps below.</p>
<ul>
<li>
<p>Create an SSL keystore as described <a href="http://docs.oracle.com/cd/E19509-01/820-3503/ggfen/index.html">here</a>. Assume this file is named <code>myapex-keystore.jks</code> and will reside at <code>/opt/apex/keystore/</code> on each Hadoop node and the keystore password is <code>storepass1</code> and the keystore key-password is <code>keypass2</code>. </p>
</li>
<li>
<p>Create the SSL configuration file with the following content. Typically the file is called <code>ssl-server.xml</code> but you can use any other name.</p>
</li>
</ul>
<pre><code>&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;ssl.server.keystore.location&lt;/name&gt;
    &lt;value&gt;/opt/apex/keystore/myapex-keystore.jks&lt;/value&gt;
    &lt;description&gt;&lt;/description&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;ssl.server.keystore.keypassword&lt;/name&gt;
    &lt;value&gt;keypass2&lt;/value&gt;
    &lt;description&gt;&lt;/description&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;ssl.server.keystore.password&lt;/name&gt;
    &lt;value&gt;storepass1&lt;/value&gt;
    &lt;description&gt;&lt;/description&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>

<p>Let's assume this file will reside on each node as <code>/opt/apex/sslConfig/my-apex-ssl-server.xml</code> .</p>
<ul>
<li>
<p>Copy the keystore file and the SSL configuration file on each Hadoop node to their designated locations and make sure they are readable by your Apex application.</p>
</li>
<li>
<p>Using the Apex CLI, pass the SSL configuration file location to your Apex application using the SSL_CONFIG attribute as follows:</p>
</li>
</ul>
<pre><code>launch -Dapex.attr.SSL_CONFIG=&quot;{\&quot;configPath\&quot;:\&quot;/opt/apex/sslConfig/my-apex-ssl-server.xml\&quot;}&quot;  &lt;apa file&gt;
</code></pre>

<ul>
<li>Alternatively you can use configuration files to supply the value of <code>apex.attr.SSL_CONFIG</code> attribute as described <a href="http://apex.apache.org/docs/apex/application_packages/#application-packages">here</a></li>
</ul>
<h4 id="approach-3-using-apex-cli-to-copy-the-keystore">Approach 3: Using Apex CLI to Copy the Keystore</h4>
<p>If you cannot pre-install the SSL files on all your cluster nodes, you can use the SSL_CONFIG attribute in such a way that Apex CLI copies the keystore file from the client node to the server node and also passes on the keystore password values to the STRAM. The keystore file needs to be accessible on the Apex CLI machine so Apex CLI can copy it. Let's assume this location is <code>/opt/apexCli/ssl/myapex-keystore.jks</code>. Use the Apex CLI launch command as follows:</p>
<pre><code>launch -Dapex.attr.SSL_CONFIG=&quot;{\&quot;keyStorePath\&quot;:\&quot;/opt/apexCli/ssl/myapex-keystore.jks\&quot;,\&quot;keyStorePassword\&quot;:\&quot;storepass1\&quot;,\&quot;keyStoreKeyPassword\&quot;:\&quot;keypass2\&quot;}&quot;  &lt;apa file&gt;
</code></pre>

<p>Apec CLI will copy the keystore file <code>/opt/apexCli/ssl/myapex-keystore.jks</code> to the destination STRAM node and also pass on the keystore password values to the STRAM. As mentioned above, you can also use configuration files to supply the value of <code>apex.attr.SSL_CONFIG</code>.</p>
<h4 id="updating-trust-store-for-the-app-proxy">Updating Trust-store for the App Proxy</h4>
<p>You need to ensure that all end-points connecting to the STRAM Web service trust the SSL certificate when SSL is enabled.
This is especially true for the <a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/WebApplicationProxy.html">Web Application Proxy</a> which connects to the STRAM Web service whenever you access the service through the App Master Proxy HTTPS URL. 
If you use a self-signed or untrusted certificate, you will need to add that certificate to the trust-store used by the RM Web Application Proxy as described <a href="https://www.cloudera.com/documentation/enterprise/5-8-x/topics/cm_sg_create_key_trust.html#concept_u35_w2m_l4">here</a> and update <code>ssl-client.xml</code> to use the trust-store as described <a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SecureMode.html#Data_Encryption_on_HTTP">here</a></p>
<h4 id="dependencies">Dependencies</h4>
<p>The use of the attribute <code>apex.attr.SSL_CONFIG</code> described in the last 2 approaches is dependent on an <a href="https://issues.apache.org/jira/browse/YARN-6457">enhancement</a> made in YARN, which is available in the following versions: 
2.9.0, 2.7.4, 3.0.0-alpha4, 2.8.2. </p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../compatibility/" class="btn btn-neutral float-right" title="Compatibility">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../apex_cli/" class="btn btn-neutral" title="Apex CLI"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>

  </div>

<div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../apex_cli/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../compatibility/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>

</body>
</html>
