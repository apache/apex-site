{
    "docs": [
        {
            "location": "/", 
            "text": "Apache Apex\n\n\nApex is a Hadoop YARN native big data processing platform, enabling real time stream as well as batch processing for your big data.  Apex provides the following benefits:\n\n\n\n\nHigh scalability and performance\n\n\nFault tolerance and state management\n\n\nHadoop-native YARN \n HDFS implementation\n\n\nEvent processing guarantees\n\n\nSeparation of functional and operational concerns\n\n\nSimple API supports generic Java code\n\n\n\n\nPlatform has been demonstated to scale linearly across Hadoop clusters under extreme loads of billions of events per second.  Hardware and process failures are quickly recovered with HDFS-backed checkpointing and automatic operator recovery, preserving application state and resuming execution in seconds.  Functional and operational specifications are separated.  Apex provides a simple API, which enables users to write generic, reusable code.  The code is dropped in as-is and platform automatically handles the various operational concerns, such as state management, fault tolerance, scalability, security, metrics, etc.  This frees users to focus on functional development, and lets platform provide operability support.\n\n\nThe core Apex platform is supplemented by Malhar, a library of connector and logic functions, enabling rapid application development.  These operators and modules provide access to HDFS, S3, NFS, FTP, and other file systems; Kafka, ActiveMQ, RabbitMQ, JMS, and other message systems; MySql, Cassandra, MongoDB, Redis, HBase, CouchDB, generic JDBC, and other database connectors.  In addition to the operators, the library contains a number of demos applications, demonstrating operator features and capabilities.  To see the full list of available operators and related documentation, visit \nApex Malhar on Github\n\n\nFor additional information visit \nApache Apex\n.", 
            "title": "Apache Apex"
        }, 
        {
            "location": "/#apache-apex", 
            "text": "Apex is a Hadoop YARN native big data processing platform, enabling real time stream as well as batch processing for your big data.  Apex provides the following benefits:   High scalability and performance  Fault tolerance and state management  Hadoop-native YARN   HDFS implementation  Event processing guarantees  Separation of functional and operational concerns  Simple API supports generic Java code   Platform has been demonstated to scale linearly across Hadoop clusters under extreme loads of billions of events per second.  Hardware and process failures are quickly recovered with HDFS-backed checkpointing and automatic operator recovery, preserving application state and resuming execution in seconds.  Functional and operational specifications are separated.  Apex provides a simple API, which enables users to write generic, reusable code.  The code is dropped in as-is and platform automatically handles the various operational concerns, such as state management, fault tolerance, scalability, security, metrics, etc.  This frees users to focus on functional development, and lets platform provide operability support.  The core Apex platform is supplemented by Malhar, a library of connector and logic functions, enabling rapid application development.  These operators and modules provide access to HDFS, S3, NFS, FTP, and other file systems; Kafka, ActiveMQ, RabbitMQ, JMS, and other message systems; MySql, Cassandra, MongoDB, Redis, HBase, CouchDB, generic JDBC, and other database connectors.  In addition to the operators, the library contains a number of demos applications, demonstrating operator features and capabilities.  To see the full list of available operators and related documentation, visit  Apex Malhar on Github  For additional information visit  Apache Apex .", 
            "title": "Apache Apex"
        }, 
        {
            "location": "/apex_development_setup/", 
            "text": "Apache Apex Development Environment Setup\n\n\nThis document discusses the steps needed for setting up a development environment for creating applications that run on the Apache Apex platform.\n\n\nDevelopment Tools\n\n\nThere are a few tools that will be helpful when developing Apache Apex applications, including:\n\n\n\n\n\n\ngit\n - A revision control system (version 1.7.1 or later). There are multiple git clients available for Windows (\nhttp://git-scm.com/download/win\n for example), so download and install a client of your choice.\n\n\n\n\n\n\njava JDK\n (not JRE) - Includes the Java Runtime Environment as well as the Java compiler and a variety of tools (version 1.7.0_79 or later). Can be downloaded from the Oracle website.\n\n\n\n\n\n\nmaven\n - Apache Maven is a build system for Java projects (version 3.0.5 or later). It can be downloaded from \nhttps://maven.apache.org/download.cgi\n.\n\n\n\n\n\n\nIDE\n (Optional) - If you prefer to use an IDE (Integrated Development Environment) such as \nNetBeans\n, \nEclipse\n or \nIntelliJ\n, install that as well.\n\n\n\n\n\n\nAfter installing these tools, make sure that the directories containing the executable files are in your PATH environment variable.\n\n\n\n\nWindows\n - Open a console window and enter the command \necho %PATH%\n to see the value of the \nPATH\n variable and verify that the above directories for Java, git, and maven executables are present.  JDK executables like \njava\n and \njavac\n, the directory might be something like \nC:\\Program Files\\Java\\jdk1.7.0\\_80\\bin\n; for \ngit\n it might be \nC:\\Program Files\\Git\\bin\n; and for maven it might be \nC:\\Users\\user\\Software\\apache-maven-3.3.3\\bin\n.  If not, you can change its value clicking on the button at \nControl Panel\n \n \nAdvanced System Settings\n \n \nAdvanced tab\n \n \nEnvironment Variables\n.\n\n\nLinux and Mac\n - Open a console/terminal window and enter the command \necho $PATH\n to see the value of the \nPATH\n variable and verify that the above directories for Java, git, and maven executables are present.  If not, make sure software is downloaded and installed, and optionally PATH reference is added and exported  in a \n~/.profile\n or \n~/.bash_profile\n.  For example to add maven located in \n/sfw/maven/apache-maven-3.3.3\n to PATH add the line: \nexport PATH=$PATH:/sfw/maven/apache-maven-3.3.3/bin\n\n\n\n\nConfirm by running the following commands and comparing with output that show in the table below:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommand\n\n\nOutput\n\n\n\n\n\n\njavac -version\n\n\njavac 1.7.0_80\n\n\n\n\n\n\njava -version\n\n\njava version \n1.7.0_80\n\n\nJava(TM) SE Runtime Environment (build 1.7.0_80-b15)\n\n\nJava HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode)\n\n\n\n\n\n\ngit --version\n\n\ngit version 2.6.1.windows.1\n\n\n\n\n\n\nmvn --version\n\n\nApache Maven 3.3.3 (7994120775791599e205a5524ec3e0dfe41d4a06; 2015-04-22T06:57:37-05:00)\n\n\n...\n\n\n\n\n\n\n\n\n\n\n\nCreating New Apex Project\n\n\nAfter development tools are configured, you can now use the maven archetype to create a basic Apache Apex project.  \nNote:\n When executing the commands below, you can optionally replace \nRELEASE\n with a \nspecific version\n of Apache Apex.\n\n\n\n\n\n\nWindows\n - Create a new Windows command file called \nnewapp.cmd\n by copying the lines below, and execute it.  When you run this file, the properties will be displayed and you will be prompted with \nY: :\n; just press \nEnter\n to complete the project generation.  The caret (^) at the end of some lines indicates that a continuation line follows. \n\n\n@echo off\n@rem Script for creating a new application\nsetlocal\nmvn archetype:generate ^\n -DarchetypeGroupId=org.apache.apex ^\n -DarchetypeArtifactId=apex-app-archetype -DarchetypeVersion=RELEASE ^\n -DgroupId=com.example -Dpackage=com.example.myapexapp -DartifactId=myapexapp ^\n -Dversion=1.0-SNAPSHOT\nendlocal\n\n\n\n\n\n\n\nLinux\n - Execute the lines below in a terminal window.  New project will be created in the curent working directory.  The backslash (\\) at the end of the lines indicates continuation.\n\n\nmvn archetype:generate \\\n -DarchetypeGroupId=org.apache.apex \\\n -DarchetypeArtifactId=apex-app-archetype -DarchetypeVersion=RELEASE \\\n -DgroupId=com.example -Dpackage=com.example.myapexapp -DartifactId=myapexapp \\\n -Dversion=1.0-SNAPSHOT\n\n\n\n\n\n\n\nWhen the run completes successfully, you should see a new directory named \nmyapexapp\n containing a maven project for building a basic Apache Apex application. It includes 3 source files:\nApplication.java\n,  \nRandomNumberGenerator.java\n and \nApplicationTest.java\n. You can now build the application by stepping into the new directory and running the maven package command:\n\n\ncd myapexapp\nmvn clean package -DskipTests\n\n\n\nThe build should create the application package file \nmyapexapp/target/myapexapp-1.0-SNAPSHOT.apa\n. This application package can then be used to launch example application via \napex\n CLI, or other visual management tools.  When running, this application will generate a stream of random numbers and print them out, each prefixed by the string \nhello world:\n.\n\n\nRunning Unit Tests\n\n\nTo run unit tests on Linux or OSX, simply run the usual maven command, for example: \nmvn test\n.\n\n\nOn Windows, an additional file, \nwinutils.exe\n, is required; download it from\n\nhttps://github.com/srccodes/hadoop-common-2.2.0-bin/archive/master.zip\n\nand unpack the archive to, say, \nC:\\hadoop\n; this file should be present under\n\nhadoop-common-2.2.0-bin-master\\bin\n within it.\n\n\nSet the \nHADOOP_HOME\n environment variable system-wide to\n\nc:\\hadoop\\hadoop-common-2.2.0-bin-master\n as described at:\n\nhttps://www.microsoft.com/resources/documentation/windows/xp/all/proddocs/en-us/sysdm_advancd_environmnt_addchange_variable.mspx?mfr=true\n. You should now be able to run unit tests normally.\n\n\nIf you prefer not to set the variable globally, you can set it on the command line or within\nyour IDE. For example, on the command line, specify the maven\nproperty \nhadoop.home.dir\n:\n\n\nmvn -Dhadoop.home.dir=c:\\hadoop\\hadoop-common-2.2.0-bin-master test\n\n\n\nor set the environment variable separately:\n\n\nset HADOOP_HOME=c:\\hadoop\\hadoop-common-2.2.0-bin-master\nmvn test\n\n\n\nWithin your IDE, set the environment variable and then run the desired\nunit test in the usual way. For example, with NetBeans you can add:\n\n\nEnv.HADOOP_HOME=c:/hadoop/hadoop-common-2.2.0-bin-master\n\n\n\nat \nProperties \n Actions \n Run project \n Set Properties\n.\n\n\nSimilarly, in Eclipse (Mars) add it to the\nproject properties at \nProperties \n Run/Debug Settings \n ApplicationTest\n\n Environment\n tab.\n\n\nBuilding Apex Demos\n\n\nIf you want to see more substantial Apex demo applications and the associated source code, you can follow these simple steps to check out and build them.\n\n\n\n\n\n\nCheck out the source code repositories:\n\n\ngit clone https://github.com/apache/apex-core\ngit clone https://github.com/apache/apex-malhar\n\n\n\n\n\n\n\nSwitch to the appropriate release branch and build each repository:\n\n\ncd apex-core\nmvn clean install -DskipTests\n\ncd apex-malhar\nmvn clean install -DskipTests\n\n\n\n\n\n\n\nThe \ninstall\n argument to the \nmvn\n command installs resources from each project to your local maven repository (typically \n.m2/repository\n under your home directory), and \nnot\n to the system directories, so Administrator privileges are not required. The  \n-DskipTests\n argument skips running unit tests since they take a long time. If this is a first-time installation, it might take several minutes to complete because maven will download a number of associated plugins.\n\n\nAfter the build completes, you should see the demo application package files in the target directory under each demo subdirectory in \napex-malhar/demos\n.\n\n\nSandbox\n\n\nTo jump-start development with Apex, please refer to the \nDownloads\n section of the Apache Apex website, which provides a list of 3rd party Apex binary packages and sandbox environments.", 
            "title": "Development Setup"
        }, 
        {
            "location": "/apex_development_setup/#apache-apex-development-environment-setup", 
            "text": "This document discusses the steps needed for setting up a development environment for creating applications that run on the Apache Apex platform.", 
            "title": "Apache Apex Development Environment Setup"
        }, 
        {
            "location": "/apex_development_setup/#development-tools", 
            "text": "There are a few tools that will be helpful when developing Apache Apex applications, including:    git  - A revision control system (version 1.7.1 or later). There are multiple git clients available for Windows ( http://git-scm.com/download/win  for example), so download and install a client of your choice.    java JDK  (not JRE) - Includes the Java Runtime Environment as well as the Java compiler and a variety of tools (version 1.7.0_79 or later). Can be downloaded from the Oracle website.    maven  - Apache Maven is a build system for Java projects (version 3.0.5 or later). It can be downloaded from  https://maven.apache.org/download.cgi .    IDE  (Optional) - If you prefer to use an IDE (Integrated Development Environment) such as  NetBeans ,  Eclipse  or  IntelliJ , install that as well.    After installing these tools, make sure that the directories containing the executable files are in your PATH environment variable.   Windows  - Open a console window and enter the command  echo %PATH%  to see the value of the  PATH  variable and verify that the above directories for Java, git, and maven executables are present.  JDK executables like  java  and  javac , the directory might be something like  C:\\Program Files\\Java\\jdk1.7.0\\_80\\bin ; for  git  it might be  C:\\Program Files\\Git\\bin ; and for maven it might be  C:\\Users\\user\\Software\\apache-maven-3.3.3\\bin .  If not, you can change its value clicking on the button at  Control Panel     Advanced System Settings     Advanced tab     Environment Variables .  Linux and Mac  - Open a console/terminal window and enter the command  echo $PATH  to see the value of the  PATH  variable and verify that the above directories for Java, git, and maven executables are present.  If not, make sure software is downloaded and installed, and optionally PATH reference is added and exported  in a  ~/.profile  or  ~/.bash_profile .  For example to add maven located in  /sfw/maven/apache-maven-3.3.3  to PATH add the line:  export PATH=$PATH:/sfw/maven/apache-maven-3.3.3/bin   Confirm by running the following commands and comparing with output that show in the table below:         Command  Output    javac -version  javac 1.7.0_80    java -version  java version  1.7.0_80  Java(TM) SE Runtime Environment (build 1.7.0_80-b15)  Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode)    git --version  git version 2.6.1.windows.1    mvn --version  Apache Maven 3.3.3 (7994120775791599e205a5524ec3e0dfe41d4a06; 2015-04-22T06:57:37-05:00)  ...", 
            "title": "Development Tools"
        }, 
        {
            "location": "/apex_development_setup/#creating-new-apex-project", 
            "text": "After development tools are configured, you can now use the maven archetype to create a basic Apache Apex project.   Note:  When executing the commands below, you can optionally replace  RELEASE  with a  specific version  of Apache Apex.    Windows  - Create a new Windows command file called  newapp.cmd  by copying the lines below, and execute it.  When you run this file, the properties will be displayed and you will be prompted with  Y: : ; just press  Enter  to complete the project generation.  The caret (^) at the end of some lines indicates that a continuation line follows.   @echo off\n@rem Script for creating a new application\nsetlocal\nmvn archetype:generate ^\n -DarchetypeGroupId=org.apache.apex ^\n -DarchetypeArtifactId=apex-app-archetype -DarchetypeVersion=RELEASE ^\n -DgroupId=com.example -Dpackage=com.example.myapexapp -DartifactId=myapexapp ^\n -Dversion=1.0-SNAPSHOT\nendlocal    Linux  - Execute the lines below in a terminal window.  New project will be created in the curent working directory.  The backslash (\\) at the end of the lines indicates continuation.  mvn archetype:generate \\\n -DarchetypeGroupId=org.apache.apex \\\n -DarchetypeArtifactId=apex-app-archetype -DarchetypeVersion=RELEASE \\\n -DgroupId=com.example -Dpackage=com.example.myapexapp -DartifactId=myapexapp \\\n -Dversion=1.0-SNAPSHOT    When the run completes successfully, you should see a new directory named  myapexapp  containing a maven project for building a basic Apache Apex application. It includes 3 source files: Application.java ,   RandomNumberGenerator.java  and  ApplicationTest.java . You can now build the application by stepping into the new directory and running the maven package command:  cd myapexapp\nmvn clean package -DskipTests  The build should create the application package file  myapexapp/target/myapexapp-1.0-SNAPSHOT.apa . This application package can then be used to launch example application via  apex  CLI, or other visual management tools.  When running, this application will generate a stream of random numbers and print them out, each prefixed by the string  hello world: .", 
            "title": "Creating New Apex Project"
        }, 
        {
            "location": "/apex_development_setup/#running-unit-tests", 
            "text": "To run unit tests on Linux or OSX, simply run the usual maven command, for example:  mvn test .  On Windows, an additional file,  winutils.exe , is required; download it from https://github.com/srccodes/hadoop-common-2.2.0-bin/archive/master.zip \nand unpack the archive to, say,  C:\\hadoop ; this file should be present under hadoop-common-2.2.0-bin-master\\bin  within it.  Set the  HADOOP_HOME  environment variable system-wide to c:\\hadoop\\hadoop-common-2.2.0-bin-master  as described at: https://www.microsoft.com/resources/documentation/windows/xp/all/proddocs/en-us/sysdm_advancd_environmnt_addchange_variable.mspx?mfr=true . You should now be able to run unit tests normally.  If you prefer not to set the variable globally, you can set it on the command line or within\nyour IDE. For example, on the command line, specify the maven\nproperty  hadoop.home.dir :  mvn -Dhadoop.home.dir=c:\\hadoop\\hadoop-common-2.2.0-bin-master test  or set the environment variable separately:  set HADOOP_HOME=c:\\hadoop\\hadoop-common-2.2.0-bin-master\nmvn test  Within your IDE, set the environment variable and then run the desired\nunit test in the usual way. For example, with NetBeans you can add:  Env.HADOOP_HOME=c:/hadoop/hadoop-common-2.2.0-bin-master  at  Properties   Actions   Run project   Set Properties .  Similarly, in Eclipse (Mars) add it to the\nproject properties at  Properties   Run/Debug Settings   ApplicationTest  Environment  tab.", 
            "title": "Running Unit Tests"
        }, 
        {
            "location": "/apex_development_setup/#building-apex-demos", 
            "text": "If you want to see more substantial Apex demo applications and the associated source code, you can follow these simple steps to check out and build them.    Check out the source code repositories:  git clone https://github.com/apache/apex-core\ngit clone https://github.com/apache/apex-malhar    Switch to the appropriate release branch and build each repository:  cd apex-core\nmvn clean install -DskipTests\n\ncd apex-malhar\nmvn clean install -DskipTests    The  install  argument to the  mvn  command installs resources from each project to your local maven repository (typically  .m2/repository  under your home directory), and  not  to the system directories, so Administrator privileges are not required. The   -DskipTests  argument skips running unit tests since they take a long time. If this is a first-time installation, it might take several minutes to complete because maven will download a number of associated plugins.  After the build completes, you should see the demo application package files in the target directory under each demo subdirectory in  apex-malhar/demos .", 
            "title": "Building Apex Demos"
        }, 
        {
            "location": "/apex_development_setup/#sandbox", 
            "text": "To jump-start development with Apex, please refer to the  Downloads  section of the Apache Apex website, which provides a list of 3rd party Apex binary packages and sandbox environments.", 
            "title": "Sandbox"
        }, 
        {
            "location": "/application_development/", 
            "text": "Application Developer Guide\n\n\nThe Apex platform is designed to process massive amounts of\nreal-time events natively in Hadoop.  It runs as a YARN (Hadoop 2.x) \napplication and leverages Hadoop as a distributed operating\nsystem.  All the basic distributed operating system capabilities of\nHadoop like resource management (YARN), distributed file system (HDFS),\nmulti-tenancy, security, fault-tolerance, and scalability are supported natively \nin all the Apex applications. \u00a0The platform handles all the details of the application \nexecution, including dynamic scaling, state checkpointing and recovery, event \nprocessing guarantees, etc. allowing you to focus on writing your application logic without\nmixing operational and functional concerns.\n\n\nIn the platform, building a streaming application can be extremely\neasy and intuitive. \u00a0The application is represented as a Directed\nAcyclic Graph (DAG) of computation units called \nOperators\n interconnected\nby the data-flow edges called  \nStreams\n.\u00a0The operators process input\nstreams and produce output streams. A library of common operators is\nprovided to enable quick application development. \u00a0In case the desired\nprocessing is not available in the Operator Library, one can easily\nwrite a custom operator. We refer those interested in creating their own\noperators to the \nOperator Development Guide\n.\n\n\nRunning A Test Application\n\n\nIf you are starting with the Apex platform for the first time,\nit can be informative to launch an existing application and see it run.\nOne of the simplest examples provided in \nApex-Malhar repository\n is a Pi demo application,\nwhich computes the value of PI using random numbers.  After \nsetting up development environment\n\nPi demo can be launched as follows:\n\n\n\n\nOpen up Apex Malhar files in your IDE (for example Eclipse, IntelliJ, NetBeans, etc)\n\n\nNavigate to \ndemos/pi/src/test/java/com/datatorrent/demos/ApplicationTest.java\n\n\nRun the test for ApplicationTest.java\n\n\nView the output in system console\n\n\n\n\nCongratulations, you just ran your first real-time streaming demo :) \nThis demo is very simple and has four operators. The first operator\nemits random integers between 0 to 30, 000. The second operator receives\nthese coefficients and emits a hashmap with x and y values each time it\nreceives two values. The third operator takes these values and computes\nx**2+y**2. The last operator counts how many computed values from\nthe previous operator were less than or equal to 30, 000**2. Assuming\nthis count is N, then PI is computed as N/number of values received.\nHere is the code snippet for the PI application. This code populates the\nDAG. Do not worry about what each line does, we will cover these\nconcepts later in this document.\n\n\n// Generates random numbers\nRandomEventGenerator rand = dag.addOperator(\nrand\n, new RandomEventGenerator());\nrand.setMinvalue(0);\nrand.setMaxvalue(30000);\n\n// Generates a round robin HashMap of \nx\n and \ny\n\nRoundRobinHashMap\nString,Object\n rrhm = dag.addOperator(\nrrhm\n, new RoundRobinHashMap\nString, Object\n());\nrrhm.setKeys(new String[] { \nx\n, \ny\n });\n\n// Calculates pi from x and y\nJavaScriptOperator calc = dag.addOperator(\npicalc\n, new Script());\ncalc.setPassThru(false);\ncalc.put(\ni\n,0);\ncalc.put(\ncount\n,0);\ncalc.addSetupScript(\nfunction pi() { if (x*x+y*y \n= \n+maxValue*maxValue+\n) { i++; } count++; return i / count * 4; }\n);\ncalc.setInvoke(\npi\n);\ndag.addStream(\nrand_rrhm\n, rand.integer_data, rrhm.data);\ndag.addStream(\nrrhm_calc\n, rrhm.map, calc.inBindings);\n\n// puts results on system console\nConsoleOutputOperator console = dag.addOperator(\nconsole\n, new ConsoleOutputOperator());\ndag.addStream(\nrand_console\n,calc.result, console.input);\n\n\n\n\nYou can review the other demos and see what they do. The examples\ngiven in the Demos project cover various features of the platform and we\nstrongly encourage you to read these to familiarize yourself with the\nplatform. In the remaining part of this document we will go through\ndetails needed for you to develop and run streaming applications in\nMalhar.\n\n\nTest Application: Yahoo! Finance Quotes\n\n\nThe PI\u00a0application was to\nget you started. It is a basic application and does not fully illustrate\nthe features of the platform. For the purpose of describing concepts, we\nwill consider the test application shown in Figure 1. The application\ndownloads tick data from  \nYahoo! Finance\n \u00a0and computes the\nfollowing for four tickers, namely \nIBM\n,\n\nGOOG\n, \nYHOO\n.\n\n\n\n\nQuote: Consisting of last trade price, last trade time, and\n    total volume for the day\n\n\nPer-minute chart data: Highest trade price, lowest trade\n    price, and volume during that minute\n\n\nSimple Moving Average: trade price over 5 minutes\n\n\n\n\nTotal volume must ensure that all trade volume for that day is\nadded, i.e. data loss would result in wrong results. Charting data needs\nall the trades in the same minute to go to the same slot, and then on it\nstarts afresh, so again data loss would result in wrong results. The\naggregation for charting data is done over 1 minute. Simple moving\naverage computes the average price over a 5 minute sliding window; it\ntoo would produce wrong results if there is data loss. Figure 1 shows\nthe application with no partitioning.\n\n\n\n\nThe operator StockTickerInput:\u00a0StockTickerInput\n\u00a0\nis\nthe input operator that reads live data from Yahoo! Finance once per\ninterval (user configurable in milliseconds), and emits the price, the\nincremental volume, and the last trade time of each stock symbol, thus\nemulating real ticks from the exchange. \u00a0We utilize the Yahoo! Finance\nCSV web service interface. \u00a0For example:\n\n\n$ GET 'http://download.finance.yahoo.com/d/quotes.csv?s=IBM,GOOG,AAPL,YHOO\nf=sl1vt1'\n\nIBM\n,203.966,1513041,\n1:43pm\n\n\nGOOG\n,762.68,1879741,\n1:43pm\n\n\nAAPL\n,444.3385,11738366,\n1:43pm\n\n\nYHOO\n,19.3681,14707163,\n1:43pm\n\n\n\n\n\nAmong all the operators in Figure 1, StockTickerInput is the only\noperator that requires extra code because it contains a custom mechanism\nto get the input data. \u00a0Other operators are used unchanged from the\nMalhar library.\n\n\nHere is the class implementation for StockTickInput:\n\n\npackage com.datatorrent.demos.yahoofinance;\n\nimport au.com.bytecode.opencsv.CSVReader;\nimport com.datatorrent.annotation.OutputPortFieldAnnotation;\nimport com.datatorrent.api.Context.OperatorContext;\nimport com.datatorrent.api.DefaultOutputPort;\nimport com.datatorrent.api.InputOperator;\nimport com.datatorrent.lib.util.KeyValPair;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.InputStreamReader;\nimport java.util.*;\nimport org.apache.commons.httpclient.HttpClient;\nimport org.apache.commons.httpclient.HttpStatus;\nimport org.apache.commons.httpclient.cookie.CookiePolicy;\nimport org.apache.commons.httpclient.methods.GetMethod;\nimport org.apache.commons.httpclient.params.DefaultHttpParams;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n/**\n * This operator sends price, volume and time into separate ports and calculates incremental volume.\n */\npublic class StockTickInput implements InputOperator\n{\n  private static final Logger logger = LoggerFactory.getLogger(StockTickInput.class);\n  /**\n   * Timeout interval for reading from server. 0 or negative indicates no timeout.\n   */\n  public int readIntervalMillis = 500;\n  /**\n   * The URL of the web service resource for the POST request.\n   */\n  private String url;\n  public String[] symbols;\n  private transient HttpClient client;\n  private transient GetMethod method;\n  private HashMap\nString, Long\n lastVolume = new HashMap\nString, Long\n();\n  private boolean outputEvenIfZeroVolume = false;\n  /**\n   * The output port to emit price.\n   */\n  @OutputPortFieldAnnotation(optional = true)\n  public final transient DefaultOutputPort\nKeyValPair\nString, Double\n price = new DefaultOutputPort\nKeyValPair\nString, Double\n();\n  /**\n   * The output port to emit incremental volume.\n   */\n  @OutputPortFieldAnnotation(optional = true)\n  public final transient DefaultOutputPort\nKeyValPair\nString, Long\n volume = new DefaultOutputPort\nKeyValPair\nString, Long\n();\n  /**\n   * The output port to emit last traded time.\n   */\n  @OutputPortFieldAnnotation(optional = true)\n  public final transient DefaultOutputPort\nKeyValPair\nString, String\n time = new DefaultOutputPort\nKeyValPair\nString, String\n();\n\n  /**\n   * Prepare URL from symbols and parameters. URL will be something like: http://download.finance.yahoo.com/d/quotes.csv?s=IBM,GOOG,AAPL,YHOO\nf=sl1vt1\n   *\n   * @return the URL\n   */\n  private String prepareURL()\n  {\n    String str = \nhttp://download.finance.yahoo.com/d/quotes.csv?s=\n;\n    for (int i = 0; i \n symbols.length; i++) {\n      if (i != 0) {\n        str += \n,\n;\n      }\n      str += symbols[i];\n    }\n    str += \nf=sl1vt1\ne=.csv\n;\n    return str;\n  }\n\n  @Override\n  public void setup(OperatorContext context)\n  {\n    url = prepareURL();\n    client = new HttpClient();\n    method = new GetMethod(url);\n    DefaultHttpParams.getDefaultParams().setParameter(\nhttp.protocol.cookie-policy\n, CookiePolicy.BROWSER_COMPATIBILITY);\n  }\n\n  @Override\n  public void teardown()\n  {\n  }\n\n  @Override\n  public void emitTuples()\n  {\n\n    try {\n      int statusCode = client.executeMethod(method);\n      if (statusCode != HttpStatus.SC_OK) {\n        System.err.println(\nMethod failed: \n + method.getStatusLine());\n      } else {\n        InputStream istream = method.getResponseBodyAsStream();\n        // Process response\n        InputStreamReader isr = new InputStreamReader(istream);\n        CSVReader reader = new CSVReader(isr);\n        List\nString[]\n myEntries = reader.readAll();\n        for (String[] stringArr: myEntries) {\n          ArrayList\nString\n tuple = new ArrayList\nString\n(Arrays.asList(stringArr));\n          if (tuple.size() != 4) {\n            return;\n          }\n          // input csv is \nSymbol\n,\nPrice\n,\nVolume\n,\nTime\n\n          String symbol = tuple.get(0);\n          double currentPrice = Double.valueOf(tuple.get(1));\n          long currentVolume = Long.valueOf(tuple.get(2));\n          String timeStamp = tuple.get(3);\n          long vol = currentVolume;\n          // Sends total volume in first tick, and incremental volume afterwards.\n          if (lastVolume.containsKey(symbol)) {\n            vol -= lastVolume.get(symbol);\n          }\n\n          if (vol \n 0 || outputEvenIfZeroVolume) {\n            price.emit(new KeyValPair\nString, Double\n(symbol, currentPrice));\n            volume.emit(new KeyValPair\nString, Long\n(symbol, vol));\n            time.emit(new KeyValPair\nString, String\n(symbol, timeStamp));\n            lastVolume.put(symbol, currentVolume);\n          }\n        }\n      }\n      Thread.sleep(readIntervalMillis);\n    } catch (InterruptedException ex) {\n      logger.debug(ex.toString());\n    } catch (IOException ex) {\n      logger.debug(ex.toString());\n    }\n  }\n\n  @Override\n  public void beginWindow(long windowId)\n  {\n  }\n\n  @Override\n  public void endWindow()\n  {\n  }\n\n  public void setOutputEvenIfZeroVolume(boolean outputEvenIfZeroVolume)\n  {\n    this.outputEvenIfZeroVolume = outputEvenIfZeroVolume;\n  }\n\n}\n\n\n\n\nThe operator has three output ports that emit the price of the\nstock, the volume of the stock and the last trade time of the stock,\ndeclared as public member variables price, volume\u00a0and  time\u00a0of the class. \u00a0The tuple of the\nprice\u00a0output port is a key-value\npair with the stock symbol being the key, and the price being the value.\n\u00a0The tuple of the volume\u00a0output\nport is a key value pair with the stock symbol being the key, and the\nincremental volume being the value. \u00a0The tuple of the  time\u00a0output port is a key value pair with the\nstock symbol being the key, and the last trade time being the\nvalue.\n\n\nImportant: Since operators will be\nserialized, all input and output ports need to be declared transient\nbecause they are stateless and should not be serialized.\n\n\nThe method\u00a0setup(OperatorContext)\ncontains the code that is necessary for setting up the HTTP\nclient for querying Yahoo! Finance.\n\n\nMethod\u00a0emitTuples() contains\nthe code that reads from Yahoo! Finance, and emits the data to the\noutput ports of the operator. \u00a0emitTuples()\u00a0will be called one or more times\nwithin one application window as long as time is allowed within the\nwindow.\n\n\nNote that we want to emulate the tick input stream by having\nincremental volume data with Yahoo! Finance data. \u00a0We therefore subtract\nthe previous volume from the current volume to emulate incremental\nvolume for each tick.\n\n\nThe operator\nDailyVolume:\u00a0This operator\nreads from the input port, which contains the incremental volume tuples\nfrom StockTickInput, and\naggregates the data to provide the cumulative volume. \u00a0It uses the\nlibrary class  SumKeyVal\nK,V\n\u00a0provided in math\u00a0package. \u00a0In this case,\nSumKeyVal\nString,Long\n, where K is the stock symbol, V is the\naggregated volume, with cumulative\nset to true. (Otherwise if  cumulativewas set to false, SumKeyVal would\nprovide the sum for the application window.) \u00a0Malhar provides a number\nof built-in operators for simple operations like this so that\napplication developers do not have to write them. \u00a0More examples to\nfollow. This operator assumes that the application restarts before the\nmarket opens every day.\n\n\nThe operator Quote:\nThis operator has three input ports, which are price (from\nStockTickInput), daily_vol (from\nDaily Volume), and time (from\n StockTickInput). \u00a0This operator\njust consolidates the three data items and and emits the consolidated\ndata. \u00a0It utilizes the class ConsolidatorKeyVal\nK\n\u00a0from the\nstream\u00a0package.\n\n\nThe operator HighLow:\u00a0This operator reads from the input port,\nwhich contains the price tuples from StockTickInput, and provides the high and the\nlow price within the application window. \u00a0It utilizes the library class\n RangeKeyVal\nK,V\n\u00a0provided\nin the math\u00a0package. In this case,\nRangeKeyVal\nString,Double\n.\n\n\nThe operator MinuteVolume:\nThis operator reads from the input port, which contains the\nvolume tuples from StockTickInput,\nand aggregates the data to provide the sum of the volume within one\nminute. \u00a0Like the operator  DailyVolume, this operator also uses\nSumKeyVal\nString,Long\n, but\nwith cumulative set to false. \u00a0The\nApplication Window is set to one minute. We will explain how to set this\nlater.\n\n\nThe operator Chart:\nThis operator is very similar to the operator Quote, except that it takes inputs from\nHigh Low\u00a0and  Minute Vol\u00a0and outputs the consolidated tuples\nto the output port.\n\n\nThe operator PriceSMA:\nSMA stands for - Simple Moving Average. It reads from the\ninput port, which contains the price tuples from StockTickInput, and\nprovides the moving average price of the stock. \u00a0It utilizes\nSimpleMovingAverage\nString,Double\n, which is provided in the\n multiwindow\u00a0package.\nSimpleMovingAverage keeps track of the data of the previous N\napplication windows in a sliding manner. \u00a0For each end window event, it\nprovides the average of the data in those application windows.\n\n\nThe operator Console:\nThis operator just outputs the input tuples to the console\n(or stdout). \u00a0In this example, there are four console\u00a0operators, which connect to the output\nof  Quote, Chart, PriceSMA and VolumeSMA. \u00a0In\npractice, they should be replaced by operators that use the data to\nproduce visualization artifacts like charts.\n\n\nConnecting the operators together and constructing the\nDAG:\u00a0Now that we know the\noperators used, we will create the DAG, set the streaming window size,\ninstantiate the operators, and connect the operators together by adding\nstreams that connect the output ports with the input ports among those\noperators. \u00a0This code is in the file  YahooFinanceApplication.java. Refer to Figure 1\nagain for the graphical representation of the DAG. \u00a0The last method in\nthe code, namely getApplication(),\ndoes all that. \u00a0The rest of the methods are just for setting up the\noperators.\n\n\npackage com.datatorrent.demos.yahoofinance;\n\nimport com.datatorrent.api.ApplicationFactory;\nimport com.datatorrent.api.Context.OperatorContext;\nimport com.datatorrent.api.DAG;\nimport com.datatorrent.api.Operator.InputPort;\nimport com.datatorrent.lib.io.ConsoleOutputOperator;\nimport com.datatorrent.lib.math.RangeKeyVal;\nimport com.datatorrent.lib.math.SumKeyVal;\nimport com.datatorrent.lib.multiwindow.SimpleMovingAverage;\nimport com.datatorrent.lib.stream.ConsolidatorKeyVal;\nimport com.datatorrent.lib.util.HighLow;\nimport org.apache.hadoop.conf.Configuration;\n\n/**\n * Yahoo! Finance application demo. \np\n\n *\n * Get Yahoo finance feed and calculate minute price range, minute volume, simple moving average of 5 minutes.\n */\npublic class Application implements StreamingApplication\n{\n  private int streamingWindowSizeMilliSeconds = 1000; // 1 second (default is 500ms)\n  private int appWindowCountMinute = 60;   // 1 minute\n  private int appWindowCountSMA = 5 * 60;  // 5 minute\n\n  /**\n   * Get actual Yahoo finance ticks of symbol, last price, total daily volume, and last traded price.\n   */\n  public StockTickInput getStockTickInputOperator(String name, DAG dag)\n  {\n    StockTickInput oper = dag.addOperator(name, StockTickInput.class);\n    oper.readIntervalMillis = 200;\n    return oper;\n  }\n\n  /**\n   * This sends total daily volume by adding volumes from each ticks.\n   */\n  public SumKeyVal\nString, Long\n getDailyVolumeOperator(String name, DAG dag)\n  {\n    SumKeyVal\nString, Long\n oper = dag.addOperator(name, new SumKeyVal\nString, Long\n());\n    oper.setType(Long.class);\n    oper.setCumulative(true);\n    return oper;\n  }\n\n  /**\n   * Get aggregated volume of 1 minute and send at the end window of 1 minute.\n   */\n  public SumKeyVal\nString, Long\n getMinuteVolumeOperator(String name, DAG dag, int appWindowCount)\n  {\n    SumKeyVal\nString, Long\n oper = dag.addOperator(name, new SumKeyVal\nString, Long\n());\n    oper.setType(Long.class);\n    oper.setEmitOnlyWhenChanged(true);\ndag.getOperatorMeta(name).getAttributes().put(OperatorContext.APPLICATION_WINDOW_COUNT,appWindowCount);\n    return oper;\n  }\n\n  /**\n   * Get High-low range for 1 minute.\n   */\n  public RangeKeyVal\nString, Double\n getHighLowOperator(String name, DAG dag, int appWindowCount)\n  {\n    RangeKeyVal\nString, Double\n oper = dag.addOperator(name, new RangeKeyVal\nString, Double\n());\n    dag.getOperatorMeta(name).getAttributes().put(OperatorContext.APPLICATION_WINDOW_COUNT,appWindowCount);\n    oper.setType(Double.class);\n    return oper;\n  }\n\n  /**\n   * Quote (Merge price, daily volume, time)\n   */\n  public ConsolidatorKeyVal\nString,Double,Long,String,?,?\n getQuoteOperator(String name, DAG dag)\n  {\n    ConsolidatorKeyVal\nString,Double,Long,String,?,?\n oper = dag.addOperator(name, new ConsolidatorKeyVal\nString,Double,Long,String,Object,Object\n());\n    return oper;\n  }\n\n  /**\n   * Chart (Merge minute volume and minute high-low)\n   */\n  public ConsolidatorKeyVal\nString,HighLow,Long,?,?,?\n getChartOperator(String name, DAG dag)\n  {\n    ConsolidatorKeyVal\nString,HighLow,Long,?,?,?\n oper = dag.addOperator(name, new ConsolidatorKeyVal\nString,HighLow,Long,Object,Object,Object\n());\n    return oper;\n  }\n\n  /**\n   * Get simple moving average of price.\n   */\n  public SimpleMovingAverage\nString, Double\n getPriceSimpleMovingAverageOperator(String name, DAG dag, int appWindowCount)\n  {\n    SimpleMovingAverage\nString, Double\n oper = dag.addOperator(name, new SimpleMovingAverage\nString, Double\n());\n    oper.setWindowSize(appWindowCount);\n    oper.setType(Double.class);\n    return oper;\n  }\n\n  /**\n   * Get console for output.\n   */\n  public InputPort\nObject\n getConsole(String name, /*String nodeName,*/ DAG dag, String prefix)\n  {\n    ConsoleOutputOperator oper = dag.addOperator(name, ConsoleOutputOperator.class);\n    oper.setStringFormat(prefix + \n: %s\n);\n    return oper.input;\n  }\n\n  /**\n   * Create Yahoo Finance Application DAG.\n   */\n  @Override\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n    dag.getAttributes().put(DAG.STRAM_WINDOW_SIZE_MILLIS,streamingWindowSizeMilliSeconds);\n\n    StockTickInput tick = getStockTickInputOperator(\nStockTickInput\n, dag);\n    SumKeyVal\nString, Long\n dailyVolume = getDailyVolumeOperator(\nDailyVolume\n, dag);\n    ConsolidatorKeyVal\nString,Double,Long,String,?,?\n quoteOperator = getQuoteOperator(\nQuote\n, dag);\n\n    RangeKeyVal\nString, Double\n highlow = getHighLowOperator(\nHighLow\n, dag, appWindowCountMinute);\n    SumKeyVal\nString, Long\n minuteVolume = getMinuteVolumeOperator(\nMinuteVolume\n, dag, appWindowCountMinute);\n    ConsolidatorKeyVal\nString,HighLow,Long,?,?,?\n chartOperator = getChartOperator(\nChart\n, dag);\n\n    SimpleMovingAverage\nString, Double\n priceSMA = getPriceSimpleMovingAverageOperator(\nPriceSMA\n, dag, appWindowCountSMA);\n       DefaultPartitionCodec\nString, Double\n codec = new DefaultPartitionCodec\nString, Double\n();\n    dag.setInputPortAttribute(highlow.data, PortContext.STREAM_CODEC, codec);\n    dag.setInputPortAttribute(priceSMA.data, PortContext.STREAM_CODEC, codec);\n    dag.addStream(\nprice\n, tick.price, quoteOperator.in1, highlow.data, priceSMA.data);\n    dag.addStream(\nvol\n, tick.volume, dailyVolume.data, minuteVolume.data);\n    dag.addStream(\ntime\n, tick.time, quoteOperator.in3);\n    dag.addStream(\ndaily_vol\n, dailyVolume.sum, quoteOperator.in2);\n\n    dag.addStream(\nquote_data\n, quoteOperator.out, getConsole(\nquoteConsole\n, dag, \nQUOTE\n));\n\n    dag.addStream(\nhigh_low\n, highlow.range, chartOperator.in1);\n    dag.addStream(\nvol_1min\n, minuteVolume.sum, chartOperator.in2);\n    dag.addStream(\nchart_data\n, chartOperator.out, getConsole(\nchartConsole\n, dag, \nCHART\n));\n\n    dag.addStream(\nsma_price\n, priceSMA.doubleSMA, getConsole(\npriceSMAConsole\n, dag, \nPrice SMA\n));\n\n    return dag;\n  }\n\n}\n\n\n\n\nNote that we also set a user-specific sliding window for SMA that\nkeeps track of the previous N data points. \u00a0Do not confuse this with the\nattribute APPLICATION_WINDOW_COUNT.\n\n\nIn the rest of this chapter we will run through the process of\nrunning this application. We assume that \u00a0you are familiar with details\nof your Hadoop infrastructure. For installation\ndetails please refer to the \nInstallation Guide\n.\n\n\nRunning a Test Application\n\n\nWe will now describe how to run the yahoo\nfinance application\u00a0described above in different modes\n(local mode, single node on Hadoop, and multi-nodes on Hadoop).\n\n\nThe platform runs streaming applications under the control of a\nlight-weight Streaming Application Manager (STRAM). Each application has\nits own instance of STRAM. STRAM launches the application and\ncontinually provides run time monitoring, analysis, and takes action\nsuch as load scaling or outage recovery as needed. \u00a0We will discuss\nSTRAM in more detail in the next chapter.\n\n\nThe instructions below assume that the platform was installed in a\ndirectory \nINSTALL_DIR\n and the command line interface (CLI) will\nbe used to launch the demo application. An application can be run in\nlocal mode\u00a0(in IDE or from command line) or on a Hadoop cluster.\n\n\nTo start the Apex CLI run\n\n\nINSTALL_DIR\n/bin/apex\n\n\n\nThe command line prompt appears.  To start the application in local mode (the actual version number in the file name may differ)\n\n\napex\n launch -local \nINSTALL_DIR\n/yahoo-finance-demo-3.2.0-SNAPSHOT.apa\n\n\n\nTo terminate the application in local mode, enter Ctrl-C\n\n\nTu run the application on the Hadoop cluster (the actual version\nnumber in the file name may differ)\n\n\napex\n launch \nINSTALL_DIR\n/yahoo-finance-demo-3.2.0-SNAPSHOT.apa\n\n\n\nTo stop the application running in Hadoop, terminate it in the Apex CLI:\n\n\napex\n kill-app\n\n\n\nExecuting the application in either mode includes the following\nsteps. At a top level, STRAM (Streaming Application Manager) validates\nthe application (DAG), translates the logical plan to the physical plan\nand then launches the execution engine. The mode determines the\nresources needed and how how they are used.\n\n\nLocal Mode\n\n\nIn local mode, the application is run as a single-process\u00a0with multiple threads. Although a\nfew Hadoop classes are needed, there is no dependency on a Hadoop\ncluster or Hadoop services. The local file system is used in place of\nHDFS. This mode allows a quick run of an application in a single process\nsandbox, and hence is the most suitable to debug and analyze the\napplication logic. This mode is recommended for developing the\napplication and can be used for running applications within the IDE for\nfunctional testing purposes. Due to limited resources and lack \u00a0of\nscalability an application running in this single process mode is more\nlikely to encounter throughput bottlenecks. A distributed cluster is\nrecommended for benchmarking and production testing.\n\n\nHadoop Cluster\n\n\nIn this section we discuss various Hadoop cluster setups.\n\n\nSingle Node Cluster\n\n\nIn a single node Hadoop cluster all services are deployed on a\nsingle server (a developer can use his/her development machine as a\nsingle node cluster). The platform does not distinguish between a single\nor multi-node setup and behaves exactly the same in both cases.\n\n\nIn this mode, the resource manager, name node, data node, and node\nmanager occupy one process each. This is an example of running a\nstreaming application as a multi-process\u00a0application on the same server.\nWith prevalence of fast, multi-core systems, this mode is effective for\ndebugging, fine tuning, and generic analysis before submitting the job\nto a larger Hadoop cluster. In this mode, execution uses the Hadoop\nservices and hence is likely to identify issues that are related to the\nHadoop environment (such issues will not be uncovered in local mode).\nThe throughput will obviously not be as high as on a multi-node Hadoop\ncluster. Additionally, since each container (i.e. Java process) requires\na significant amount of memory, you will be able to run a much smaller\nnumber of containers than on a multi-node cluster.\n\n\nMulti-Node Cluster\n\n\nIn a multi-node Hadoop cluster all the services of Hadoop are\ntypically distributed across multiple nodes in a production or\nproduction-level test environment. Upon launch the application is\nsubmitted to the Hadoop cluster and executes as a  multi-processapplication on\u00a0multiple nodes.\n\n\nBefore you start deploying, testing and troubleshooting your\napplication on a cluster, you should ensure that Hadoop (version 2.6.0\nor later)\u00a0is properly installed and\nyou have basic skills for working with it.\n\n\n\n\nApache Apex Platform Overview\n\n\nStreaming Computational Model\n\n\nIn this chapter, we describe the the basics of the real-time streaming platform and its computational model.\n\n\nThe platform is designed to enable completely asynchronous real time computations\u00a0done in as unblocked a way as possible with\nminimal overhead .\n\n\nApplications running in the platform are represented by a Directed\nAcyclic Graph (DAG) made up of \u00a0operators and streams. All computations\nare done in memory on arrival of\nthe input data, with an option to save the output to disk (HDFS) in a\nnon-blocking way. The data that flows between operators consists of\natomic data elements. Each data element along with its type definition\n(henceforth called  schema) is\ncalled a tuple.\u00a0An application is a\ndesign of the flow of these tuples to and from\nthe appropriate compute units to enable the computation of the final\ndesired results.\u00a0A message queue (henceforth called\n\u00a0buffer server) manages tuples streaming\nbetween compute units in different processes.This server keeps track of\nall consumers, publishers, partitions, and enables replay. More\ninformation is given in later section.\n\n\nThe streaming application is monitored by a decision making entity\ncalled STRAM (streaming application\nmanager).\u00a0STRAM is designed to be a light weight\ncontroller that has minimal but sufficient interaction with the\napplication. This is done via periodic heartbeats. The\nSTRAM does the initial launch and periodically analyzes the system\nmetrics to decide if any run time action needs to be taken.\n\n\nA fundamental building block for the streaming platform\nis the concept of breaking up a stream into equal finite time slices\ncalled streaming windows. Each window contains the ordered\nset of tuples in that time slice. A typical duration of a window is 500\nms, but can be configured per application (the Yahoo! Finance\napplication configures this value in the  properties.xml\u00a0file to be 1000ms = 1s). Each\nwindow is preceded by a begin_window\u00a0event and is terminated by an\nend_window\u00a0event, and is assigned\na unique window ID. Even though the platform performs computations at\nthe tuple level, bookkeeping is done at the window boundary, making the\ncomputations within a window an atomic event in the platform. \u00a0We can\nthink of each window as an  atomic\nmicro-batch\u00a0of tuples, to be processed together as one\natomic operation (See Figure 2). \u00a0\n\n\nThis atomic batching allows the platform to avoid the very steep\nper tuple bookkeeping cost and instead has a manageable per batch\nbookkeeping cost. This translates to higher throughput, low recovery\ntime, and higher scalability. Later in this document we illustrate how\nthe atomic micro-batch concept allows more efficient optimization\nalgorithms.\n\n\nThe platform also has in-built support for\napplication windows.\u00a0 An application window is part of the\napplication specification, and can be a small or large multiple of the\nstreaming window. \u00a0An example from our Yahoo! Finance test application\nis the moving average, calculated over a sliding application window of 5\nminutes which equates to 300 (= 5 * 60) streaming windows.\n\n\nNote that these two window concepts are distinct. \u00a0A streaming\nwindow is an abstraction of many tuples into a higher atomic event for\neasier management. \u00a0An application window is a group of consecutive\nstreaming windows used for data aggregation (e.g. sum, average, maximum,\nminimum) on a per operator level.\n\n\n\n\nAlongside the platform,\u00a0a set of\npredefined, benchmarked standard library operator templates is provided\nfor ease of use and rapid development of application.\u00a0These\noperators are open sourced to Apache Software Foundation under the\nproject name \u201cMalhar\u201d as part of our efforts to foster community\ninnovation. These operators can be used in a DAG as is, while others\nhave properties\u00a0that can be set to specify the\ndesired computation. Those interested in details, should refer to\n\nApex-Malhar operator library\n.\n\n\nThe platform is a Hadoop YARN native\napplication. It runs in a Hadoop cluster just like any\nother YARN application (MapReduce etc.) and is designed to seamlessly\nintegrate with rest of Hadoop technology stack. It leverages Hadoop as\nmuch as possible and relies on it as its distributed operating system.\nHadoop dependencies include resource management, compute/memory/network\nallocation, HDFS, security, fault tolerance, monitoring, metrics,\nmulti-tenancy, logging etc. Hadoop classes/concepts are reused as much\nas possible.  The aim is to enable enterprises\nto leverage their existing Hadoop infrastructure for real time streaming\napplications. The platform is designed to scale with big\ndata applications and scale with Hadoop.\n\n\nA streaming application is an asynchronous execution of\ncomputations across distributed nodes. All computations are done in\nparallel on a distributed cluster. The computation model is designed to\ndo as many parallel computations as possible in a non-blocking fashion.\nThe task of monitoring of the entire application is done on (streaming)\nwindow boundaries with a streaming window as an atomic entity. A window\ncompletion is a quantum of work done. There is no assumption that an\noperator can be interrupted at precisely a particular tuple or window.\n\n\nAn operator itself also\ncannot assume or predict the exact time a tuple that it emitted would\nget consumed by downstream operators. The operator processes the tuples\nit gets and simply emits new tuples based on its business logic. The\nonly guarantee it has is that the upstream operators are processing\neither the current or some later window, and the downstream operator is\nprocessing either the current or some earlier window. The completion of\na window (i.e. propagation of the  end_window\u00a0event through an operator) in any\noperator guarantees that all upstream operators have finished processing\nthis window. Thus, the end_window\u00a0event is blocking on an operator\nwith multiple outputs, and is a synchronization point in the DAG. The\n begin_window\u00a0event does not have\nany such restriction, a single begin_window\u00a0event from any upstream operator\ntriggers the operator to start processing tuples.\n\n\nStreaming Application Manager (STRAM)\n\n\nStreaming Application Manager (STRAM) is the Hadoop YARN native\napplication master. STRAM is the first process that is activated upon\napplication launch and orchestrates the streaming application on the\nplatform. STRAM is a lightweight controller process. The\nresponsibilities of STRAM include\n\n\n\n\n\n\nRunning the Application\n\n\n\n\nRead the\u00a0logical plan\u00a0of the application (DAG) submitted by the client\n\n\nValidate the logical plan\n\n\nTranslate the logical plan into a physical plan, where certain operators may  be partitioned (i.e. replicated) to multiple operators for  handling load.\n\n\nRequest resources (Hadoop containers) from Resource Manager,\n    per physical plan\n\n\nBased on acquired resources and application attributes, create\n    an execution plan\u00a0by partitioning the DAG into fragments,\n    each assigned to different containers.\n\n\nExecutes the application by deploying each fragment to\n    its container. Containers then start stream processing and run\n    autonomously, processing one streaming window after another. Each\n    container is represented as an instance of the  StreamingContainer\u00a0class, which updates\n    STRAM via the heartbeat protocol and processes directions received\n    from STRAM.\n\n\n\n\n\n\n\n\nContinually monitoring the application via heartbeats from each StreamingContainer\n\n\n\n\nCollecting Application System Statistics and Logs\n\n\nLogging all application-wide decisions taken\n\n\nProviding system data on the state of the application via a  Web Service.\n\n\n\n\nSupporting Fault Tolerance\n\n\na.  Detecting a node outage\nb.  Requesting a replacement resource from the Resource Manager\n    and scheduling state restoration for the streaming operators\nc.  Saving state to Zookeeper\n\n\n\n\n\n\nSupporting Dynamic Partitioning:\u00a0Periodically evaluating the SLA and modifying the physical plan if required\n    (logical plan does not change).\n\n\n\n\nEnabling Security:\u00a0Distributing security tokens for distributed components of the execution engine\n    and securing web service requests.\n\n\nEnabling Dynamic modification of DAG: In the future, we intend to allow for user initiated\n    modification of the logical plan to allow for changes to the\n    processing logic and functionality.\n\n\n\n\nAn example of the Yahoo! Finance Quote application scheduled on a\ncluster of 5 Hadoop containers (processes) is shown in Figure 3.\n\n\n\n\nAn example for the translation from a logical plan to a physical\nplan and an execution plan for a subset of the application is shown in\nFigure 4.\n\n\n\n\nHadoop Components\n\n\nIn this section we cover some aspects of Hadoop that your\nstreaming application interacts with. This section is not meant to\neducate the reader on Hadoop, but just get the reader acquainted with\nthe terms. We strongly advise readers to learn Hadoop from other\nsources.\n\n\nA streaming application runs as a native Hadoop 2.x application.\nHadoop 2.x does not differentiate between a map-reduce job and other\napplications, and hence as far as Hadoop is concerned, the streaming\napplication is just another job. This means that your application\nleverages all the bells and whistles Hadoop provides and is fully\nsupported within Hadoop technology stack. The platform is responsible\nfor properly integrating itself with the relevant components of Hadoop\nthat exist today and those that may emerge in the future\n\n\nAll investments that leverage multi-tenancy (for example quotas\nand queues), security (for example kerberos), data flow integration (for\nexample copying data in-out of HDFS), monitoring, metrics collections,\netc. will require no changes when streaming applications run on\nHadoop.\n\n\nYARN\n\n\nYARN\nis\nthe core library of Hadoop 2.x that is tasked with resource management\nand works as a distributed application framework. In this section we\nwill walk through Yarn's components. In Hadoop 2.x, the old jobTracker\nhas been replaced by a combination of ResourceManager (RM) and\nApplicationMaster (AM).\n\n\nResource Manager (RM)\n\n\nResourceManager\n(RM)\nmanages all the distributed resources. It allocates and arbitrates all\nthe slots and the resources (cpu, memory, network) of these slots. It\nworks with per-node NodeManagers (NMs) and per-application\nApplicationMasters (AMs). Currently memory usage is monitored by RM; in\nupcoming releases it will have CPU as well as network management. RM is\nshared by map-reduce and streaming applications. Running streaming\napplications requires no changes in the RM.\n\n\nApplication Master (AM)\n\n\nThe AM is the watchdog or monitoring process for your application\nand has the responsibility of negotiating resources with RM and\ninteracting with NodeManagers to get the allocated containers started.\nThe AM is the starting point of your application and is considered user\ncode (not system Hadoop code). The AM itself runs in one container. All\nresource management within the application are managed by the AM. This\nis a critical feature for Hadoop 2.x where tasks done by jobTracker in\nHadoop 1.0 have been distributed allowing Hadoop 2.x to scale much\nbeyond Hadoop 1.0. STRAM is a native YARN ApplicationManager.\n\n\nNode Managers (NM)\n\n\nThere is one \nNodeManager\n(NM)\nper node in the cluster. All the containers (i.e. processes) on that\nnode are monitored by the NM. It takes instructions from RM and manages\nresources of that node as per RM instructions. NMs interactions are same\nfor map-reduce and for streaming applications. Running streaming\napplications requires no changes in the NM.\n\n\nRPC Protocol\n\n\nCommunication among RM, AM, and NM is done via the Hadoop RPC\nprotocol. Streaming applications use the same protocol to send their\ndata. No changes are needed in RPC support provided by Hadoop to enable\ncommunication done by components of your application.\n\n\nHDFS\n\n\nHadoop includes a highly fault tolerant, high throughput\ndistributed file system (\nHDFS\n).\nIt runs on commodity hardware, and your streaming application will, by\ndefault, use it. There is no difference between files created by a\nstreaming application and those created by map-reduce.\n\n\nDeveloping An Application\n\n\nIn this chapter we describe the methodology to develop an\napplication using the Realtime Streaming Platform. The platform was\ndesigned to make it easy to build and launch sophisticated streaming\napplications with the developer having to deal only with the\napplication/business logic. The platform deals with details of where to\nrun what operators on which servers and how to correctly route streams\nof data among them.\n\n\nDevelopment Process\n\n\nWhile the platform does not mandate a specific methodology or set\nof development tools, we have recommendations to maximize productivity\nfor the different phases of application development.\n\n\nDesign\n\n\n\n\nIdentify common, reusable operators. Use a library\n    if possible.\n\n\nIdentify scalability and performance requirements before\n    designing the DAG.\n\n\nLeverage attributes that the platform supports for scalability\n    and performance.\n\n\nUse operators that are benchmarked and tested so that later\n    surprises are minimized. If you have glue code, create appropriate\n    unit tests for it.\n\n\nUse THREAD_LOCAL locality for high throughput streams. If all\n    the operators on that stream cannot fit in one container,\n    try\u00a0NODE_LOCAL\u00a0locality. Both THREAD_LOCAL and\n    NODE_LOCAL streams avoid the Network Interface Card (NIC)\n    completly. The former uses intra-process communication to also avoid\n    serialization-deserialization overhead.\n\n\nThe overall throughput and latencies are are not necessarily\n    correlated to the number of operators in a simple way -- the\n    relationship is more nuanced. A lot depends on how much work\n    individual operators are doing, how many are able to operate in\n    parallel, and how much data is flowing through the arcs of the DAG.\n    It is, at times, better to break a computation down into its\n    constituent simple parts and then stitch them together via streams\n    to better utilize the compute resources of the cluster. Decide on a\n    per application basis the fine line between complexity of each\n    operator vs too many streams. Doing multiple computations in one\n    operator does save network I/O, while operators that are too complex\n    are hard to maintain.\n\n\nDo not use operators that depend on the order of two streams\n    as far as possible. In such cases behavior is not idempotent.\n\n\nPersist key information to HDFS if possible; it may be useful\n    for debugging later.\n\n\nDecide on an appropriate fault tolerance mechanism. If some\n    data loss is acceptable, use the at-most-once mechanism as it has\n    fastest recovery.\n\n\n\n\nCreating New Project\n\n\nPlease refer to the \nApex Application Packages\n\u00a0for\nthe basic steps for creating a new project.\n\n\nWriting the application code\n\n\nPreferably use an IDE (Eclipse, Netbeans etc.) that allows you to\nmanage dependencies and assists with the Java coding. Specific benefits\ninclude ease of managing operator library jar files, individual operator\nclasses, ports and properties. It will also highlight and assist to\nrectify issues such as type mismatches when adding streams while\ntyping.\n\n\nTesting\n\n\nWrite test cases with JUnit or similar test framework so that code\nis tested as it is written. For such testing, the DAG can run in local\nmode within the IDE. Doing this may involve writing mock input or output\noperators for the integration points with external systems. For example,\ninstead of reading from a live data stream, the application in test mode\ncan read from and write to files. This can be done with a single\napplication DAG by instrumenting a test mode using settings in the\nconfiguration that is passed to the application factory\ninterface.\n\n\nGood test coverage will not only eliminate basic validation errors\nsuch as missing port connections or property constraint violations, but\nalso validate the correct processing of the data. The same tests can be\nre-run whenever the application or its dependencies change (operator\nlibraries, version of the platform etc.)\n\n\nRunning an application\n\n\nThe platform provides a commandline tool called \napex\n for managing applications (launching,\nkilling, viewing, etc.). This tool was already discussed above briefly\nin the section entitled Running the Test Application. It will introspect\nthe jar file specified with the launch command for applications (classes\nthat implement ApplicationFactory) or properties files that define\napplications. It will also deploy the dependency jar files from the\napplication package to the cluster.\n\n\nApex CLI can run the application in local mode (i.e. outside a\ncluster). It is recommended to first run the application in local mode\nin the development environment before launching on the Hadoop cluster.\nThis way some of the external system integration and correct\nfunctionality of the application can be verified in an easier to debug\nenvironment before testing distributed mode.\n\n\nFor more details on CLI please refer to the \nApex CLI Guide\n.\n\n\nApplication API\n\n\nThis section introduces the API to write a streaming application.\nThe work involves connecting operators via streams to form the logical\nDAG. The steps are\n\n\n\n\n\n\nInstantiate an application (DAG)\n\n\n\n\n\n\n(Optional) Set Attributes\n\n\n\n\nAssign application name\n\n\nSet any other attributes as per application requirements\n\n\n\n\n\n\n\n\nCreate/re-use and instantiate operators\n\n\n\n\nAssign operator name that is unique within the  application\n\n\nDeclare schema upfront for each operator (and thereby its ports)\n\n\n(Optional) Set properties\u00a0 and attributes on the dag as per specification\n\n\nConnect ports of operators via streams\n\n\nEach stream connects one output port of an operator to one or  more input ports of other operators.\n\n\n(Optional) Set attributes on the streams\n\n\n\n\n\n\n\n\n\n\n\n\nTest the application.\n\n\n\n\n\n\nThere are two methods to create an application, namely Java, and\nProperties file. Java API is for applications being developed by humans,\nand properties file (Hadoop like) is more suited for DAGs generated by\ntools.\n\n\nJava API\n\n\nThe Java API is the most common way to create a streaming\napplication. It is meant for application developers who prefer to\nleverage the features of Java, and the ease of use and enhanced\nproductivity provided by IDEs like NetBeans or Eclipse. Using Java to\nspecify the application provides extra validation abilities of Java\ncompiler, such as compile time checks for type safety at the time of\nwriting the code. Later in this chapter you can read more about\nvalidation support in the platform.\n\n\nThe developer specifies the streaming application by implementing\nthe ApplicationFactory interface, which is how platform tools (CLI etc.)\nrecognize and instantiate applications. Here we show how to create a\nYahoo! Finance application that streams the last trade price of a ticker\nand computes the high and low price in every 1 min window. Run above\n test application\u00a0to execute the\nDAG in local mode within the IDE.\n\n\nLet us revisit how the Yahoo! Finance test application constructs the DAG:\n\n\npublic class Application implements StreamingApplication\n{\n\n  ...\n\n  @Override\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n    dag.getAttributes().attr(DAG.STRAM_WINDOW_SIZE_MILLIS).set(streamingWindowSizeMilliSeconds);\n\n    StockTickInput tick = getStockTickInputOperator(\nStockTickInput\n, dag);\n    SumKeyVal\nString, Long\n dailyVolume = getDailyVolumeOperator(\nDailyVolume\n, dag);\n    ConsolidatorKeyVal\nString,Double,Long,String,?,?\n quoteOperator = getQuoteOperator(\nQuote\n, dag);\n\n    RangeKeyVal\nString, Double\n highlow = getHighLowOperator(\nHighLow\n, dag, appWindowCountMinute);\n    SumKeyVal\nString, Long\n minuteVolume = getMinuteVolumeOperator(\nMinuteVolume\n, dag, appWindowCountMinute);\n    ConsolidatorKeyVal\nString,HighLow,Long,?,?,?\n chartOperator = getChartOperator(\nChart\n, dag);\n\n    SimpleMovingAverage\nString, Double\n priceSMA = getPriceSimpleMovingAverageOperator(\nPriceSMA\n, dag, appWindowCountSMA);\n\n    dag.addStream(\nprice\n, tick.price, quoteOperator.in1, highlow.data, priceSMA.data);\n    dag.addStream(\nvol\n, tick.volume, dailyVolume.data, minuteVolume.data);\n    dag.addStream(\ntime\n, tick.time, quoteOperator.in3);\n    dag.addStream(\ndaily_vol\n, dailyVolume.sum, quoteOperator.in2);\n\n    dag.addStream(\nquote_data\n, quoteOperator.out, getConsole(\nquoteConsole\n, dag, \nQUOTE\n));\n\n    dag.addStream(\nhigh_low\n, highlow.range, chartOperator.in1);\n    dag.addStream(\nvol_1min\n, minuteVolume.sum, chartOperator.in2);\n    dag.addStream(\nchart_data\n, chartOperator.out, getConsole(\nchartConsole\n, dag, \nCHART\n));\n\n    dag.addStream(\nsma_price\n, priceSMA.doubleSMA, getConsole(\npriceSMAConsole\n, dag, \nPrice SMA\n));\n\n    return dag;\n  }\n}\n\n\n\n\nJSON File DAG Specification\n\n\nIn addition to Java, you can also specify the DAG using JSON, provided the operators in the DAG are present in the dependency jars. Create src/main/resources/app directory under your app package project, and put your JSON files there. This is the specification of a JSON file that specifies an application.\n\n\nCreate a json file under src/main/resources/app, For example \nmyApplication.json\n\n\n{\n  \ndescription\n: \n{application description}\n,\n  \noperators\n: [\n    {\n      \nname\n: \n{operator name}\n,\n      \nclass\n: \n{fully qualified class name of the operator}\n,\n      \nproperties\n: {\n        \n{property key}\n: \n{property value}\n,\n        ...\n      }\n    }, ...\n  ],\n  \nstreams\n: [\n    {\n      \nname\n: \n{stream name}\n,\n      \nsource\n: {\n        \noperatorName\n: \n{source operator name}\n,\n        \nportName\n: \n{source operator output port name}\n\n      }\n      \nsinks\n: [\n        {\n          \noperatorName\n: \n{sink operator name}\n,\n          \nportName\n: \n{sink operator input port name}\n\n        }, ...\n      ]\n    }, ...\n  ]\n}\n\n\n\n\n\n\n\nThe name of the JSON file is taken as the name of the application.\n\n\nThe \ndescription\n field is the description of the application and is optional.\n\n\nThe \noperators\n field is the list of operators the application has. You can specifiy the name, the Java class, and the properties of each operator here.\n\n\nThe \nstreams\n field is the list of streams that connects the operators together to form the DAG. Each stream consists of the stream name, the operator and port that it connects from, and the list of operators and ports that it connects to. Note that you can connect from \none\n output port of an operator to \nmultiple\n different input ports of different operators.\n\n\n\n\nIn Apex Malhar, there is an \nexample\n in the Pi Demo doing just that.\n\n\nProperties File DAG Specification\n\n\nThe platform also supports specification of a DAG via a properties\nfile. The aim here to make it easy for tools to create and run an\napplication. This method of specification does not have the Java\ncompiler support of compile time check, but since these applications\nwould be created by software, they should be correct by construction.\nThe syntax is derived from Hadoop properties and should be easy for\nfolks who are used to creating software that integrated with\nHadoop.\n\n\nUnder the src/main/resources/app directory (create if it doesn't exist), create a properties file.\nFor example \nmyApplication.properties\n\n\n# input operator that reads from a file\napex.operator.inputOp.classname=com.acme.SampleInputOperator\napex.operator.inputOp.fileName=somefile.txt\n\n# output operator that writes to the console\napex.operator.outputOp.classname=com.acme.ConsoleOutputOperator\n\n# stream connecting both operators\napex.stream.inputStream.source=inputOp.outputPort\napex.stream.inputStream.sinks=outputOp.inputPort\n\n\n\n\nAbove snippet is intended to convey the basic idea of specifying\nthe DAG using properties file. Operators would come from a predefined\nlibrary and referenced in the specification by class name and port names\n(obtained from the library providers documentation or runtime\nintrospection by tools). For those interested in details, see later\nsections and refer to the  Operation and\nInstallation Guide\u00a0mentioned above.\n\n\nAttributes\n\n\nAttributes impact the runtime behavior of the application. They do\nnot impact the functionality. An example of an attribute is application\nname. Setting it changes the application name. Another example is\nstreaming window size. Setting it changes the streaming window size from\nthe default value to the specified value. Users cannot add new\nattributes, they can only choose from the ones that come packaged and\npre-supported by the platform. Details of attributes are covered in the\n Operation and Installation\nGuide.\n\n\nOperators\n\n\nOperators\u00a0are basic compute units.\nOperators process each incoming tuple and emit zero or more tuples on\noutput ports as per the business logic. The data flow, connectivity,\nfault tolerance (node outage), etc. is taken care of by the platform. As\nan operator developer, all that is needed is to figure out what to do\nwith the incoming tuple and when (and which output port) to send out a\nparticular output tuple. Correctly designed operators will most likely\nget reused. Operator design needs care and foresight. For details, refer\nto the  \nOperator Developer Guide\n. As an application developer you need to connect operators\nin a way that it implements your business logic. You may also require\noperator customization for functionality and use attributes for\nperformance/scalability etc.\n\n\nAll operators process tuples asynchronously in a distributed\ncluster. An operator cannot assume or predict the exact time a tuple\nthat it emitted will get consumed by a downstream operator. An operator\nalso cannot predict the exact time when a tuple arrives from an upstream\noperator. The only guarantee is that the upstream operators are\nprocessing the current or a future window, i.e. the windowId of upstream\noperator is equals or exceeds its own windowId. Conversely the windowId\nof a downstream operator is less than or equals its own windowId. The\nend of a window operation, i.e. the API call to endWindow on an operator\nrequires that all upstream operators have finished processing this\nwindow. This means that completion of processing a window propagates in\na blocking fashion through an operator. Later sections provides more\ndetails on streams and data flow of tuples.\n\n\nEach operator has a unique name within the DAG as provided by the\nuser. This is the name of the operator in the logical plan. The name of\nthe operator in the physical plan is an integer assigned to it by STRAM.\nThese integers are use the sequence from 1 to N, where N is total number\nof physically unique operators in the DAG. \u00a0Following the same rule,\neach partitioned instance of a logical operator has its own integer as\nan id. This id along with the Hadoop container name uniquely identifies\nthe operator in the execution plan of the DAG. The logical names and the\nphysical names are required for web service support. Operators can be\naccessed via both names. These same names are used while interacting\nwith apex cli\u00a0to access an operator.\nIdeally these names should be self-descriptive. For example in Figure 1,\nthe node named \u201cDaily volume\u201d has a physical identifier of 2.\n\n\nOperator Interface\n\n\nOperator interface in a DAG consists of ports,\u00a0properties,\u00a0and attributes.\nOperators interact with other components of the DAG via ports. Functional behavior of the operators\ncan be customized via parameters. Run time performance and physical\ninstantiation is controlled by attributes. Ports and parameters are\nfields (variables) of the Operator class/object, while attributes are\nmeta information that is attached to the operator object via an\nAttributeMap. An operator must have at least one port. Properties are\noptional. Attributes are provided by the platform and always have a\ndefault value that enables normal functioning of operators.\n\n\nPorts\n\n\nPorts are connection points by which an operator receives and\nemits tuples. These should be transient objects instantiated in the\noperator object, that implement particular interfaces. Ports should be\ntransient as they contain no state. They have a pre-defined schema and\ncan only be connected to other ports with the same schema. An input port\nneeds to implement the interface  Operator.InputPort\u00a0and\ninterface Sink. A default\nimplementation of these is provided by the abstract class DefaultInputPort. An output port needs to\nimplement the interface  Operator.OutputPort. A default implementation\nof this is provided by the concrete class DefaultOutputPort. These two are a quick way to\nimplement the above interfaces, but operator developers have the option\nof providing their own implementations.\n\n\nHere are examples of an input and an output port from the operator\nSum.\n\n\n@InputPortFieldAnnotation(name = \ndata\n)\npublic final transient DefaultInputPort\nV\n data = new DefaultInputPort\nV\n() {\n  @Override\n  public void process(V tuple)\n  {\n    ...\n  }\n}\n@OutputPortFieldAnnotation(optional=true)\npublic final transient DefaultOutputPort\nV\n sum = new DefaultOutputPort\nV\n(){ \u2026 };\n\n\n\n\nThe process call is in the Sink interface. An emit on an output\nport is done via emit(tuple) call. For the above example it would be\nsum.emit(t), where the type of t is the generic parameter V.\n\n\nThere is no limit on how many ports an operator can have. However\nany operator must have at least one port. An operator with only one port\nis called an Input Adapter if it has no input port and an Output Adapter\nif it has no output port. These are special operators needed to get/read\ndata from outside system/source into the application, or push/write data\ninto an outside system/sink. These could be in Hadoop or outside of\nHadoop. These two operators are in essence gateways for the streaming\napplication to communicate with systems outside the application.\n\n\nPort connectivity can be validated during compile time by adding\nPortFieldAnnotations shown above. By default all ports have to be\nconnected, to allow a port to go unconnected, you need to add\n\u201coptional=true\u201d to the annotation.\n\n\nAttributes can be specified for ports that affect the runtime\nbehavior. An example of an attribute is parallel partition that specifes\na parallel computation flow per partition. It is described in detail in\nthe Parallel Partitions section. Another example is queue capacity that specifies the buffer size for the\nport. Details of attributes are covered in  Operation and Installation Guide.\n\n\nProperties\n\n\nProperties are the abstractions by which functional behavior of an\noperator can be customized. They should be non-transient objects\ninstantiated in the operator object. They need to be non-transient since\nthey are part of the operator state and re-construction of the operator\nobject from its checkpointed state must restore the operator to the\ndesired state. Properties are optional, i.e. an operator may or may not\nhave properties; they are part of user code and their values are not\ninterpreted by the platform in any way.\n\n\nAll non-serializable objects should be declared transient.\nExamples include sockets, session information, etc. These objects should\nbe initialized during setup call, which is called every time the\noperator is initialized.\n\n\nAttributes\n\n\nAttributes are values assigned to the operators that impact\nrun-time. This includes things like the number of partitions, at most\nonce or at least once or exactly once recovery modes, etc. Attributes do\nnot impact functionality of the operator. Users can change certain\nattributes in runtime. Users cannot add attributes to operators; they\nare pre-defined by the platform. They are interpreted by the platform\nand thus cannot be defined in user created code (like properties).\nDetails of attributes are covered in  \nConfiguration Guide\n.\n\n\nOperator State\n\n\nThe state of an operator is defined as the data that it transfers\nfrom one window to a future window. Since the computing model of the\nplatform is to treat windows like micro-batches, the operator state can\nbe checkpointed every Nth window, or every T units of time, where T is significantly greater\nthan the streaming window. \u00a0When an operator is checkpointed, the entire\nobject is written to HDFS. \u00a0The larger the amount of state in an\noperator, the longer it takes to recover from a failure. A stateless\noperator can recover much quicker than a stateful one. The needed\nwindows are preserved by the upstream buffer server and are used to\nrecompute the lost windows, and also rebuild the buffer server in the\ncurrent container.\n\n\nThe distinction between Stateless and Stateful is based solely on\nthe need to transfer data in the operator from one window to the next.\nThe state of an operator is independent of the number of ports.\n\n\nStateless\n\n\nA Stateless operator is defined as one where no data is needed to\nbe kept at the end of every window. This means that all the computations\nof a window can be derived from all the tuples the operator receives\nwithin that window. This guarantees that the output of any window can be\nreconstructed by simply replaying the tuples that arrived in that\nwindow. Stateless operators are more efficient in terms of fault\ntolerance, and cost to achieve SLA.\n\n\nStateful\n\n\nA Stateful operator is defined as one where data is needed to be\nstored at the end of a window for computations occurring in later\nwindow; a common example is the computation of a sum of values in the\ninput tuples.\n\n\nOperator API\n\n\nThe Operator API consists of methods that operator developers may\nneed to override. In this section we will discuss the Operator APIs from\nthe point of view of an application developer. Knowledge of how an\noperator works internally is critical for writing an application. Those\ninterested in the details should refer to  Malhar Operator Developer Guide.\n\n\nThe APIs are available in three modes, namely Single Streaming\nWindow, Sliding Application Window, and Aggregate Application Window.\nThese are not mutually exclusive, i.e. an operator can use single\nstreaming window as well as sliding application window. A physical\ninstance of an operator is always processing tuples from a single\nwindow. The processing of tuples is guaranteed to be sequential, no\nmatter which input port the tuples arrive on.\n\n\nIn the later part of this section we will evaluate three common\nuses of streaming windows by applications. They have different\ncharacteristics and implications on optimization and recovery mechanisms\n(i.e. algorithm used to recover a node after outage) as discussed later\nin the section.\n\n\nStreaming Window\n\n\nStreaming window is atomic micro-batch computation period. The API\nmethods relating to a streaming window are as follows\n\n\npublic void process(\ntuple_type\n tuple) // Called on the input port on which the tuple arrives\npublic void beginWindow(long windowId) // Called at the start of the window as soon as the first begin_window tuple arrives\npublic void endWindow() // Called at the end of the window after end_window tuples arrive on all input ports\npublic void setup(OperatorContext context) // Called once during initialization of the operator\npublic void teardown() // Called once when the operator is being shutdown\n\n\n\n\nA tuple can be emitted in any of the three streaming run-time\ncalls, namely beginWindow, process, and endWindow but not in setup or\nteardown.\n\n\nAggregate Application Window\n\n\nAn operator with an aggregate window is stateful within the\napplication window timeframe and possibly stateless at the end of that\napplication window. An size of an aggregate application window is an\noperator attribute and is defined as a multiple of the streaming window\nsize. The platform recognizes this attribute and optimizes the operator.\nThe beginWindow, and endWindow calls are not invoked for those streaming\nwindows that do not align with the application window. For example in\ncase of streaming window of 0.5 second and application window of 5\nminute, an application window spans 600 streaming windows (5*60*2 =\n600). At the start of the sequence of these 600 atomic streaming\nwindows, a beginWindow gets invoked, and at the end of these 600\nstreaming windows an endWindow gets invoked. All the intermediate\nstreaming windows do not invoke beginWindow or endWindow. Bookkeeping,\nnode recovery, stats, UI, etc. continue to work off streaming windows.\nFor example if operators are being checkpointed say on an average every\n30th window, then the above application window would have about 20\ncheckpoints.\n\n\nSliding Application Window\n\n\nA sliding window is computations that requires previous N\nstreaming windows. After each streaming window the Nth past window is\ndropped and the new window is added to the computation. An operator with\nsliding window is a stateful operator at end of any window. The sliding\nwindow period is an attribute and is a multiple of streaming window. The\nplatform recognizes this attribute and leverages it during bookkeeping.\nA sliding aggregate window with tolerance to data loss does not have a\nvery high bookkeeping cost. The cost of all three recovery mechanisms,\n at most once\u00a0(data loss tolerant),\nat least once\u00a0(data loss\nintolerant), and exactly once\u00a0(data\nloss intolerant and no extra computations) is same as recovery\nmechanisms based on streaming window. STRAM is not able to leverage this\noperator for any extra optimization.\n\n\nSingle vs Multi-Input Operator\n\n\nA single-input operator by definition has a single upstream\noperator, since there can only be one writing port for a stream. \u00a0If an\noperator has a single upstream operator, then the beginWindow on the\nupstream also blocks the beginWindow of the single-input operator. For\nan operator to start processing any window at least one upstream\noperator has to start processing that window. A multi-input operator\nreads from more than one upstream ports. Such an operator would start\nprocessing as soon as the first begin_window event arrives. However the\nwindow would not close (i.e. invoke endWindow) till all ports receive\nend_window events for that windowId. Thus the end of a window is a\nblocking event. As we saw earlier, a multi-input operator is also the\npoint in the DAG where windows of all upstream operators are\nsynchronized. The windows (atomic micro-batches) from a faster (or just\nahead in processing) upstream operators are queued up till the slower\nupstream operator catches up. STRAM monitors such bottlenecks and takes\ncorrective actions. The platform ensures minimal delay, i.e processing\nstarts as long as at least one upstream operator has started\nprocessing.\n\n\nRecovery Mechanisms\n\n\nApplication developers can set any of the recovery mechanisms\nbelow to deal with node outage. In general, the cost of recovery depends\non the state of the operator, while data integrity is dependant on the\napplication. The mechanisms are per window as the platform treats\nwindows as atomic compute units. Three recovery mechanisms are\nsupported, namely\n\n\n\n\nAt-least-once: All atomic batches are processed at least once.\n    No data loss occurs.\n\n\nAt-most-once: All atomic batches are processed at most once.\n    Data loss is possible; this is the most efficient setting.\n\n\nExactly-once: All atomic batches are processed exactly once.\n    No data loss occurs; this is the least efficient setting since\n    additional work is needed to ensure proper semantics.\n\n\n\n\nAt-least-once is the default. During a recovery event, the\noperator connects to the upstream buffer server and asks for windows to\nbe replayed. At-least-once and exactly-once mechanisms start from its\ncheckpointed state. At-most-once starts from the next begin-window\nevent.\n\n\nRecovery mechanisms can be specified per Operator while writing\nthe application as shown below.\n\n\nOperator o = dag.addOperator(\u201coperator\u201d, \u2026);\ndag.setAttribute(o,  OperatorContext.PROCESSING_MODE,  ProcessingMode.AT_MOST_ONCE);\n\n\n\n\nAlso note that once an operator is attributed to AT_MOST_ONCE,\nall the operators downstream to it have to be AT_MOST_ONCE. The client\nwill give appropriate warnings or errors if that\u2019s not the case.\n\n\nDetails are explained in the chapter on Fault Tolerance below.\n\n\nStreams\n\n\nA stream\u00a0is a connector\n(edge) abstraction, and is a fundamental building block of the platform.\nA stream consists of tuples that flow from one port (called the\noutput\u00a0port) to one or more ports\non other operators (called  input\u00a0ports) another -- so note a potentially\nconfusing aspect of this terminology: tuples enter a stream through its\noutput port and leave via one or more input ports. A stream has the\nfollowing characteristics\n\n\n\n\nTuples are always delivered in the same order in which they\n    were emitted.\n\n\nConsists of a sequence of windows one after another. Each\n    window being a collection of in-order tuples.\n\n\nA stream that connects two containers passes through a\n    buffer server.\n\n\nAll streams can be persisted (by default in HDFS).\n\n\nExactly one output port writes to the stream.\n\n\nCan be read by one or more input ports.\n\n\nConnects operators within an application, not outside\n    an application.\n\n\nHas an unique name within an application.\n\n\nHas attributes which act as hints to STRAM.\n\n\n\n\nStreams have four modes, namely in-line, in-node, in-rack,\n    and other. Modes may be overruled (for example due to lack\n    of containers). They are defined as follows:\n\n\n\n\nTHREAD_LOCAL: In the same thread, uses thread\n    stack (intra-thread). This mode can only be used for a downstream\n    operator which has only one input port connected; also called\n    in-line.\n\n\nCONTAINER_LOCAL: In the same container (intra-process); also\n    called in-container.\n\n\nNODE_LOCAL: In the same Hadoop node (inter processes, skips\n    NIC); also called in-node.\n\n\nRACK_LOCAL: On nodes in the same rack; also called\n    in-rack.\n\n\nunspecified: No guarantee. Could be anywhere within the\n    cluster\n\n\n\n\n\n\n\n\nAn example of a stream declaration is given below\n\n\nDAG dag = new DAG();\n \u2026\ndag.addStream(\nviews\n, viewAggregate.sum, cost.data).setLocality(CONTAINER_LOCAL); // A container local  stream\ndag.addStream(\u201cclicks\u201d, clickAggregate.sum, rev.data); // An example of unspecified locality\n\n\n\n\nThe platform guarantees in-order delivery of tuples in a stream.\nSTRAM views each stream as collection of ordered windows. Since no tuple\ncan exist outside a window, a replay of a stream consists of replay of a\nset of windows. When multiple input ports read the same stream, the\nexecution plan of a stream ensures that each input port is logically not\nblocked by the reading of another input port. The schema of a stream is\nsame as the schema of the tuple.\n\n\nIn a stream all tuples emitted by an operator in a window belong\nto that window. A replay of this window would consists of an in-order\nreplay of all the tuples. Thus the tuple order within a stream is\nguaranteed. However since an operator may receive multiple streams (for\nexample an operator with two input ports), the order of arrival of two\ntuples belonging to different streams is not guaranteed. In general in\nan asynchronous distributed architecture this is expected. Thus the\noperator (specially one with multiple input ports) should not depend on\nthe tuple order from two streams. One way to cope with this\nindeterminate order, if necessary, is to wait to get all the tuples of a\nwindow and emit results in endWindow call. All operator templates\nprovided as part of Malhar operator library follow these principles.\n\n\nA logical stream gets partitioned into physical streams each\nconnecting the partition to the upstream operator. If two different\nattributes are needed on the same stream, it should be split using\nStreamDuplicator\u00a0operator.\n\n\nModes of the streams are critical for performance. An in-line\nstream is the most optimal as it simply delivers the tuple as-is without\nserialization-deserialization. Streams should be marked\ncontainer_local, specially in case where there is a large tuple volume\nbetween two operators which then on drops significantly. Since the\nsetLocality call merely provides a hint, STRAM may ignore it. An In-node\nstream is not as efficient as an in-line one, but it is clearly better\nthan going off-node since it still avoids the potential bottleneck of\nthe network card.\n\n\nTHREAD_LOCAL and CONTAINER_LOCAL streams do not use a buffer\nserver as this stream is in a single process. The other two do.\n\n\nAffinity Rules\n\n\nAffinity Rules in Apex provide a way to specify hints on how operators should be deployed in a cluster. Sometimes you may want to allocate certain operators on the same or different nodes for performance or other reasons. Affinity rules can be used in such cases to make sure these considerations are honored by platform.\n\n\nThere can be two types of rules: Affinity and anti-affinity rules. Affinity rule indicates that the group of operators in the rule should be allocated together.  On the other hand, anti-affinity rule indicates that the group of operators should be allocated separately.\n\n\nSpecifying Affinity Rules\n\n\nA list of Affinity Rules can be specified for an application by setting attribute: DAGContext.AFFINITY_RULES_SET.\nHere is an example for setting Affinity rules for an application in the populateDag method:\n\n\nAffinityRulesSet ruleSet = new AffinityRulesSet();\nList\nAffinityRule\n rules = new ArrayList\n();\n// Add Affinity rules as per requirement\nrules.add(new AffinityRule(Type.ANTI_AFFINITY, Locality.NODE_LOCAL, false, \nrand\n, \noperator1\n, \noperator2\n));\nrules.add(new AffinityRule(Type.AFFINITY, Locality.CONTAINER_LOCAL, false, \nconsole\n, \nrand\n));\nruleSet.setAffinityRules(rules);\ndag.setAttribute(DAGContext.AFFINITY_RULES_SET, ruleSet);\n\n\n\n\nAs shown in the example above, each rule has a type AFFINITY or ANTI_AFFINITY indicating whether operators in the group should be allocated together or separate. These can be applied on any operators in DAG.\n\n\nThe operators for rule can be provided either as a list of 2 or more operator names or as a regular expression. The regex should match at least two operators in DAG to be considered a valid rule. Here is example of rule with regex to allocate all the operators in DAG on the same node:\n\n\n// Allocate all operators starting with console on same container\nrules.add(new AffinityRule(Type.AFFINITY, \n*\n , Locality.NODE_LOCAL, false));\n\n\n\n\nLikewise, operators for rule can also be added as a list:\n\n\n// Rule for Operators rand, operator1 and operator2 should not be allocated on same node\nrules.add(new AffinityRule(Type.ANTI_AFFINITY, Locality.NODE_LOCAL, false, \nrand\n, \noperator1\n, \noperator2\n));\n\n\n\n\nTo indicate affinity or anti-affinity between partitions of a single operator, list should contain the same operator name twice, as shown in the example below for  'TestOperator'. This will ensure that platform will allocate physical partitions of this operator on different nodes.\n\n\n// Rule for Partitions of TestOperator should not be allocated on the same node\nrules.add(new AffinityRule(Type.ANTI_AFFINITY, Locality.NODE_LOCAL, false, \nTestOperator\n, \nTestOperator\n));\n\n\n\n\nAnother important parameter to indicate affinity rule is the locality constraint. Similar to stream locality, the affinity rule will be applied at either THREAD_LOCAL, CONTAINER_LOCAL or NODE_LOCAL level. Support for RACK_LOCAL is not added yet.\n\n\nThe last configurable parameter for affinity rules is strict or preferred rule. A true value for this parameter indicates that the rule should be relaxed in case sufficient resources are not available.\n\n\nSpecifying affinity rules from properties\n\n\nThe same set of rules can also be added from properties.xml by setting value for attribute DAGContext.AFFINITY_RULES_SET as JSON string.  For example:\n\n\nproperty\n\n    \nname\napex.application.AffinityRulesSampleApplication.attr.AFFINITY_RULES_SET\n/name\n\n    \nvalue\n\n    {\n      \naffinityRules\n: [\n        {\n          \noperatorRegex\n: \nconsole*\n,\n          \nlocality\n: \nCONTAINER_LOCAL\n,\n          \ntype\n: \nAFFINITY\n,\n          \nrelaxLocality\n: false\n        },\n        {\n         \noperatorsList\n: [\n          \nrand\n,\n          \npassThru\n\n          ],\n          \nlocality\n: \nNODE_LOCAL\n,\n          \ntype\n: \nANTI_AFFINITY\n,\n          \nrelaxLocality\n: false\n        },\n        {\n          \noperatorsList\n: [\n          \npassThru\n,\n          \npassThru\n\n          ],\n          \nlocality\n: \nNODE_LOCAL\n,\n          \ntype\n: \nANTI_AFFINITY\n,\n          \nrelaxLocality\n: false\n        }\n      ]\n    }\n\n/property\n\n\n\n\n\nAffinity rules which conflict with stream locality or host preference are validated during DAG validation phase.\n\n\nValidating an Application\n\n\nThe platform provides various ways of validating the application\nspecification and data input. An understanding of these checks is very\nimportant for an application developer since it affects productivity.\nValidation of an application is done in three phases, namely\n\n\n\n\nCompile Time: Caught during application development, and is\n    most cost effective. These checks are mainly done on declarative\n    objects and leverages the Java compiler. An example is checking that\n    the schemas specified on all ports of a stream are\n    mutually compatible.\n\n\nInitialization Time: When the application is being\n    initialized, before submitting to Hadoop. These checks are related\n    to configuration/context of an application, and are done by the\n    logical DAG builder implementation. An example is the checking that\n    all non-optional ports are connected to other ports.\n\n\nRun Time: Validations done when the application is running.\n    This is the costliest of all checks. These are checks that can only\n    be done at runtime as they involve data. For example divide by 0\n    check as part of business logic.\n\n\n\n\nCompile Time\n\n\nCompile time validations apply when an application is specified in\nJava code and include all checks that can be done by Java compiler in\nthe development environment (including IDEs like NetBeans or Eclipse).\nExamples include\n\n\n\n\nSchema Validation: The tuples on ports are POJO (plain old\n    java objects) and compiler checks to ensure that all the ports on a\n    stream have the same schema.\n\n\nStream Check: Single Output Port and at least one Input port\n    per stream. A stream can only have one output port writer. This is\n    part of the addStream api. This\n    check ensures that developers only connect one output port to\n    a stream. The same signature also ensures that there is at least one\n    input port for a stream\n\n\nNaming: Compile time checks ensures that applications\n    components operators, streams are named\n\n\n\n\nInitialization/Instantiation Time\n\n\nInitialization time validations include various checks that are\ndone post compile, and before the application starts running in a\ncluster (or local mode). These are mainly configuration/contextual in\nnature. These checks are as critical to proper functionality of the\napplication as the compile time validations.\n\n\nExamples include\n\n\n\n\n\n\nJavaBeans Validation\n:\n    Examples include\n\n\n\n\n@Max(): Value must be less than or equal to the number\n\n\n@Min(): Value must be greater than or equal to the\n    number\n\n\n@NotNull: The value of the field or property must not be\n    null\n\n\n@Pattern(regexp = \u201c....\u201d): Value must match the regular\n    expression\n\n\nInput port connectivity: By default, every non-optional input\n    port must be connected. A port can be declared optional by using an\n    annotation: \u00a0 \u00a0 @InputPortFieldAnnotation(name = \"...\", optional\n    = true)\n\n\nOutput Port Connectivity: Similar. The annotation here is: \u00a0 \u00a0\n    @OutputPortFieldAnnotation(name = \"...\", optional = true)\n\n\n\n\n\n\n\n\nUnique names in application scope: Operators, streams, must have\n    unique names.\n\n\n\n\nCycles in the dag: DAG cannot have a cycle.\n\n\nUnique names in operator scope: Ports, properties, annotations\n    must have unique names.\n\n\nOne stream per port: A port can connect to only one stream.\n    This check applies to input as well as output ports even though an\n    output port can technically write to two streams. If you must have\n    two streams originating from a single output port, use \u00a0a\u00a0streamDuplicator operator.\n\n\nApplication Window Period: Has to be an integral multiple the\n    streaming window period.\n\n\n\n\nRun Time\n\n\nRun time checks are those that are done when the application is\nrunning. The real-time streaming platform provides rich run time error\nhandling mechanisms. The checks are exclusively done by the application\nbusiness logic, but the platform allows applications to count and audit\nthese. Some of these features are in the process of development (backend\nand UI) and this section will be updated as they are developed. Upon\ncompletion examples will be added to demos to illustrate these.\n\n\nError ports are output ports with error annotations. Since they\nare normal ports, they can be monitored and tuples counted, persisted\nand counts shown in the UI.\n\n\n\n\nMulti-Tenancy and Security\n\n\nHadoop is a multi-tenant distributed operating system. Security is\nan intrinsic element of multi-tenancy as without it a cluster cannot be\nreasonably be shared among enterprise applications. Streaming\napplications follow all multi-tenancy security models used in Hadoop as\nthey are native Hadoop applications.\n\n\nSecurity\n\n\nThe platform includes Kerberos support. Both access points, namely\nSTRAM and Bufferserver are secure. STRAM passes the token over to\nStreamingContainer, which then gives it to the Bufferserver. The most\nimportant aspect for an application developer is to note that STRAM is\nthe single point of access to ensure security measures are taken by all\ncomponents of the platform.\n\n\nResource Limits\n\n\nHadoop enforces quotas on resources. This includes hard-disk (name\nspace and total disk quota) as well as priority queues for schedulers.\nThe platform uses Hadoop resource limits to manage a streaming\napplication. In addition network I/O quotas can be enforced. An operator\ncan be dynamically partitioned if it reaches its resource limits; these\nlimits may be expressed in terms of throughput, latency, or just\naggregate resource utilization of a container.\n\n\n\n\nScalability and Partitioning\n\n\nScalability is a foundational element of this platform and is a\nbuilding block for an eco-system where big-data meets real-time.\nEnterprises need to continually meet SLA as data grows. Without the\nability to scale as load grows, or new applications with higher loads\ncome to fruition, enterprise grade SLA cannot be met. A big issue with\nthe streaming application space is that, it is not just about high load,\nbut also the fluctuations in it. There is no way to guarantee future\nload requirements and there is a big difference between high and low\nload within a day for the same feed. Traditional streaming platforms\nsolve these two cases by simply throwing more hardware at the\nproblem.\n\n\nDaily spikes are managed by ensuring enough hardware for peak\nload, which then idles during low load, and future needs are handled by\na very costly re-architecture, or investing heavily in building a\nscalable distributed operating system. Another salient and often\noverlooked cost is the need to manage SLA -- let\u2019s call it  buffer capacity. Since this means computing the\npeak load within required time, that translates to allocating enough\nresources over and above peak load as daily peaks fluctuate. For example\nan average peak load of 100 resource units (cpu and/or memory and/or\nnetwork) may mean allocating about 200 resource units to be safe. A\ndistributed cluster that cannot dynamically scale up and down, in effect\npays buffer capacity per application. Another big aspect of streaming\napplications is that the load is not just ingestion rate, more often\nthan not, the internal operators produce lot more events than the\ningestion rate. For example a dimensional data (with, say  d\u00a0dimensions) computation needs 2*d -1\u00a0computations per ingested event. A lot\nof applications have over 10 dimensions, i.e over 1000 computations per\nincoming event and these need to be distributed across the cluster,\nthereby causing an explosion in the throughput (events/sec) that needs\nto be managed.\n\n\nThe platform is designed to handle such cases at a very low cost.\nThe platform scales linearly with Hadoop. If applications need more\nresources, the enterprise can simply add more commodity nodes to Hadoop\nwithout any downtime, and the Hadoop native platform will take care of\nthe rest. If some nodes go bad, these can be removed without downtime.\nThe daily peaks and valleys in the load are managed by the platform by\ndynamically scaling at the peak and then giving the resources back to\nHadoop during low load. This means that a properly designed Hadoop\ncluster does several things for enterprises: (a) reduces the cost of\nhardware due to use of commodity hardware (b) shares buffer capacity\nacross all applications as peaks of all applications may not align and\n(c) raises the average CPU usage on a 24x7 basis. As a general design\nthis is similar to scale that a map-reduce application can deliver. In\nthe following sections of this chapter we will see how this is\ndone.\n\n\nPartitioning\n\n\nIf all tuples sent through the stream(s) that are connected to the\ninput port(s) of an operator in the DAG are received by a single\nphysical instance of that operator, that operator can become a\nperformance bottleneck. This leads to scalability issues when\nthroughput, memory, or CPU needs exceed the processing capacity of that\nsingle instance.\n\n\nTo address the problem, the platform offers the capability to\npartition the inflow of data so that it is divided across multiple\nphysical instances of a logical operator in the DAG. There are two\nfunctional ways to partition\n\n\n\n\nLoad balance: Incoming load is simply partitioned\n    into stream(s) that go to separate instances of physical operators\n    and scalability is achieved via adding more physical operators. Each\n    tuple is sent to physical operator (partition) based on a\n    round-robin or other similar algorithm. This scheme scales linearly.\n    A lot of key based computations can load balance in the platform due\n    to the ability to insert  Unifiers. For many computations, the\n    endWindow and Unifier setup is similar to the combiner and reducer\n    mechanism in a Map-Reduce computation.\n\n\nSticky Key: The key assertion is that distribution of tuples\n    are sticky, i.e the data with\n    same key will always be processed by the same physical operator, no\n    matter how many times it is sent through the stream. This stickiness\n    will continue even if the number of partitions grows dynamically and\n    can eventually be leveraged for advanced features like\n    bucket testing. How this is accomplished and what is required to\n    develop compliant operators will be explained below.\n\n\n\n\nWe plan to add more partitioning mechanisms proactively to the\nplatform over time as needed by emerging usage patterns. The aim is to\nallow enterprises to be able to focus on their business logic, and\nsignificantly reduce the cost of operability. As an enabling technology\nfor managing high loads, this platform provides enterprises with a\nsignificant innovative edge. Scalability and Partitioning is a\nfoundational building block for this platform.\n\n\nSticky Partition vs Round Robin\n\n\nAs noted above, partitioning via sticky key is data aware but\nround-robin partitioning is not. An example for non-sticky load\nbalancing would be round robin distribution over multiple instances,\nwhere for example a tuple stream of  A, A,\nA with 3 physical operator\ninstances would result in processing of a single A by each of the instances, In contrast, sticky\npartitioning means that exactly one instance of the operators will\nprocess all of the  Atuples if they\nfall into the same bucket, while B\nmay be processed by another operator. Data aware mapping of\ntuples to partitions (similar to distributed hash table) is accomplished\nvia Stream Codecs. In later sections we would show how these two\napproaches can be used in combination.\n\n\nStream Codec\n\n\nThe platform does not make assumptions about the tuple\ntype, it could be any Java object. The operator developer knows what\ntuple type an input port expects and is capable of processing. Each\ninput port has a stream codec \u00a0associated that defines how data is serialized when transmitted over a socket\nstream; it also defines another\nfunction that computes the partition hash key for the tuple. The engine\nuses that key to determine which physical instance(s) \u00a0(for a\npartitioned operator) receive that \u00a0tuple. For this to work, consistent hashing is required.\nThe default codec uses the Java Object#hashCode function, which is\nsufficient for basic types such as Integer, String etc. It will also\nwork with custom tuple classes as long as they implement hashCode\nappropriately. Reliance on hashCode may not work when generic containers\nare used that do not hash the actual data, such as standard collection\nclasses (HashMap etc.), in which case a custom stream codec must be\nassigned to the input port.\n\n\nStatic Partitioning\n\n\nDAG designers can specify at design time how they would like\ncertain operators to be partitioned. STRAM then instantiates the DAG\nwith the physical plan which adheres to the partitioning scheme defined\nby the design. This plan is the initial partition of the application. In\nother words, Static Partitioning is used to tell STRAM to compute the\nphysical DAG from a logical DAG once, without taking into consideration\nruntime states or loads of various operators.\n\n\nDynamic Partitioning\n\n\nIn streaming applications the load changes during the day, thus\ncreating situations where the number of partitioned operator instances\nneeds to adjust dynamically. The load can be measured in terms of\nprocessing within the DAG based on throughput, or latency, or\nconsiderations in external system components (time based etc.) that the\nplatform may not be aware of. Whatever the trigger, the resource\nrequirement for the current processing needs to be adjusted at run-time.\nThe platform may detect that operator instances are over or under\nutilized and may need to dynamically adjust the number of instances on\nthe fly. More instances of a logical operator may be required (partition\nsplit) or underutilized operator instances may need decommissioning\n(partition merge). We refer to either of the changes as dynamic\npartitioning. The default partitioning scheme supports split and merge\nof partitions, but without state transfer. The contract of the\nPartitioner\u00a0interface allows the operator\ndeveloper to implement split/merge and the associated state transfer, if\nnecessary.\n\n\nSince partitioning is a key scalability measure, our goal is to\nmake it as simple as possible without removing the flexibility needed\nfor sophisticated applications. Basic partitioning can be enabled at\ncompile time through the DAG specification. A slightly involved\npartitioning involves writing custom codecs to calculate data aware\npartitioning scheme. More complex partitioning cases may require users\nto provide a custom implementation of Partitioner, which gives the\ndeveloper full control over state transfer between multiple instances of\nthe partitioned operator.\n\n\nDefault Partitioning\n\n\nThe platform provides a default partitioning implementation that\ncan be enabled without implementing Partitioner\u00a0(or writing any other extra Java\ncode), which is designed to support simple sticky partitioning out of\nthe box for operators with logic agnostic to the partitioning scheme\nthat can be enabled by means of DAG construction alone.\n\n\nTypically an operator that can work with the default partitioning\nscheme would have a single input port. If there are multiple input\nports, only one port will be partitioned (the port first connected in\nthe DAG). The number of partitions will be calculated based on the\ninitial partition count - set as attribute on the operator in the DAG\n(if the attribute is not present, partitioning is off). Each partition\nwill handle tuples based on matching the lower bits of the hash code.\nFor example, if the tuple type was Integer and 2 partitions requested,\nall even numbers would go to one operator instance and all odd numbers\nto the other.\n\n\nDefault Dynamic Partitioning\n\n\nTriggering partition load evaluation and repartitioning action\nitself are separate concerns. Triggers are not specified further here,\nwe are planning to support it in a customizable fashion that, for\nexample, allows latency or SLA based implementations. Triggers calculate\na load indicator (signed number) that tells the framework that a given\npartition is either underutilized, operating normally within the\nexpected thresholds or overloaded and becoming a bottleneck. The\nindicator is then presented to the partitioning logic (default or custom\nimplementation of Partitioner) to provide the opportunity to make any\nneeded adjustments.\n\n\nThe default partitioning logic divides the key space\naccording to the lower bits of the hash codes that are generated by the\nstream codec, by assigning each partitioned operator instance via a bit\nmask and the respective value. For example, the operator may have\ninitially two partitions,  0and 1, each\nwith a bit mask of 1.\nIn the case where load evaluation flags partition\n0  as over utilized\n(most data tuples processed yield a hash code with lowest bit cleared),\napartition split\u00a0occurs, resulting in 00\nand  10with mask 11. Operator instance 0 will be replaced with 2 new instances and partition\n1  remains unchanged,\nresulting in three active partitions. The same process could repeat if\nmost tuples fall into the01 partition, leading to a split into 001  and101\nwith mask 111, etc.\n\n\nShould load decrease in two sibling partitions, a\npartition merge\u00a0could\nreverse the split, reducing the mask length and replacing two operators\nwith one. Should only one of two sibling partitions be underutilized,\n it cannot be\u00a0merged.\nInstead, the platform can attempt to deploy the affected operator\ninstance along with other operator instances for resource sharing\namongst underutilized partitions (not implemented yet). Keeping separate\noperator instances allows\u00a0us  to\npin load increases directly to the affected instance with a single\nspecific partition key, which would not be the case had we assigned a\nshared instance to handle multiple keys.\n\n\nNxM Partitions\n\n\nWhen two consecutive logical operators are partitioned a special\noptimization is done. Technically the output of the first operator\nshould be unified and streamed to the next logical node. But that can\ncreate a network bottleneck. The platform optimizes this by partitioning\nthe output stream of each partition of the first operator as per the\npartitions needed by the next operator. For example if the first\noperator has N partitions and the second operator has M partitions then\neach of the N partitions would send out M streams. The first of each of\nthese M streams would be unified and routed to the first of the M\npartitions, and so on. Such an optimization allows for higher\nscalability and eliminates a network bottleneck (one unifier in between\nthe two operators) by having M unifiers. This also enables the\napplication to perform within the resource limits enforced by YARN.\nSTRAM has a much better understanding and estimation of unifier resource\nneeds and is thus able to optimize for resource constraints.\n\n\nFigure 5 shows a case where we have a 3x2 partition; the single\nintermediate unifier between operator 1\u00a0and 2\u00a0is\noptimized away. The partition computation for operator  2\u00a0is executed on outbound streams of each\npartitions of operator 1. Each\npartition of operator 2\u00a0has its own\nCONTAINER_LOCAL unifier. In such a situation, the in-bound network\ntuple flow is split between containers for  2a\u00a0and 2b\u00a0each of which take half the traffic. STRAM\ndoes this by default since it always has better performance.\n\n\n\n\nParallel\n\n\nIn cases where all the downstream operators use the same\npartitioning scheme and the DAG is network bound an optimization called\nparallel partition\u00a0is very\neffective. In such a scenario all the downstream operators are also\npartitioned to create computation flow per partition. This optimization\nis extremely efficient for network bound streams, In some cases this\noptimization would also apply for CPU or RAM bounded\napplications.\n\n\nIn Figure 6a, operator 1\u00a0is\npartitioned into 1a\u00a0and\n1b. Both the downstream operators\n2\u00a0and  3\u00a0follow the same partition scheme as\n1, however the network I/O between\n1\u00a0and 2, and between 2\u00a0and  3\u00a0is\nhigh. Then users can decide to optimize using parallel partitions. This\nallows STRAM to completely skip the insertion of intermediate Unifier\noperators between 1 and 2 as well as between 2 and 3; a single unifier\njust before operator  4, is\nadequate by which time tuple flow volume is low.\n\n\nSince operator 4 has sufficient resources to manage the combined\noutput of multiple instances of operator 3, it need not be partitioned. A further\noptimization can be done by declaring operators  1, 2, and\n3\u00a0as THREAD_LOCAL (intra-thread)\nor CONTAINER_LOCAL (intra-process) or NODE_LOCAL (intra-node).\nParallel partition is not used by default, users have to specify it\nexplicitly via an attribute of the input port (reader) of the stream as\nshown below.\n\n\n\n\nThe following code shows an example of creating a parallel partition.\n\n\ndag.addStream(\nDenormalizedUserId\n, idAssigner.userid, uniqUserCount.data);\ndag.setInputPortAttribute(uniqUserCount.data, PortContext.PARTITION_PARALLEL, partitionParallel);\n\n\n\n\nParallel partitions can be used with other partitions, for example\na parallel partition could be sticky key or load balanced.\n\n\nParallel Partitions with Streams Modes\n\n\nParallel partitions can be further optimized if the parallel\npartitions are combined with streams being in-line or in-node or in-rack\nmode. This is very powerful feature and should be used if operators have\nvery high throughput within them and the outbound merge does an\naggregation. For example in Figure 6b, if operator 3 significantly\nreduces the throughput, which usually is a reason to do parallel\npartition, then making the streams in-line or in-node within nodes\n1-\n2 and 2-\n3 significantly impacts the performance.\n\n\nCONTAINER_LOCAL stream has high bandwidth, and can manage to\nconsume massive tuple count without taxing the NIC and networking stack.\nThe downside is that all operators (1,2,3) in this case need to be able\nto fit within the resource limits of CPU and memory enforced on a Hadoop\ncontainer. A way around this is to request RM to provide a big\ncontainer. On a highly used Hadoop grid, getting a bigger container may\nbe a problem, and operational complexities of managing a Hadoop cluster\nwith different container sizes may be higher. If THREAD_LOCAL or\nCONTAINER_LOCAL streams are needed to get the throughput, increasing\nthe partition count should be considered. In future STRAM may take this\ndecision automatically. Unless there is a very bad skew and sticky key\npartitioning is in use, the approach to partition till each container\nhas enough resources works well.\n\n\nA NODE_LOCAL stream has lower bandwidth compared to a\nCONTAINER_LOCAL stream, but it works well with the RM in terms of\nrespecting container size limits. A NODE_LOCAL parallel partition uses\nlocal loop back for streams and is much better than using NIC. Though\nNODE_LOCAL stream fits well with similar size containers, it does need\nRM to be able to deliver two containers on the same Hadoop node. On a\nheavily used Hadoop cluster, this may not always be possible. In future\nSTRAM would do these trade-offs automatically at run-time.\n\n\nA RACK_LOCAL stream has much lower bandwidth than NODE_LOCAL\nstream, as events go through the NIC. But it still is able to better\nmanage SLA and latency. Moreover RM has much better ability to give a\nrack local container as opposed to the other two.\n\n\nParallel partitions with CONTAINER_LOCAL streams can be done by\nsetting all the intermediate streams to CONTAINER_LOCAL. Parallel\npartitions with THREAD_LOCAL streams can be done by setting all the\nintermediate streams to THREAD_LOCAL. Platform supports the following\nvia attributes.\n\n\n\n\nParallel-Partition\n\n\nParallel-Partition with THREAD_LOCAL stream\n\n\nParallel-Partition with CONTAINER_LOCAL stream\n\n\nParallel-Partition with NODE_LOCAL stream\n\n\nParallel-Partition with RACK_LOCAL stream\n\n\n\n\nThese attributes would nevertheless be initial starting point and\nSTRAM can improve on them at run time.\n\n\n\n\nSkew Balancing Partition\n\n\nSkew balancing partition is useful to manage skews in the stream\nthat is load balanced using a sticky key. Incoming events may have a\nskew, and these may change depending on various factors like time of the\nday or other special circumstances. To manage the uneven load, users can\nset a limit on the ratio of maximum load on a partition to the minimum\nload on a partition. STRAM would use this to dynamically change the\npartitions. For example suppose there are 6 partitions, and the load\nhappens to be distributed as follows: one with 40%, and the rest with\n12% each. The ratio of maximum to minimum is 3.33. If the desired ratio\nis set to 2, STRAM would partition the first instance into two\npartitions, each with 20% load to bring the ratio down to the desired\nlevel. This will be tried repeatedly till partitions are balanced. The\ntime period between each attempt is controlled via an attribute to avoid\nrebalancing too frequently. As mentioned earlier, dynamic operations\ninclude both splitting a partition as well as merging partitions with\nlow load.\n\n\nFigure 7 shows an example of skew balancing partition. An example\nof 3x1 paritition is shown. Let's say that skew balance is kept at \u201cno\npartition to take up more than 50% load. If in runtime the load type\nchanges to create a skew. For example, consider an application in the US\nthat is processing a website clickstream. At night in the US, the\nmajority of accesses come from the Far East, while in the daytime it\ncomes from the Americas. Similarly, in the early morning, the majority\nof the accesses are from east coast of the US, with the skew shifting to\nthe west coast as the day progresses. Assume operator 1 is partitioned\ninto 1a, 1b, and 1c.\n\n\nLet's see what happens if the logical operator 1 gets into a 20%,\n20%, 60% skew as shown in Figure 7. This would trigger the skew\nbalancing partition. One example of attaining balance is to merge 1a,\nand 1b to get 1a+1b in a single partition to take the load to 40%; then\nsplit 1c into two partitions 1ca and 1cb to get 30% on each of them.\nThis way STRAM is able to get back to under 50% per partition. As a live\n24x7 application, this kind of skew partitioning can be applied several\ntimes in a day. Skew-balancing at runtime is a critical feature for SLA\ncompliance; it also enables cost savings. This partitioning scheme will\nbe available in later release.\n\n\n\n\nSkew Unifier Partition\n\n\nIn this section we would take a look at another way to balance the\nskew. This method is a little less disruptive, but is useful in\naggregate operators. Let us take the same example as in Figure 7 with\nskew 20%, 20%, and 60%. To manage the load we could have either worked\non rebalancing the partition, which involves a merge and split of\npartitions to get to a new distribution or by partitioning  only\u00a0the partition with the big skew. Since the\nbest way to manage skew is to load balance, if possible, this scheme\nattempts to do so. The method is less useful than the others we discusse\n-- the main reason being that if the developer has chosen a sticky key\npartition to start with, it is unlikely that a load balancing scheme can\nhelp. Assuming that it is worthwhile to load balance, a special\none-purpose unifier can be inserted for the skew partition. If the cause\nof resource bottleneck is not the I/O, specially the I/O into the\ndownstream operator, but is the compute (memory, CPU) power of a\npartition, it makes sense to split the skew partition without having to\nchange the in-bound I/O to the upstream operator.\n\n\nTo trigger this users can set a limit on the ratio of maximum load\non a partition to the minimum load on a partition, and ask to use this\nscheme. STRAM would use this to load balance.The time period between\neach attempt is controlled via the same attribute to avoid rebalancing\ntoo frequently.\n\n\nFigure 8 shows an example of skew load balancing partition with a\ndedicated unifier. The 20%, 20%, and 60% triggers the skew load\nbalancing partition with an unifier. Partition 1c would be split into\ntwo and it would get its own dedicated unifier. Ideally these two\nadditional partitions 1ca and 1cb will get 30% load. This way STRAM is\nable to get back to under 50% per partition. This scheme is very useful\nwhen the number of partitions is very high and we still have a bad\nskew.\n\n\nIn the steady state no physical partition is computing more than\n30% of the load. Memory and CPU resources are thus well distributed. The\nunifier that was inserted has to handle 60% of the load, distributed\nmore evenly, as opposed to the final unifier that had a 60% skew to\nmanage at a much higher total load. This partitioning scheme will be\navailable in later release.\n\n\n\n\nCascading Unifier\n\n\nLet's take the case of an upstream operator oprU\u00a0that connects to a downstream operator\noprD. Let's assume the application\nis set to scale oprU by load balancing. So this could be either Nx1 or\nNxM partitioning scheme. The upstream operator oprU scales by increasing\nN. An increase in the load triggers more resource needs (CPU, Memory, or\nI/O), which in turn triggers more containers and raises N, the\ndownstream node may be impacted in a lot of situations. In this section\nwe review a method to shield oprD from dynamic changes in the execution\nplan of oprU. On aggregate operators (Sum, Count, Max, Min, Range \u2026) it\nis better to do load balanced partitioning to avoid impact of skew. This\nworks very well as each partition emits tuples at the order of number of\nkeys (range) in the incoming stream per application window. But as N\ngrows the in-bound I/O to the unifier of oprU that runs in the container\nof oprD goes up proportionately as each upstream partition sends tuples\nof the order of unique keys (range). This means that the partitioning\nwould not scale linearly. The platform has mechanisms to manage this and\nget the scale back to being linear.\n\n\nCascading unifiers are implemented by inserting a series of\nintermediate unifiers before the final unifier in the container of oprD.\nSince each unifier guarantees that the outbound I/O would be in order of\nthe number of unique keys, the unifier in the oprD container can expect\nto achieve an upper limit on the inbound I/O. The problem is the same\nirrespective of the value of M (1 or more), wherein the amount of\ninbound I/O is proportional to N, not M. Figure 8 illustrates how\ncascading unifier works.\n\n\n\n\nFigure 8 shows an example where a 4x1 partition with single\nunifier is split into three 2x1 partitions to enable the final unifier\nin oprD container to get an upper limit on inbound I/O. This is useful\nto ensure that network I/O to containers is within limits, or within a\nlimit specified by users. The platform allows setting an upper limit of\nfan-in of the stream between oprU and oprD. Let's say that this is F (in\nthe figure F=2). STRAM would plan N/F (let's call it N1) containers,\neach with one unifier. The inbound fan-in to these unifiers is F. If N1\n\n F, another level of unifiers would be inserted. Let's say at some\npoint N/(F1*F2*...Fk) \n F, where K is the level of unifiers. The\noutbound I/O of each unifier is guaranteed to be under F, specially the\nunifier for oprD. This ensures that the application scales linearly as\nthe load grows. The downside is the additional latency imposed by each\nunifier level (a few milliseconds), but the SLA is maintained, and the\napplication is able to run within the resource limits imposed by YARN.\nThe value of F can be derived from any of the following\n\n\n\n\nI/O limit on containers to allow proper behavior in an\n    multi-tenant environment\n\n\nLoad on oprD instance\n\n\nBuffer server limits on fan-in, fan-out\n\n\nSize of reservoir buffer for inbound fan-in\n\n\n\n\nA more intriguing optimization comes when cascading unifiers are\ncombined with node-local execution plan, in which the bounds of two or\nmore containers are used and much higher local loopback limits are\nleveraged. In general the first level fan-in limit (F1) and the last\nstage fan-in limit (Fk) need not be same. In fact a much open and better\nleveraged execution plan may indeed have F1 != F2 != \u2026 != Fk, as Fk\ndetermines the fan-in for oprD, while F1, \u2026 Fk-1 are fan-ins for\nunifier-only containers. The platform will have these schemes in later\nversions.\n\n\nSLA\n\n\nA Service Level Agreement translates to guaranteeing that the\napplication would meet the requirements X% of the time. For example six\nsigma X is\u00a099.99966%. For\nreal-time streaming applications this translates to requirements for\nlatency, throughput, uptime, data loss etc. and that in turn indirectly\nleads to various resource requirements, recovery mechanisms, etc. The\nplatform is designed to handle these and features would be released in\nfuture as they get developed. At a top level, STRAM monitors throughput\nper operator, computes latency per operator, manages uptime and supports\nvarious recovery mechanisms to handle data loss. A lot of this decision\nmaking and algorithms will be customizable.\n\n\n\n\nFault Tolerance\n\n\nFault tolerance in the platform is defined as the ability to\nrecognize the outage of any part of the application, get resources,\nre-initialize the failed operators, and re-compute the lost data. The\ndefault method is to bring the affected part of the DAG \u00a0back to a known\n(checkpointed) state and recompute atomic micro batches from there on.\nThus the default is  at least\nonce\u00a0processing mode. An operator can be configured for\nat most once\u00a0recovery, in which\ncase the re-initialized operator starts from next available window; or\nfor exactly once\u00a0recovery, in which\ncase the operator only recomputes the window it was processing when the\noutage happened.\n\n\nState of the Application\n\n\nThe state of the application is traditionally defined as the state\nof all operators and streams at any given time. Monitoring state as\nevery tuple is processed asynchronously in a distributed environment\nbecomes a near impossible task, and cost paid to achieve it is very\nhigh. Consequently, in the platform, state is not saved per tuple, but\nrather at window boundaries. The platform treats windows as atomic micro\nbatches. The state saving task is delegated by STRAM to the individual\noperator or container. This ensures that the bookkeeping cost is very\nlow and works in a distributed way. Thus, the state of the application\nis defined as the collection of states of every operator and the set of\nall windows stored in the buffer server. This allows STRAM to rebuild\nany part of the application from the last saved state of the impacted\noperators and the windows retained by the buffer server. The state of an\noperator is intrinsically associated with a window id. Since operators\ncan override the default checkpointing period, operators may save state\nat the end of different windows. This works because the buffer server\nsaves all windows for as long as they are needed (state in the buffer\nserver is purged once STRAM determines that it is not longer needed\nbased on checkpointing in downstream operators).\n\n\nOperators can be stateless or stateful. A stateless operator\nretains no data between windows. All results of all computations done in\na window are emitted in that window. Variables in such an operator are\neither transient or are cleared by an end_window event. Such operators\nneed no state restoration after an outage. A stateful operator retains\ndata between windows and has data in checkpointed state. This data\n(state) is used for computation in future windows. Such an operator\nneeds its state restored after an outage. By default the platform\nassumes the operator is stateful. In order to optimize recovery (skip\nprocessing related to state recovery) for a stateless operator, the\noperator needs to be declared as stateless to STRAM. Operators can\nexplicitly mark themselves stateless via an annotation or an\nattribute.\n\n\nRecovery mechanisms are explained later in this section. Operator\ndevelopers have to ensure that there is no dependency on the order of\ntuples between two different streams. As mentioned earlier in this\ndocument, the platform guarantees in-order tuple delivery within a\nsingle stream, For operators with multiple input ports, a replay may\nresult in a different relative order of tuples among the different input\nports. If the output tuple computation is affected by this relative\norder, the operator may have to wait for the endWindow call (at which\npoint it would have seen all the tuples from all input ports in the\ncurrent window), perform order-dependent computations correctly and\nfinally, emit results.\n\n\nCheckpointing\n\n\nSTRAM provides checkpointing parameters to StreamingContainer\nduring initialization. A checkpoint period is given to the containers\nthat have the window generators. A control tuple is sent at the end of\ncheckpoint interval. This tuple traverses through the data path via\nstreams and triggers each StreamingContainer in the path to instrument a\ncheckpoint of the operator that receives this tuple. This ensures that\nall the operators checkpoint at exactly the same window boundary (except\nin those cases where a different checkpoint interval was configured for\nan operator by the user).\n\n\nThe only delay is the latency of the control tuple to reach all\nthe operators. Checkpoint is thus done between the endWindow call of a\nwindow and the beginWindow call of the next window. Since most operators\nare computing in parallel (with the exception of those connected by\nTHREAD_LOCAL streams) they each checkpoint as and when they are ready\nto process the \u201ccheckpoint\u201d control tuple. The asynchronous design of\nthe platform means that there is no guarantee that two operators would\ncheckpoint at exactly the same time, but there is a guarantee that by\ndefault they would checkpoint at the same window boundary. This feature\nalso ensures that purge of old data can be efficiently done: Once the\ncheckpoint window tuple is done traversing the DAG, the checkpoint state\nof the entire DAG increments to this window id at which point prior\ncheckpoint data can be discarded.\n\n\nIn case of an operator that has an application window size that is\nlarger than the size of the streaming window, the checkpointing by\ndefault still happens at same intervals as with other operators. To\nalign checkpointing with application window boundary, the application\ndeveloper should set the attribute \u201cCHECKPOINT_WINDOW_COUNT\u201d to\n\u201cAPPLICATION_WINDOW_COUNT\u201d. This ensures that the checkpoint happens\nat the  end\u00a0of the application\nwindow and not within\u00a0that window.\nSuch operators now treat the application window as an atomic computation\nunit. The downside is that it does need the upstream buffer server to\nkeep tuples for the entire application window.\n\n\nIf an operator is completely stateless, i.e. an outbound tuple is\nonly emitted in the process\u00a0call\nand only depends on the tuple of that call, there is no need to align\ncheckpointing with application window end. If the operator is stateful\nonly within a window, the operator developer should strongly consider\ncheckpointing only on the application window boundary.\n\n\nCheckpointing involves pausing an operator, serializing the state\nto persistent storage and then resuming the operator. Thus checkpointing\nhas a latency cost that can negatively affect computational throughput;\nto minimize that impact, it is important to ensure that checkpointing is\ndone with minimal required objects. This means, as mentioned earlier,\nall data that is not part of the operator state should be declared as\ntransient so that it is not persisted.\n\n\nAn operator developer can also create a stateless operator (marked\nwith the Stateless annotation). Stateless operators are not\ncheckpointed. Obviously, in such an operator, computation should not\ndepend on state from a previous window.\n\n\nThe serialized \u00a0state of an operator is stored as a file, and is\nthe state to which that the operator is restored if an outage happens\nbefore the next checkpoint. The id of the last completed window (per\noperator) is sent back to STRAM in the next heartbeat. The default\nimplementation for serialization uses KRYO. Multiple past checkpoints\nare kept per operator. Depending on the downstream checkpoint, one of\nthese are chosen for recovery. Checkpoints and buffer server state are\npurged once STRAM sees windows as fully processed in the DAG.\n\n\nA complete recovery of an operator needs the operator to be\ncreated, its checkpointed state restored and then all the lost atomic\nwindows replayed by the upstream buffer server(s). The above design\nkeeps the bookkeeping cost low with quick catch up time. In the next\nsection we will see how this simple abstraction allows applications to\nrecover under different requirements.\n\n\nRecovery Mechanisms\n\n\nRecovery mechanism are ways to recover from a container (or an\noperator) outage. In this section we discuss a single container outage.\nMultiple container outages are handled as independent events. Recovery\nrequires the upstream buffer server to replay windows and it would\nsimply go one more level upstream if the immediate upstream container\nhas also failed. If multiple operators are in a container (THREAD_LOCAL\nor CONTAINER_LOCAL stream) the container recovery treats each operator\nas an independent object when figuring out the recovery steps.\nApplication developers can set any of the recovery mechanisms discussed\nbelow for node outage.\n\n\nIn general, the cost of recovery depends on the state of the\noperator and the recovery mechanism selected, while data loss tolerance\nis specified by the application. For example a data-loss tolerant\napplication would prefer at most\nonce\u00a0recovery. All recovery mechanisms treat a streaming\nwindow as an atomic computation unit. In all three recovery mechanisms\nthe new operator connects to the upstream buffer server and asks for\ndata from a particular window onwards. Thus all recovery methods\ntranslate to deciding which atomic units to re-compute and which state\nthe new operator resumes from. A partially computed micro-batch is\nalways dropped. Such micro-batches are re-computed in at-least-once or\nexactly-once mode and skipped in at-most-once mode. The notiion of an\natomic micro-batch is a critical guiding principle as it enables very\nlow bookkeeping costs, high throughput, low recovery times, and high\nscalability. Within an application each operator can have its own\nrecovery mechanism.\n\n\nAt Least Once\n\n\nAt least once recovery is the default recovery mechanism, i.e it\nis used when no mechanism is specified. In this method, the lost\noperator is brought back to its latest viable checkpointed state and the\nupstream buffer server is asked to replay all subsequent windows. There\nis no data loss in recovery. The viable checkpoint state is defined as\nthe one whose window id is in the past as compared to all the\ncheckpoints of all the downstream operators. All downstream operators\nare restarted at their checkpointed state. They ignore all incoming data\nthat belongs to windows prior their checkpointed window. The lost\nwindows are thus recomputed and the application catches up with live\nincoming data. This is called \" at least\nonce\"\u00a0because lost windows are recomputed. For example if\nthe streaming window is 0.5 seconds and checkpointing is being done\nevery 30 seconds, then upon node outage all windows since the last\ncheckpoint (up to 60 windows) need to be re-processed. If the\napplication can handle loss of data, then this is not the most optimal\nrecovery mechanism.\n\n\nIn general for this recovery mode, the average time lag on a node\noutage is\n\n\n= (CP/2*SW)*T + HC\n\n\nwhere\n\n\n\n\nCP\n\u00a0\u00a0- Checkpointing period (default value is 30 seconds)\n\n\nSW\n\u00a0\u00a0- Streaming window period (default value is 0.5 seconds)\n\n\nT\n\u00a0\u00a0\u00a0- \u00a0Time taken to re-compute one lost window from data in memory\n\n\nHC\n\u00a0\u00a0- Time it takes to get a new Hadoop Container, or make do with the current ones\n\n\n\n\nA lower CP is a trade off between cost of checkpointing and the\nneed to have to use it in case of outage. Input adapters cannot use\nat-least-once recovery without the support from sources outside Hadoop.\nFor an output adapter care may needed if the external system cannot\nhandle re-write of the same data.\n\n\nAt Most Once\n\n\nThis recovery mechanism is for applications that can tolerate\ndata-loss; they get the quickest recovery in return. The restarted node\nconnects to the upstream buffer server, subscribing to data from the\nstart of the next window. It then starts processing that window. The\ndownstream operators ignore the lost windows and continue to process\nincoming data normally. Thus, this mechanism forces all downstream\noperators to follow.\n\n\nFor multiple inputs, the operator waits for all ports with the\nat-most-once attribute to get responses from their respective buffer\nservers. Then, the operator starts processing till the end window of the\nlatest window id on each input port is reached. In this case the end\nwindow tuple is non-blocking till the common window id is reached. At\nthis point the input ports are now properly synchronized. Upstream nodes\nreconnect under  at most\nonce\u00a0paradigm in same way. \u00a0For example, assume an operator\nhas ports in1\u00a0and in2\u00a0and a checkpointed window of 95. Assume further that the buffer servers of\noperators upstream of  in1\u00a0and\nin2\u00a0respond with window id 100 and\n102 respectively. Then port in1\u00a0would continue to process till end window of\n101, while port  in2\u00a0will wait for in1\nto catch up to 102.\nFrom \u00a0then on, both ports process their tuples normally. So windows from\n96 to  99are lost. Window 100\nand 101 has only\nin1 active, and 102 onwards both ports are active. The other\nports of upstream nodes would also catch up till  102in a similar fashion. This operator may not\nneed to be checkpointed. Currently the option to not do checkpoint in\nsuch cases is not available.\n\n\nIn general, in this recovery mode, the average time lag on a node\noutage is\n\n\n= SW/2 + HC\n\n\nwhere\n\n\n\n\n\n\nSW\n\u00a0- Streaming window period (default value is 0.5\nseconds)\n\n\n\n\n\n\nHC\n\u00a0- Time it takes to get a new Hadoop Container, or make\ndo with the current ones\n\n\n\n\n\n\nExactly Once\n\n\nThis recovery mechanism is for applications that require no\ndata-loss as well are no recomputation. Since a window is an atomic\ncompute unit, exactly once applies to the window as a whole. In this\nrecovery mode, the operator is brought back to the start of the window\nin which the outage happened and the window is recomputed. The window is\nconsidered closed when all the data computations are done and end window\ntuple is emitted. \u00a0Exactly once requires every window to be\ncheckpointed. From then on, the operator asks the upstream buffer server\nto send data from the last checkpoint. The upstream node behaves the\nsame as in at-most-once recovery. Checkpointing after every streaming\nwindow is very costly, but users would most often do exactly once per\napplication window; if the application window size is substantially\nlarger than the streaming window size (which typically is the case) the\ncost of running an operator in this recovery mode may not be as\nhigh.\n\n\nSpeculative Execution\n\n\nIn future we looking at possibility of adding speculative execution for the applications. This would be enabled in multiple ways.\n\n\n\n\n\n\nAt an operator level: The upstream operator would emit to\n    two copies. The downstream operator would receive from both copies\n    and pick a winner. The winner (primary) would be picked in either of\n    the following ways\n\n\n\n\nStatically as dictated by STRAM\n\n\nDynamically based on whose tuple arrives first. This mode\n    needs both copies to guarantee that the computation result would\n    have identical functionality\n\n\n\n\n\n\n\n\nAt a sub-query level: A part of the application DAG would be\n    run in parallel and all upstream operators would feed to two copies\n    and all downstream operators would receive from both copies. The\n    winners would again be picked in a static or dynamic manner\n\n\n\n\nEntire DAG: Another copy of the application would be run by\n    STRAM and the winner would be decided outside the application. In\n    this mode the output adapters would both be writing\n    the result.\n\n\n\n\nIn all cases the two copies would run on different Hadoop nodes.\nSpeculative execution is under development and\nis not yet available.\n\n\n\n\nDynamic Application Modifications\n\n\nDynamic application modifications are being worked on and most of\nthe features discussed here are now available. The platform supports the\nability to modify the DAG of the application as per inputs as well as\nset constraints, and will continue to provide abilities to deepen\nfeatures based on this ability. All these changes have one thing in\ncommon and that is the application does not need to be restarted as\nSTRAM will instrument the changes and the streaming will catch-up and\ncontinue.\n\n\nSome examples are\n\n\n\n\nDynamic Partitioning:\u00a0Automatic\n    changes in partitioning of computations to match constraints on a\n    run time basis. Examples includes STRAM adding resource during spike\n    in streams and returning them once spike is gone. Scale up and scale\n    down is done automatically without human intervention.\n\n\nModification via constraints: Attributes can be changed via\n    Webservices and STRAM would adapt the execution plan to meet these.\n    Examples include operations folks asking STRAM to reduce container\n    count, or changing network resource restrictions.\n\n\nModification via properties: Properties of operators can be\n    changed in run time. This enables application developers to trigger\n    a new behavior as need be. Examples include triggering an alert ON.\n    The platform supports changes to any property of an operator that\n    has a setter function defined.\n\n\nModification of DAG structure: Operators and streams can be\n    added to or removed from a running DAG, provided the code of the\n    operator being added is already in the classpath of the running\n    application master. \u00a0This enables application developers to add or\n    remove processing pipelines on the fly without having to restart\n    the application.\n\n\nQuery Insertion: Addition of sub-queries to currently\n    running application. This query would take current streams as inputs\n    and start computations as per their specs. Examples insertion of\n    SQL-queries on live data streams, dynamic query submission and\n    result from STRAM (not yet available).\n\n\n\n\nDynamic modifications to applications are foundational part of the\nplatform. They enable users to build layers over the applications. Users\ncan also save all the changes done since the application launch, and\ntherefore predictably get the application to its current state. For\ndetails refer to  \nConfiguration Guide\n\n.\n\n\n\n\nDemos\n\n\nThe source code for the demos is available in the open-source\n\nApache Apex-Malhar repository\n.\nAll of these do computations in real-time. Developers are encouraged to\nreview them as they use various features of the platform and provide an\nopportunity for quick learning.", 
            "title": "Applications"
        }, 
        {
            "location": "/application_development/#application-developer-guide", 
            "text": "The Apex platform is designed to process massive amounts of\nreal-time events natively in Hadoop.  It runs as a YARN (Hadoop 2.x) \napplication and leverages Hadoop as a distributed operating\nsystem.  All the basic distributed operating system capabilities of\nHadoop like resource management (YARN), distributed file system (HDFS),\nmulti-tenancy, security, fault-tolerance, and scalability are supported natively \nin all the Apex applications. \u00a0The platform handles all the details of the application \nexecution, including dynamic scaling, state checkpointing and recovery, event \nprocessing guarantees, etc. allowing you to focus on writing your application logic without\nmixing operational and functional concerns.  In the platform, building a streaming application can be extremely\neasy and intuitive. \u00a0The application is represented as a Directed\nAcyclic Graph (DAG) of computation units called  Operators  interconnected\nby the data-flow edges called   Streams .\u00a0The operators process input\nstreams and produce output streams. A library of common operators is\nprovided to enable quick application development. \u00a0In case the desired\nprocessing is not available in the Operator Library, one can easily\nwrite a custom operator. We refer those interested in creating their own\noperators to the  Operator Development Guide .", 
            "title": "Application Developer Guide"
        }, 
        {
            "location": "/application_development/#running-a-test-application", 
            "text": "If you are starting with the Apex platform for the first time,\nit can be informative to launch an existing application and see it run.\nOne of the simplest examples provided in  Apex-Malhar repository  is a Pi demo application,\nwhich computes the value of PI using random numbers.  After  setting up development environment \nPi demo can be launched as follows:   Open up Apex Malhar files in your IDE (for example Eclipse, IntelliJ, NetBeans, etc)  Navigate to  demos/pi/src/test/java/com/datatorrent/demos/ApplicationTest.java  Run the test for ApplicationTest.java  View the output in system console   Congratulations, you just ran your first real-time streaming demo :) \nThis demo is very simple and has four operators. The first operator\nemits random integers between 0 to 30, 000. The second operator receives\nthese coefficients and emits a hashmap with x and y values each time it\nreceives two values. The third operator takes these values and computes\nx**2+y**2. The last operator counts how many computed values from\nthe previous operator were less than or equal to 30, 000**2. Assuming\nthis count is N, then PI is computed as N/number of values received.\nHere is the code snippet for the PI application. This code populates the\nDAG. Do not worry about what each line does, we will cover these\nconcepts later in this document.  // Generates random numbers\nRandomEventGenerator rand = dag.addOperator( rand , new RandomEventGenerator());\nrand.setMinvalue(0);\nrand.setMaxvalue(30000);\n\n// Generates a round robin HashMap of  x  and  y \nRoundRobinHashMap String,Object  rrhm = dag.addOperator( rrhm , new RoundRobinHashMap String, Object ());\nrrhm.setKeys(new String[] {  x ,  y  });\n\n// Calculates pi from x and y\nJavaScriptOperator calc = dag.addOperator( picalc , new Script());\ncalc.setPassThru(false);\ncalc.put( i ,0);\ncalc.put( count ,0);\ncalc.addSetupScript( function pi() { if (x*x+y*y  =  +maxValue*maxValue+ ) { i++; } count++; return i / count * 4; } );\ncalc.setInvoke( pi );\ndag.addStream( rand_rrhm , rand.integer_data, rrhm.data);\ndag.addStream( rrhm_calc , rrhm.map, calc.inBindings);\n\n// puts results on system console\nConsoleOutputOperator console = dag.addOperator( console , new ConsoleOutputOperator());\ndag.addStream( rand_console ,calc.result, console.input);  You can review the other demos and see what they do. The examples\ngiven in the Demos project cover various features of the platform and we\nstrongly encourage you to read these to familiarize yourself with the\nplatform. In the remaining part of this document we will go through\ndetails needed for you to develop and run streaming applications in\nMalhar.", 
            "title": "Running A Test Application"
        }, 
        {
            "location": "/application_development/#test-application-yahoo-finance-quotes", 
            "text": "The PI\u00a0application was to\nget you started. It is a basic application and does not fully illustrate\nthe features of the platform. For the purpose of describing concepts, we\nwill consider the test application shown in Figure 1. The application\ndownloads tick data from   Yahoo! Finance  \u00a0and computes the\nfollowing for four tickers, namely  IBM , GOOG ,  YHOO .   Quote: Consisting of last trade price, last trade time, and\n    total volume for the day  Per-minute chart data: Highest trade price, lowest trade\n    price, and volume during that minute  Simple Moving Average: trade price over 5 minutes   Total volume must ensure that all trade volume for that day is\nadded, i.e. data loss would result in wrong results. Charting data needs\nall the trades in the same minute to go to the same slot, and then on it\nstarts afresh, so again data loss would result in wrong results. The\naggregation for charting data is done over 1 minute. Simple moving\naverage computes the average price over a 5 minute sliding window; it\ntoo would produce wrong results if there is data loss. Figure 1 shows\nthe application with no partitioning.   The operator StockTickerInput:\u00a0StockTickerInput \u00a0 is\nthe input operator that reads live data from Yahoo! Finance once per\ninterval (user configurable in milliseconds), and emits the price, the\nincremental volume, and the last trade time of each stock symbol, thus\nemulating real ticks from the exchange. \u00a0We utilize the Yahoo! Finance\nCSV web service interface. \u00a0For example:  $ GET 'http://download.finance.yahoo.com/d/quotes.csv?s=IBM,GOOG,AAPL,YHOO f=sl1vt1' IBM ,203.966,1513041, 1:43pm  GOOG ,762.68,1879741, 1:43pm  AAPL ,444.3385,11738366, 1:43pm  YHOO ,19.3681,14707163, 1:43pm   Among all the operators in Figure 1, StockTickerInput is the only\noperator that requires extra code because it contains a custom mechanism\nto get the input data. \u00a0Other operators are used unchanged from the\nMalhar library.  Here is the class implementation for StockTickInput:  package com.datatorrent.demos.yahoofinance;\n\nimport au.com.bytecode.opencsv.CSVReader;\nimport com.datatorrent.annotation.OutputPortFieldAnnotation;\nimport com.datatorrent.api.Context.OperatorContext;\nimport com.datatorrent.api.DefaultOutputPort;\nimport com.datatorrent.api.InputOperator;\nimport com.datatorrent.lib.util.KeyValPair;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.InputStreamReader;\nimport java.util.*;\nimport org.apache.commons.httpclient.HttpClient;\nimport org.apache.commons.httpclient.HttpStatus;\nimport org.apache.commons.httpclient.cookie.CookiePolicy;\nimport org.apache.commons.httpclient.methods.GetMethod;\nimport org.apache.commons.httpclient.params.DefaultHttpParams;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n/**\n * This operator sends price, volume and time into separate ports and calculates incremental volume.\n */\npublic class StockTickInput implements InputOperator\n{\n  private static final Logger logger = LoggerFactory.getLogger(StockTickInput.class);\n  /**\n   * Timeout interval for reading from server. 0 or negative indicates no timeout.\n   */\n  public int readIntervalMillis = 500;\n  /**\n   * The URL of the web service resource for the POST request.\n   */\n  private String url;\n  public String[] symbols;\n  private transient HttpClient client;\n  private transient GetMethod method;\n  private HashMap String, Long  lastVolume = new HashMap String, Long ();\n  private boolean outputEvenIfZeroVolume = false;\n  /**\n   * The output port to emit price.\n   */\n  @OutputPortFieldAnnotation(optional = true)\n  public final transient DefaultOutputPort KeyValPair String, Double  price = new DefaultOutputPort KeyValPair String, Double ();\n  /**\n   * The output port to emit incremental volume.\n   */\n  @OutputPortFieldAnnotation(optional = true)\n  public final transient DefaultOutputPort KeyValPair String, Long  volume = new DefaultOutputPort KeyValPair String, Long ();\n  /**\n   * The output port to emit last traded time.\n   */\n  @OutputPortFieldAnnotation(optional = true)\n  public final transient DefaultOutputPort KeyValPair String, String  time = new DefaultOutputPort KeyValPair String, String ();\n\n  /**\n   * Prepare URL from symbols and parameters. URL will be something like: http://download.finance.yahoo.com/d/quotes.csv?s=IBM,GOOG,AAPL,YHOO f=sl1vt1\n   *\n   * @return the URL\n   */\n  private String prepareURL()\n  {\n    String str =  http://download.finance.yahoo.com/d/quotes.csv?s= ;\n    for (int i = 0; i   symbols.length; i++) {\n      if (i != 0) {\n        str +=  , ;\n      }\n      str += symbols[i];\n    }\n    str +=  f=sl1vt1 e=.csv ;\n    return str;\n  }\n\n  @Override\n  public void setup(OperatorContext context)\n  {\n    url = prepareURL();\n    client = new HttpClient();\n    method = new GetMethod(url);\n    DefaultHttpParams.getDefaultParams().setParameter( http.protocol.cookie-policy , CookiePolicy.BROWSER_COMPATIBILITY);\n  }\n\n  @Override\n  public void teardown()\n  {\n  }\n\n  @Override\n  public void emitTuples()\n  {\n\n    try {\n      int statusCode = client.executeMethod(method);\n      if (statusCode != HttpStatus.SC_OK) {\n        System.err.println( Method failed:   + method.getStatusLine());\n      } else {\n        InputStream istream = method.getResponseBodyAsStream();\n        // Process response\n        InputStreamReader isr = new InputStreamReader(istream);\n        CSVReader reader = new CSVReader(isr);\n        List String[]  myEntries = reader.readAll();\n        for (String[] stringArr: myEntries) {\n          ArrayList String  tuple = new ArrayList String (Arrays.asList(stringArr));\n          if (tuple.size() != 4) {\n            return;\n          }\n          // input csv is  Symbol , Price , Volume , Time \n          String symbol = tuple.get(0);\n          double currentPrice = Double.valueOf(tuple.get(1));\n          long currentVolume = Long.valueOf(tuple.get(2));\n          String timeStamp = tuple.get(3);\n          long vol = currentVolume;\n          // Sends total volume in first tick, and incremental volume afterwards.\n          if (lastVolume.containsKey(symbol)) {\n            vol -= lastVolume.get(symbol);\n          }\n\n          if (vol   0 || outputEvenIfZeroVolume) {\n            price.emit(new KeyValPair String, Double (symbol, currentPrice));\n            volume.emit(new KeyValPair String, Long (symbol, vol));\n            time.emit(new KeyValPair String, String (symbol, timeStamp));\n            lastVolume.put(symbol, currentVolume);\n          }\n        }\n      }\n      Thread.sleep(readIntervalMillis);\n    } catch (InterruptedException ex) {\n      logger.debug(ex.toString());\n    } catch (IOException ex) {\n      logger.debug(ex.toString());\n    }\n  }\n\n  @Override\n  public void beginWindow(long windowId)\n  {\n  }\n\n  @Override\n  public void endWindow()\n  {\n  }\n\n  public void setOutputEvenIfZeroVolume(boolean outputEvenIfZeroVolume)\n  {\n    this.outputEvenIfZeroVolume = outputEvenIfZeroVolume;\n  }\n\n}  The operator has three output ports that emit the price of the\nstock, the volume of the stock and the last trade time of the stock,\ndeclared as public member variables price, volume\u00a0and  time\u00a0of the class. \u00a0The tuple of the\nprice\u00a0output port is a key-value\npair with the stock symbol being the key, and the price being the value.\n\u00a0The tuple of the volume\u00a0output\nport is a key value pair with the stock symbol being the key, and the\nincremental volume being the value. \u00a0The tuple of the  time\u00a0output port is a key value pair with the\nstock symbol being the key, and the last trade time being the\nvalue.  Important: Since operators will be\nserialized, all input and output ports need to be declared transient\nbecause they are stateless and should not be serialized.  The method\u00a0setup(OperatorContext)\ncontains the code that is necessary for setting up the HTTP\nclient for querying Yahoo! Finance.  Method\u00a0emitTuples() contains\nthe code that reads from Yahoo! Finance, and emits the data to the\noutput ports of the operator. \u00a0emitTuples()\u00a0will be called one or more times\nwithin one application window as long as time is allowed within the\nwindow.  Note that we want to emulate the tick input stream by having\nincremental volume data with Yahoo! Finance data. \u00a0We therefore subtract\nthe previous volume from the current volume to emulate incremental\nvolume for each tick.  The operator\nDailyVolume:\u00a0This operator\nreads from the input port, which contains the incremental volume tuples\nfrom StockTickInput, and\naggregates the data to provide the cumulative volume. \u00a0It uses the\nlibrary class  SumKeyVal K,V \u00a0provided in math\u00a0package. \u00a0In this case,\nSumKeyVal String,Long , where K is the stock symbol, V is the\naggregated volume, with cumulative\nset to true. (Otherwise if  cumulativewas set to false, SumKeyVal would\nprovide the sum for the application window.) \u00a0Malhar provides a number\nof built-in operators for simple operations like this so that\napplication developers do not have to write them. \u00a0More examples to\nfollow. This operator assumes that the application restarts before the\nmarket opens every day.  The operator Quote:\nThis operator has three input ports, which are price (from\nStockTickInput), daily_vol (from\nDaily Volume), and time (from\n StockTickInput). \u00a0This operator\njust consolidates the three data items and and emits the consolidated\ndata. \u00a0It utilizes the class ConsolidatorKeyVal K \u00a0from the\nstream\u00a0package.  The operator HighLow:\u00a0This operator reads from the input port,\nwhich contains the price tuples from StockTickInput, and provides the high and the\nlow price within the application window. \u00a0It utilizes the library class\n RangeKeyVal K,V \u00a0provided\nin the math\u00a0package. In this case,\nRangeKeyVal String,Double .  The operator MinuteVolume:\nThis operator reads from the input port, which contains the\nvolume tuples from StockTickInput,\nand aggregates the data to provide the sum of the volume within one\nminute. \u00a0Like the operator  DailyVolume, this operator also uses\nSumKeyVal String,Long , but\nwith cumulative set to false. \u00a0The\nApplication Window is set to one minute. We will explain how to set this\nlater.  The operator Chart:\nThis operator is very similar to the operator Quote, except that it takes inputs from\nHigh Low\u00a0and  Minute Vol\u00a0and outputs the consolidated tuples\nto the output port.  The operator PriceSMA:\nSMA stands for - Simple Moving Average. It reads from the\ninput port, which contains the price tuples from StockTickInput, and\nprovides the moving average price of the stock. \u00a0It utilizes\nSimpleMovingAverage String,Double , which is provided in the\n multiwindow\u00a0package.\nSimpleMovingAverage keeps track of the data of the previous N\napplication windows in a sliding manner. \u00a0For each end window event, it\nprovides the average of the data in those application windows.  The operator Console:\nThis operator just outputs the input tuples to the console\n(or stdout). \u00a0In this example, there are four console\u00a0operators, which connect to the output\nof  Quote, Chart, PriceSMA and VolumeSMA. \u00a0In\npractice, they should be replaced by operators that use the data to\nproduce visualization artifacts like charts.  Connecting the operators together and constructing the\nDAG:\u00a0Now that we know the\noperators used, we will create the DAG, set the streaming window size,\ninstantiate the operators, and connect the operators together by adding\nstreams that connect the output ports with the input ports among those\noperators. \u00a0This code is in the file  YahooFinanceApplication.java. Refer to Figure 1\nagain for the graphical representation of the DAG. \u00a0The last method in\nthe code, namely getApplication(),\ndoes all that. \u00a0The rest of the methods are just for setting up the\noperators.  package com.datatorrent.demos.yahoofinance;\n\nimport com.datatorrent.api.ApplicationFactory;\nimport com.datatorrent.api.Context.OperatorContext;\nimport com.datatorrent.api.DAG;\nimport com.datatorrent.api.Operator.InputPort;\nimport com.datatorrent.lib.io.ConsoleOutputOperator;\nimport com.datatorrent.lib.math.RangeKeyVal;\nimport com.datatorrent.lib.math.SumKeyVal;\nimport com.datatorrent.lib.multiwindow.SimpleMovingAverage;\nimport com.datatorrent.lib.stream.ConsolidatorKeyVal;\nimport com.datatorrent.lib.util.HighLow;\nimport org.apache.hadoop.conf.Configuration;\n\n/**\n * Yahoo! Finance application demo.  p \n *\n * Get Yahoo finance feed and calculate minute price range, minute volume, simple moving average of 5 minutes.\n */\npublic class Application implements StreamingApplication\n{\n  private int streamingWindowSizeMilliSeconds = 1000; // 1 second (default is 500ms)\n  private int appWindowCountMinute = 60;   // 1 minute\n  private int appWindowCountSMA = 5 * 60;  // 5 minute\n\n  /**\n   * Get actual Yahoo finance ticks of symbol, last price, total daily volume, and last traded price.\n   */\n  public StockTickInput getStockTickInputOperator(String name, DAG dag)\n  {\n    StockTickInput oper = dag.addOperator(name, StockTickInput.class);\n    oper.readIntervalMillis = 200;\n    return oper;\n  }\n\n  /**\n   * This sends total daily volume by adding volumes from each ticks.\n   */\n  public SumKeyVal String, Long  getDailyVolumeOperator(String name, DAG dag)\n  {\n    SumKeyVal String, Long  oper = dag.addOperator(name, new SumKeyVal String, Long ());\n    oper.setType(Long.class);\n    oper.setCumulative(true);\n    return oper;\n  }\n\n  /**\n   * Get aggregated volume of 1 minute and send at the end window of 1 minute.\n   */\n  public SumKeyVal String, Long  getMinuteVolumeOperator(String name, DAG dag, int appWindowCount)\n  {\n    SumKeyVal String, Long  oper = dag.addOperator(name, new SumKeyVal String, Long ());\n    oper.setType(Long.class);\n    oper.setEmitOnlyWhenChanged(true);\ndag.getOperatorMeta(name).getAttributes().put(OperatorContext.APPLICATION_WINDOW_COUNT,appWindowCount);\n    return oper;\n  }\n\n  /**\n   * Get High-low range for 1 minute.\n   */\n  public RangeKeyVal String, Double  getHighLowOperator(String name, DAG dag, int appWindowCount)\n  {\n    RangeKeyVal String, Double  oper = dag.addOperator(name, new RangeKeyVal String, Double ());\n    dag.getOperatorMeta(name).getAttributes().put(OperatorContext.APPLICATION_WINDOW_COUNT,appWindowCount);\n    oper.setType(Double.class);\n    return oper;\n  }\n\n  /**\n   * Quote (Merge price, daily volume, time)\n   */\n  public ConsolidatorKeyVal String,Double,Long,String,?,?  getQuoteOperator(String name, DAG dag)\n  {\n    ConsolidatorKeyVal String,Double,Long,String,?,?  oper = dag.addOperator(name, new ConsolidatorKeyVal String,Double,Long,String,Object,Object ());\n    return oper;\n  }\n\n  /**\n   * Chart (Merge minute volume and minute high-low)\n   */\n  public ConsolidatorKeyVal String,HighLow,Long,?,?,?  getChartOperator(String name, DAG dag)\n  {\n    ConsolidatorKeyVal String,HighLow,Long,?,?,?  oper = dag.addOperator(name, new ConsolidatorKeyVal String,HighLow,Long,Object,Object,Object ());\n    return oper;\n  }\n\n  /**\n   * Get simple moving average of price.\n   */\n  public SimpleMovingAverage String, Double  getPriceSimpleMovingAverageOperator(String name, DAG dag, int appWindowCount)\n  {\n    SimpleMovingAverage String, Double  oper = dag.addOperator(name, new SimpleMovingAverage String, Double ());\n    oper.setWindowSize(appWindowCount);\n    oper.setType(Double.class);\n    return oper;\n  }\n\n  /**\n   * Get console for output.\n   */\n  public InputPort Object  getConsole(String name, /*String nodeName,*/ DAG dag, String prefix)\n  {\n    ConsoleOutputOperator oper = dag.addOperator(name, ConsoleOutputOperator.class);\n    oper.setStringFormat(prefix +  : %s );\n    return oper.input;\n  }\n\n  /**\n   * Create Yahoo Finance Application DAG.\n   */\n  @Override\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n    dag.getAttributes().put(DAG.STRAM_WINDOW_SIZE_MILLIS,streamingWindowSizeMilliSeconds);\n\n    StockTickInput tick = getStockTickInputOperator( StockTickInput , dag);\n    SumKeyVal String, Long  dailyVolume = getDailyVolumeOperator( DailyVolume , dag);\n    ConsolidatorKeyVal String,Double,Long,String,?,?  quoteOperator = getQuoteOperator( Quote , dag);\n\n    RangeKeyVal String, Double  highlow = getHighLowOperator( HighLow , dag, appWindowCountMinute);\n    SumKeyVal String, Long  minuteVolume = getMinuteVolumeOperator( MinuteVolume , dag, appWindowCountMinute);\n    ConsolidatorKeyVal String,HighLow,Long,?,?,?  chartOperator = getChartOperator( Chart , dag);\n\n    SimpleMovingAverage String, Double  priceSMA = getPriceSimpleMovingAverageOperator( PriceSMA , dag, appWindowCountSMA);\n       DefaultPartitionCodec String, Double  codec = new DefaultPartitionCodec String, Double ();\n    dag.setInputPortAttribute(highlow.data, PortContext.STREAM_CODEC, codec);\n    dag.setInputPortAttribute(priceSMA.data, PortContext.STREAM_CODEC, codec);\n    dag.addStream( price , tick.price, quoteOperator.in1, highlow.data, priceSMA.data);\n    dag.addStream( vol , tick.volume, dailyVolume.data, minuteVolume.data);\n    dag.addStream( time , tick.time, quoteOperator.in3);\n    dag.addStream( daily_vol , dailyVolume.sum, quoteOperator.in2);\n\n    dag.addStream( quote_data , quoteOperator.out, getConsole( quoteConsole , dag,  QUOTE ));\n\n    dag.addStream( high_low , highlow.range, chartOperator.in1);\n    dag.addStream( vol_1min , minuteVolume.sum, chartOperator.in2);\n    dag.addStream( chart_data , chartOperator.out, getConsole( chartConsole , dag,  CHART ));\n\n    dag.addStream( sma_price , priceSMA.doubleSMA, getConsole( priceSMAConsole , dag,  Price SMA ));\n\n    return dag;\n  }\n\n}  Note that we also set a user-specific sliding window for SMA that\nkeeps track of the previous N data points. \u00a0Do not confuse this with the\nattribute APPLICATION_WINDOW_COUNT.  In the rest of this chapter we will run through the process of\nrunning this application. We assume that \u00a0you are familiar with details\nof your Hadoop infrastructure. For installation\ndetails please refer to the  Installation Guide .", 
            "title": "Test Application: Yahoo! Finance Quotes"
        }, 
        {
            "location": "/application_development/#running-a-test-application_1", 
            "text": "We will now describe how to run the yahoo\nfinance application\u00a0described above in different modes\n(local mode, single node on Hadoop, and multi-nodes on Hadoop).  The platform runs streaming applications under the control of a\nlight-weight Streaming Application Manager (STRAM). Each application has\nits own instance of STRAM. STRAM launches the application and\ncontinually provides run time monitoring, analysis, and takes action\nsuch as load scaling or outage recovery as needed. \u00a0We will discuss\nSTRAM in more detail in the next chapter.  The instructions below assume that the platform was installed in a\ndirectory  INSTALL_DIR  and the command line interface (CLI) will\nbe used to launch the demo application. An application can be run in\nlocal mode\u00a0(in IDE or from command line) or on a Hadoop cluster.  To start the Apex CLI run  INSTALL_DIR /bin/apex  The command line prompt appears.  To start the application in local mode (the actual version number in the file name may differ)  apex  launch -local  INSTALL_DIR /yahoo-finance-demo-3.2.0-SNAPSHOT.apa  To terminate the application in local mode, enter Ctrl-C  Tu run the application on the Hadoop cluster (the actual version\nnumber in the file name may differ)  apex  launch  INSTALL_DIR /yahoo-finance-demo-3.2.0-SNAPSHOT.apa  To stop the application running in Hadoop, terminate it in the Apex CLI:  apex  kill-app  Executing the application in either mode includes the following\nsteps. At a top level, STRAM (Streaming Application Manager) validates\nthe application (DAG), translates the logical plan to the physical plan\nand then launches the execution engine. The mode determines the\nresources needed and how how they are used.", 
            "title": "Running a Test Application"
        }, 
        {
            "location": "/application_development/#local-mode", 
            "text": "In local mode, the application is run as a single-process\u00a0with multiple threads. Although a\nfew Hadoop classes are needed, there is no dependency on a Hadoop\ncluster or Hadoop services. The local file system is used in place of\nHDFS. This mode allows a quick run of an application in a single process\nsandbox, and hence is the most suitable to debug and analyze the\napplication logic. This mode is recommended for developing the\napplication and can be used for running applications within the IDE for\nfunctional testing purposes. Due to limited resources and lack \u00a0of\nscalability an application running in this single process mode is more\nlikely to encounter throughput bottlenecks. A distributed cluster is\nrecommended for benchmarking and production testing.", 
            "title": "Local Mode"
        }, 
        {
            "location": "/application_development/#hadoop-cluster", 
            "text": "In this section we discuss various Hadoop cluster setups.", 
            "title": "Hadoop Cluster"
        }, 
        {
            "location": "/application_development/#single-node-cluster", 
            "text": "In a single node Hadoop cluster all services are deployed on a\nsingle server (a developer can use his/her development machine as a\nsingle node cluster). The platform does not distinguish between a single\nor multi-node setup and behaves exactly the same in both cases.  In this mode, the resource manager, name node, data node, and node\nmanager occupy one process each. This is an example of running a\nstreaming application as a multi-process\u00a0application on the same server.\nWith prevalence of fast, multi-core systems, this mode is effective for\ndebugging, fine tuning, and generic analysis before submitting the job\nto a larger Hadoop cluster. In this mode, execution uses the Hadoop\nservices and hence is likely to identify issues that are related to the\nHadoop environment (such issues will not be uncovered in local mode).\nThe throughput will obviously not be as high as on a multi-node Hadoop\ncluster. Additionally, since each container (i.e. Java process) requires\na significant amount of memory, you will be able to run a much smaller\nnumber of containers than on a multi-node cluster.", 
            "title": "Single Node Cluster"
        }, 
        {
            "location": "/application_development/#multi-node-cluster", 
            "text": "In a multi-node Hadoop cluster all the services of Hadoop are\ntypically distributed across multiple nodes in a production or\nproduction-level test environment. Upon launch the application is\nsubmitted to the Hadoop cluster and executes as a  multi-processapplication on\u00a0multiple nodes.  Before you start deploying, testing and troubleshooting your\napplication on a cluster, you should ensure that Hadoop (version 2.6.0\nor later)\u00a0is properly installed and\nyou have basic skills for working with it.", 
            "title": "Multi-Node Cluster"
        }, 
        {
            "location": "/application_development/#apache-apex-platform-overview", 
            "text": "", 
            "title": "Apache Apex Platform Overview"
        }, 
        {
            "location": "/application_development/#streaming-computational-model", 
            "text": "In this chapter, we describe the the basics of the real-time streaming platform and its computational model.  The platform is designed to enable completely asynchronous real time computations\u00a0done in as unblocked a way as possible with\nminimal overhead .  Applications running in the platform are represented by a Directed\nAcyclic Graph (DAG) made up of \u00a0operators and streams. All computations\nare done in memory on arrival of\nthe input data, with an option to save the output to disk (HDFS) in a\nnon-blocking way. The data that flows between operators consists of\natomic data elements. Each data element along with its type definition\n(henceforth called  schema) is\ncalled a tuple.\u00a0An application is a\ndesign of the flow of these tuples to and from\nthe appropriate compute units to enable the computation of the final\ndesired results.\u00a0A message queue (henceforth called\n\u00a0buffer server) manages tuples streaming\nbetween compute units in different processes.This server keeps track of\nall consumers, publishers, partitions, and enables replay. More\ninformation is given in later section.  The streaming application is monitored by a decision making entity\ncalled STRAM (streaming application\nmanager).\u00a0STRAM is designed to be a light weight\ncontroller that has minimal but sufficient interaction with the\napplication. This is done via periodic heartbeats. The\nSTRAM does the initial launch and periodically analyzes the system\nmetrics to decide if any run time action needs to be taken.  A fundamental building block for the streaming platform\nis the concept of breaking up a stream into equal finite time slices\ncalled streaming windows. Each window contains the ordered\nset of tuples in that time slice. A typical duration of a window is 500\nms, but can be configured per application (the Yahoo! Finance\napplication configures this value in the  properties.xml\u00a0file to be 1000ms = 1s). Each\nwindow is preceded by a begin_window\u00a0event and is terminated by an\nend_window\u00a0event, and is assigned\na unique window ID. Even though the platform performs computations at\nthe tuple level, bookkeeping is done at the window boundary, making the\ncomputations within a window an atomic event in the platform. \u00a0We can\nthink of each window as an  atomic\nmicro-batch\u00a0of tuples, to be processed together as one\natomic operation (See Figure 2). \u00a0  This atomic batching allows the platform to avoid the very steep\nper tuple bookkeeping cost and instead has a manageable per batch\nbookkeeping cost. This translates to higher throughput, low recovery\ntime, and higher scalability. Later in this document we illustrate how\nthe atomic micro-batch concept allows more efficient optimization\nalgorithms.  The platform also has in-built support for\napplication windows.\u00a0 An application window is part of the\napplication specification, and can be a small or large multiple of the\nstreaming window. \u00a0An example from our Yahoo! Finance test application\nis the moving average, calculated over a sliding application window of 5\nminutes which equates to 300 (= 5 * 60) streaming windows.  Note that these two window concepts are distinct. \u00a0A streaming\nwindow is an abstraction of many tuples into a higher atomic event for\neasier management. \u00a0An application window is a group of consecutive\nstreaming windows used for data aggregation (e.g. sum, average, maximum,\nminimum) on a per operator level.   Alongside the platform,\u00a0a set of\npredefined, benchmarked standard library operator templates is provided\nfor ease of use and rapid development of application.\u00a0These\noperators are open sourced to Apache Software Foundation under the\nproject name \u201cMalhar\u201d as part of our efforts to foster community\ninnovation. These operators can be used in a DAG as is, while others\nhave properties\u00a0that can be set to specify the\ndesired computation. Those interested in details, should refer to Apex-Malhar operator library .  The platform is a Hadoop YARN native\napplication. It runs in a Hadoop cluster just like any\nother YARN application (MapReduce etc.) and is designed to seamlessly\nintegrate with rest of Hadoop technology stack. It leverages Hadoop as\nmuch as possible and relies on it as its distributed operating system.\nHadoop dependencies include resource management, compute/memory/network\nallocation, HDFS, security, fault tolerance, monitoring, metrics,\nmulti-tenancy, logging etc. Hadoop classes/concepts are reused as much\nas possible.  The aim is to enable enterprises\nto leverage their existing Hadoop infrastructure for real time streaming\napplications. The platform is designed to scale with big\ndata applications and scale with Hadoop.  A streaming application is an asynchronous execution of\ncomputations across distributed nodes. All computations are done in\nparallel on a distributed cluster. The computation model is designed to\ndo as many parallel computations as possible in a non-blocking fashion.\nThe task of monitoring of the entire application is done on (streaming)\nwindow boundaries with a streaming window as an atomic entity. A window\ncompletion is a quantum of work done. There is no assumption that an\noperator can be interrupted at precisely a particular tuple or window.  An operator itself also\ncannot assume or predict the exact time a tuple that it emitted would\nget consumed by downstream operators. The operator processes the tuples\nit gets and simply emits new tuples based on its business logic. The\nonly guarantee it has is that the upstream operators are processing\neither the current or some later window, and the downstream operator is\nprocessing either the current or some earlier window. The completion of\na window (i.e. propagation of the  end_window\u00a0event through an operator) in any\noperator guarantees that all upstream operators have finished processing\nthis window. Thus, the end_window\u00a0event is blocking on an operator\nwith multiple outputs, and is a synchronization point in the DAG. The\n begin_window\u00a0event does not have\nany such restriction, a single begin_window\u00a0event from any upstream operator\ntriggers the operator to start processing tuples.", 
            "title": "Streaming Computational Model"
        }, 
        {
            "location": "/application_development/#streaming-application-manager-stram", 
            "text": "Streaming Application Manager (STRAM) is the Hadoop YARN native\napplication master. STRAM is the first process that is activated upon\napplication launch and orchestrates the streaming application on the\nplatform. STRAM is a lightweight controller process. The\nresponsibilities of STRAM include    Running the Application   Read the\u00a0logical plan\u00a0of the application (DAG) submitted by the client  Validate the logical plan  Translate the logical plan into a physical plan, where certain operators may  be partitioned (i.e. replicated) to multiple operators for  handling load.  Request resources (Hadoop containers) from Resource Manager,\n    per physical plan  Based on acquired resources and application attributes, create\n    an execution plan\u00a0by partitioning the DAG into fragments,\n    each assigned to different containers.  Executes the application by deploying each fragment to\n    its container. Containers then start stream processing and run\n    autonomously, processing one streaming window after another. Each\n    container is represented as an instance of the  StreamingContainer\u00a0class, which updates\n    STRAM via the heartbeat protocol and processes directions received\n    from STRAM.     Continually monitoring the application via heartbeats from each StreamingContainer   Collecting Application System Statistics and Logs  Logging all application-wide decisions taken  Providing system data on the state of the application via a  Web Service.   Supporting Fault Tolerance  a.  Detecting a node outage\nb.  Requesting a replacement resource from the Resource Manager\n    and scheduling state restoration for the streaming operators\nc.  Saving state to Zookeeper    Supporting Dynamic Partitioning:\u00a0Periodically evaluating the SLA and modifying the physical plan if required\n    (logical plan does not change).   Enabling Security:\u00a0Distributing security tokens for distributed components of the execution engine\n    and securing web service requests.  Enabling Dynamic modification of DAG: In the future, we intend to allow for user initiated\n    modification of the logical plan to allow for changes to the\n    processing logic and functionality.   An example of the Yahoo! Finance Quote application scheduled on a\ncluster of 5 Hadoop containers (processes) is shown in Figure 3.   An example for the translation from a logical plan to a physical\nplan and an execution plan for a subset of the application is shown in\nFigure 4.", 
            "title": "Streaming Application Manager (STRAM)"
        }, 
        {
            "location": "/application_development/#hadoop-components", 
            "text": "In this section we cover some aspects of Hadoop that your\nstreaming application interacts with. This section is not meant to\neducate the reader on Hadoop, but just get the reader acquainted with\nthe terms. We strongly advise readers to learn Hadoop from other\nsources.  A streaming application runs as a native Hadoop 2.x application.\nHadoop 2.x does not differentiate between a map-reduce job and other\napplications, and hence as far as Hadoop is concerned, the streaming\napplication is just another job. This means that your application\nleverages all the bells and whistles Hadoop provides and is fully\nsupported within Hadoop technology stack. The platform is responsible\nfor properly integrating itself with the relevant components of Hadoop\nthat exist today and those that may emerge in the future  All investments that leverage multi-tenancy (for example quotas\nand queues), security (for example kerberos), data flow integration (for\nexample copying data in-out of HDFS), monitoring, metrics collections,\netc. will require no changes when streaming applications run on\nHadoop.", 
            "title": "Hadoop Components"
        }, 
        {
            "location": "/application_development/#yarn", 
            "text": "YARN is\nthe core library of Hadoop 2.x that is tasked with resource management\nand works as a distributed application framework. In this section we\nwill walk through Yarn's components. In Hadoop 2.x, the old jobTracker\nhas been replaced by a combination of ResourceManager (RM) and\nApplicationMaster (AM).", 
            "title": "YARN"
        }, 
        {
            "location": "/application_development/#resource-manager-rm", 
            "text": "ResourceManager (RM)\nmanages all the distributed resources. It allocates and arbitrates all\nthe slots and the resources (cpu, memory, network) of these slots. It\nworks with per-node NodeManagers (NMs) and per-application\nApplicationMasters (AMs). Currently memory usage is monitored by RM; in\nupcoming releases it will have CPU as well as network management. RM is\nshared by map-reduce and streaming applications. Running streaming\napplications requires no changes in the RM.", 
            "title": "Resource Manager (RM)"
        }, 
        {
            "location": "/application_development/#application-master-am", 
            "text": "The AM is the watchdog or monitoring process for your application\nand has the responsibility of negotiating resources with RM and\ninteracting with NodeManagers to get the allocated containers started.\nThe AM is the starting point of your application and is considered user\ncode (not system Hadoop code). The AM itself runs in one container. All\nresource management within the application are managed by the AM. This\nis a critical feature for Hadoop 2.x where tasks done by jobTracker in\nHadoop 1.0 have been distributed allowing Hadoop 2.x to scale much\nbeyond Hadoop 1.0. STRAM is a native YARN ApplicationManager.", 
            "title": "Application Master (AM)"
        }, 
        {
            "location": "/application_development/#node-managers-nm", 
            "text": "There is one  NodeManager (NM)\nper node in the cluster. All the containers (i.e. processes) on that\nnode are monitored by the NM. It takes instructions from RM and manages\nresources of that node as per RM instructions. NMs interactions are same\nfor map-reduce and for streaming applications. Running streaming\napplications requires no changes in the NM.", 
            "title": "Node Managers (NM)"
        }, 
        {
            "location": "/application_development/#rpc-protocol", 
            "text": "Communication among RM, AM, and NM is done via the Hadoop RPC\nprotocol. Streaming applications use the same protocol to send their\ndata. No changes are needed in RPC support provided by Hadoop to enable\ncommunication done by components of your application.", 
            "title": "RPC Protocol"
        }, 
        {
            "location": "/application_development/#hdfs", 
            "text": "Hadoop includes a highly fault tolerant, high throughput\ndistributed file system ( HDFS ).\nIt runs on commodity hardware, and your streaming application will, by\ndefault, use it. There is no difference between files created by a\nstreaming application and those created by map-reduce.", 
            "title": "HDFS"
        }, 
        {
            "location": "/application_development/#developing-an-application", 
            "text": "In this chapter we describe the methodology to develop an\napplication using the Realtime Streaming Platform. The platform was\ndesigned to make it easy to build and launch sophisticated streaming\napplications with the developer having to deal only with the\napplication/business logic. The platform deals with details of where to\nrun what operators on which servers and how to correctly route streams\nof data among them.", 
            "title": "Developing An Application"
        }, 
        {
            "location": "/application_development/#development-process", 
            "text": "While the platform does not mandate a specific methodology or set\nof development tools, we have recommendations to maximize productivity\nfor the different phases of application development.", 
            "title": "Development Process"
        }, 
        {
            "location": "/application_development/#design", 
            "text": "Identify common, reusable operators. Use a library\n    if possible.  Identify scalability and performance requirements before\n    designing the DAG.  Leverage attributes that the platform supports for scalability\n    and performance.  Use operators that are benchmarked and tested so that later\n    surprises are minimized. If you have glue code, create appropriate\n    unit tests for it.  Use THREAD_LOCAL locality for high throughput streams. If all\n    the operators on that stream cannot fit in one container,\n    try\u00a0NODE_LOCAL\u00a0locality. Both THREAD_LOCAL and\n    NODE_LOCAL streams avoid the Network Interface Card (NIC)\n    completly. The former uses intra-process communication to also avoid\n    serialization-deserialization overhead.  The overall throughput and latencies are are not necessarily\n    correlated to the number of operators in a simple way -- the\n    relationship is more nuanced. A lot depends on how much work\n    individual operators are doing, how many are able to operate in\n    parallel, and how much data is flowing through the arcs of the DAG.\n    It is, at times, better to break a computation down into its\n    constituent simple parts and then stitch them together via streams\n    to better utilize the compute resources of the cluster. Decide on a\n    per application basis the fine line between complexity of each\n    operator vs too many streams. Doing multiple computations in one\n    operator does save network I/O, while operators that are too complex\n    are hard to maintain.  Do not use operators that depend on the order of two streams\n    as far as possible. In such cases behavior is not idempotent.  Persist key information to HDFS if possible; it may be useful\n    for debugging later.  Decide on an appropriate fault tolerance mechanism. If some\n    data loss is acceptable, use the at-most-once mechanism as it has\n    fastest recovery.", 
            "title": "Design"
        }, 
        {
            "location": "/application_development/#creating-new-project", 
            "text": "Please refer to the  Apex Application Packages \u00a0for\nthe basic steps for creating a new project.", 
            "title": "Creating New Project"
        }, 
        {
            "location": "/application_development/#writing-the-application-code", 
            "text": "Preferably use an IDE (Eclipse, Netbeans etc.) that allows you to\nmanage dependencies and assists with the Java coding. Specific benefits\ninclude ease of managing operator library jar files, individual operator\nclasses, ports and properties. It will also highlight and assist to\nrectify issues such as type mismatches when adding streams while\ntyping.", 
            "title": "Writing the application code"
        }, 
        {
            "location": "/application_development/#testing", 
            "text": "Write test cases with JUnit or similar test framework so that code\nis tested as it is written. For such testing, the DAG can run in local\nmode within the IDE. Doing this may involve writing mock input or output\noperators for the integration points with external systems. For example,\ninstead of reading from a live data stream, the application in test mode\ncan read from and write to files. This can be done with a single\napplication DAG by instrumenting a test mode using settings in the\nconfiguration that is passed to the application factory\ninterface.  Good test coverage will not only eliminate basic validation errors\nsuch as missing port connections or property constraint violations, but\nalso validate the correct processing of the data. The same tests can be\nre-run whenever the application or its dependencies change (operator\nlibraries, version of the platform etc.)", 
            "title": "Testing"
        }, 
        {
            "location": "/application_development/#running-an-application", 
            "text": "The platform provides a commandline tool called  apex  for managing applications (launching,\nkilling, viewing, etc.). This tool was already discussed above briefly\nin the section entitled Running the Test Application. It will introspect\nthe jar file specified with the launch command for applications (classes\nthat implement ApplicationFactory) or properties files that define\napplications. It will also deploy the dependency jar files from the\napplication package to the cluster.  Apex CLI can run the application in local mode (i.e. outside a\ncluster). It is recommended to first run the application in local mode\nin the development environment before launching on the Hadoop cluster.\nThis way some of the external system integration and correct\nfunctionality of the application can be verified in an easier to debug\nenvironment before testing distributed mode.  For more details on CLI please refer to the  Apex CLI Guide .", 
            "title": "Running an application"
        }, 
        {
            "location": "/application_development/#application-api", 
            "text": "This section introduces the API to write a streaming application.\nThe work involves connecting operators via streams to form the logical\nDAG. The steps are    Instantiate an application (DAG)    (Optional) Set Attributes   Assign application name  Set any other attributes as per application requirements     Create/re-use and instantiate operators   Assign operator name that is unique within the  application  Declare schema upfront for each operator (and thereby its ports)  (Optional) Set properties\u00a0 and attributes on the dag as per specification  Connect ports of operators via streams  Each stream connects one output port of an operator to one or  more input ports of other operators.  (Optional) Set attributes on the streams       Test the application.    There are two methods to create an application, namely Java, and\nProperties file. Java API is for applications being developed by humans,\nand properties file (Hadoop like) is more suited for DAGs generated by\ntools.", 
            "title": "Application API"
        }, 
        {
            "location": "/application_development/#java-api", 
            "text": "The Java API is the most common way to create a streaming\napplication. It is meant for application developers who prefer to\nleverage the features of Java, and the ease of use and enhanced\nproductivity provided by IDEs like NetBeans or Eclipse. Using Java to\nspecify the application provides extra validation abilities of Java\ncompiler, such as compile time checks for type safety at the time of\nwriting the code. Later in this chapter you can read more about\nvalidation support in the platform.  The developer specifies the streaming application by implementing\nthe ApplicationFactory interface, which is how platform tools (CLI etc.)\nrecognize and instantiate applications. Here we show how to create a\nYahoo! Finance application that streams the last trade price of a ticker\nand computes the high and low price in every 1 min window. Run above\n test application\u00a0to execute the\nDAG in local mode within the IDE.  Let us revisit how the Yahoo! Finance test application constructs the DAG:  public class Application implements StreamingApplication\n{\n\n  ...\n\n  @Override\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n    dag.getAttributes().attr(DAG.STRAM_WINDOW_SIZE_MILLIS).set(streamingWindowSizeMilliSeconds);\n\n    StockTickInput tick = getStockTickInputOperator( StockTickInput , dag);\n    SumKeyVal String, Long  dailyVolume = getDailyVolumeOperator( DailyVolume , dag);\n    ConsolidatorKeyVal String,Double,Long,String,?,?  quoteOperator = getQuoteOperator( Quote , dag);\n\n    RangeKeyVal String, Double  highlow = getHighLowOperator( HighLow , dag, appWindowCountMinute);\n    SumKeyVal String, Long  minuteVolume = getMinuteVolumeOperator( MinuteVolume , dag, appWindowCountMinute);\n    ConsolidatorKeyVal String,HighLow,Long,?,?,?  chartOperator = getChartOperator( Chart , dag);\n\n    SimpleMovingAverage String, Double  priceSMA = getPriceSimpleMovingAverageOperator( PriceSMA , dag, appWindowCountSMA);\n\n    dag.addStream( price , tick.price, quoteOperator.in1, highlow.data, priceSMA.data);\n    dag.addStream( vol , tick.volume, dailyVolume.data, minuteVolume.data);\n    dag.addStream( time , tick.time, quoteOperator.in3);\n    dag.addStream( daily_vol , dailyVolume.sum, quoteOperator.in2);\n\n    dag.addStream( quote_data , quoteOperator.out, getConsole( quoteConsole , dag,  QUOTE ));\n\n    dag.addStream( high_low , highlow.range, chartOperator.in1);\n    dag.addStream( vol_1min , minuteVolume.sum, chartOperator.in2);\n    dag.addStream( chart_data , chartOperator.out, getConsole( chartConsole , dag,  CHART ));\n\n    dag.addStream( sma_price , priceSMA.doubleSMA, getConsole( priceSMAConsole , dag,  Price SMA ));\n\n    return dag;\n  }\n}", 
            "title": "Java API"
        }, 
        {
            "location": "/application_development/#json-file-dag-specification", 
            "text": "In addition to Java, you can also specify the DAG using JSON, provided the operators in the DAG are present in the dependency jars. Create src/main/resources/app directory under your app package project, and put your JSON files there. This is the specification of a JSON file that specifies an application.  Create a json file under src/main/resources/app, For example  myApplication.json  {\n   description :  {application description} ,\n   operators : [\n    {\n       name :  {operator name} ,\n       class :  {fully qualified class name of the operator} ,\n       properties : {\n         {property key} :  {property value} ,\n        ...\n      }\n    }, ...\n  ],\n   streams : [\n    {\n       name :  {stream name} ,\n       source : {\n         operatorName :  {source operator name} ,\n         portName :  {source operator output port name} \n      }\n       sinks : [\n        {\n           operatorName :  {sink operator name} ,\n           portName :  {sink operator input port name} \n        }, ...\n      ]\n    }, ...\n  ]\n}   The name of the JSON file is taken as the name of the application.  The  description  field is the description of the application and is optional.  The  operators  field is the list of operators the application has. You can specifiy the name, the Java class, and the properties of each operator here.  The  streams  field is the list of streams that connects the operators together to form the DAG. Each stream consists of the stream name, the operator and port that it connects from, and the list of operators and ports that it connects to. Note that you can connect from  one  output port of an operator to  multiple  different input ports of different operators.   In Apex Malhar, there is an  example  in the Pi Demo doing just that.", 
            "title": "JSON File DAG Specification"
        }, 
        {
            "location": "/application_development/#properties-file-dag-specification", 
            "text": "The platform also supports specification of a DAG via a properties\nfile. The aim here to make it easy for tools to create and run an\napplication. This method of specification does not have the Java\ncompiler support of compile time check, but since these applications\nwould be created by software, they should be correct by construction.\nThe syntax is derived from Hadoop properties and should be easy for\nfolks who are used to creating software that integrated with\nHadoop.  Under the src/main/resources/app directory (create if it doesn't exist), create a properties file.\nFor example  myApplication.properties  # input operator that reads from a file\napex.operator.inputOp.classname=com.acme.SampleInputOperator\napex.operator.inputOp.fileName=somefile.txt\n\n# output operator that writes to the console\napex.operator.outputOp.classname=com.acme.ConsoleOutputOperator\n\n# stream connecting both operators\napex.stream.inputStream.source=inputOp.outputPort\napex.stream.inputStream.sinks=outputOp.inputPort  Above snippet is intended to convey the basic idea of specifying\nthe DAG using properties file. Operators would come from a predefined\nlibrary and referenced in the specification by class name and port names\n(obtained from the library providers documentation or runtime\nintrospection by tools). For those interested in details, see later\nsections and refer to the  Operation and\nInstallation Guide\u00a0mentioned above.", 
            "title": "Properties File DAG Specification"
        }, 
        {
            "location": "/application_development/#attributes", 
            "text": "Attributes impact the runtime behavior of the application. They do\nnot impact the functionality. An example of an attribute is application\nname. Setting it changes the application name. Another example is\nstreaming window size. Setting it changes the streaming window size from\nthe default value to the specified value. Users cannot add new\nattributes, they can only choose from the ones that come packaged and\npre-supported by the platform. Details of attributes are covered in the\n Operation and Installation\nGuide.", 
            "title": "Attributes"
        }, 
        {
            "location": "/application_development/#operators", 
            "text": "Operators\u00a0are basic compute units.\nOperators process each incoming tuple and emit zero or more tuples on\noutput ports as per the business logic. The data flow, connectivity,\nfault tolerance (node outage), etc. is taken care of by the platform. As\nan operator developer, all that is needed is to figure out what to do\nwith the incoming tuple and when (and which output port) to send out a\nparticular output tuple. Correctly designed operators will most likely\nget reused. Operator design needs care and foresight. For details, refer\nto the   Operator Developer Guide . As an application developer you need to connect operators\nin a way that it implements your business logic. You may also require\noperator customization for functionality and use attributes for\nperformance/scalability etc.  All operators process tuples asynchronously in a distributed\ncluster. An operator cannot assume or predict the exact time a tuple\nthat it emitted will get consumed by a downstream operator. An operator\nalso cannot predict the exact time when a tuple arrives from an upstream\noperator. The only guarantee is that the upstream operators are\nprocessing the current or a future window, i.e. the windowId of upstream\noperator is equals or exceeds its own windowId. Conversely the windowId\nof a downstream operator is less than or equals its own windowId. The\nend of a window operation, i.e. the API call to endWindow on an operator\nrequires that all upstream operators have finished processing this\nwindow. This means that completion of processing a window propagates in\na blocking fashion through an operator. Later sections provides more\ndetails on streams and data flow of tuples.  Each operator has a unique name within the DAG as provided by the\nuser. This is the name of the operator in the logical plan. The name of\nthe operator in the physical plan is an integer assigned to it by STRAM.\nThese integers are use the sequence from 1 to N, where N is total number\nof physically unique operators in the DAG. \u00a0Following the same rule,\neach partitioned instance of a logical operator has its own integer as\nan id. This id along with the Hadoop container name uniquely identifies\nthe operator in the execution plan of the DAG. The logical names and the\nphysical names are required for web service support. Operators can be\naccessed via both names. These same names are used while interacting\nwith apex cli\u00a0to access an operator.\nIdeally these names should be self-descriptive. For example in Figure 1,\nthe node named \u201cDaily volume\u201d has a physical identifier of 2.", 
            "title": "Operators"
        }, 
        {
            "location": "/application_development/#operator-interface", 
            "text": "Operator interface in a DAG consists of ports,\u00a0properties,\u00a0and attributes.\nOperators interact with other components of the DAG via ports. Functional behavior of the operators\ncan be customized via parameters. Run time performance and physical\ninstantiation is controlled by attributes. Ports and parameters are\nfields (variables) of the Operator class/object, while attributes are\nmeta information that is attached to the operator object via an\nAttributeMap. An operator must have at least one port. Properties are\noptional. Attributes are provided by the platform and always have a\ndefault value that enables normal functioning of operators.", 
            "title": "Operator Interface"
        }, 
        {
            "location": "/application_development/#ports", 
            "text": "Ports are connection points by which an operator receives and\nemits tuples. These should be transient objects instantiated in the\noperator object, that implement particular interfaces. Ports should be\ntransient as they contain no state. They have a pre-defined schema and\ncan only be connected to other ports with the same schema. An input port\nneeds to implement the interface  Operator.InputPort\u00a0and\ninterface Sink. A default\nimplementation of these is provided by the abstract class DefaultInputPort. An output port needs to\nimplement the interface  Operator.OutputPort. A default implementation\nof this is provided by the concrete class DefaultOutputPort. These two are a quick way to\nimplement the above interfaces, but operator developers have the option\nof providing their own implementations.  Here are examples of an input and an output port from the operator\nSum.  @InputPortFieldAnnotation(name =  data )\npublic final transient DefaultInputPort V  data = new DefaultInputPort V () {\n  @Override\n  public void process(V tuple)\n  {\n    ...\n  }\n}\n@OutputPortFieldAnnotation(optional=true)\npublic final transient DefaultOutputPort V  sum = new DefaultOutputPort V (){ \u2026 };  The process call is in the Sink interface. An emit on an output\nport is done via emit(tuple) call. For the above example it would be\nsum.emit(t), where the type of t is the generic parameter V.  There is no limit on how many ports an operator can have. However\nany operator must have at least one port. An operator with only one port\nis called an Input Adapter if it has no input port and an Output Adapter\nif it has no output port. These are special operators needed to get/read\ndata from outside system/source into the application, or push/write data\ninto an outside system/sink. These could be in Hadoop or outside of\nHadoop. These two operators are in essence gateways for the streaming\napplication to communicate with systems outside the application.  Port connectivity can be validated during compile time by adding\nPortFieldAnnotations shown above. By default all ports have to be\nconnected, to allow a port to go unconnected, you need to add\n\u201coptional=true\u201d to the annotation.  Attributes can be specified for ports that affect the runtime\nbehavior. An example of an attribute is parallel partition that specifes\na parallel computation flow per partition. It is described in detail in\nthe Parallel Partitions section. Another example is queue capacity that specifies the buffer size for the\nport. Details of attributes are covered in  Operation and Installation Guide.", 
            "title": "Ports"
        }, 
        {
            "location": "/application_development/#properties", 
            "text": "Properties are the abstractions by which functional behavior of an\noperator can be customized. They should be non-transient objects\ninstantiated in the operator object. They need to be non-transient since\nthey are part of the operator state and re-construction of the operator\nobject from its checkpointed state must restore the operator to the\ndesired state. Properties are optional, i.e. an operator may or may not\nhave properties; they are part of user code and their values are not\ninterpreted by the platform in any way.  All non-serializable objects should be declared transient.\nExamples include sockets, session information, etc. These objects should\nbe initialized during setup call, which is called every time the\noperator is initialized.", 
            "title": "Properties"
        }, 
        {
            "location": "/application_development/#attributes_1", 
            "text": "Attributes are values assigned to the operators that impact\nrun-time. This includes things like the number of partitions, at most\nonce or at least once or exactly once recovery modes, etc. Attributes do\nnot impact functionality of the operator. Users can change certain\nattributes in runtime. Users cannot add attributes to operators; they\nare pre-defined by the platform. They are interpreted by the platform\nand thus cannot be defined in user created code (like properties).\nDetails of attributes are covered in   Configuration Guide .", 
            "title": "Attributes"
        }, 
        {
            "location": "/application_development/#operator-state", 
            "text": "The state of an operator is defined as the data that it transfers\nfrom one window to a future window. Since the computing model of the\nplatform is to treat windows like micro-batches, the operator state can\nbe checkpointed every Nth window, or every T units of time, where T is significantly greater\nthan the streaming window. \u00a0When an operator is checkpointed, the entire\nobject is written to HDFS. \u00a0The larger the amount of state in an\noperator, the longer it takes to recover from a failure. A stateless\noperator can recover much quicker than a stateful one. The needed\nwindows are preserved by the upstream buffer server and are used to\nrecompute the lost windows, and also rebuild the buffer server in the\ncurrent container.  The distinction between Stateless and Stateful is based solely on\nthe need to transfer data in the operator from one window to the next.\nThe state of an operator is independent of the number of ports.", 
            "title": "Operator State"
        }, 
        {
            "location": "/application_development/#stateless", 
            "text": "A Stateless operator is defined as one where no data is needed to\nbe kept at the end of every window. This means that all the computations\nof a window can be derived from all the tuples the operator receives\nwithin that window. This guarantees that the output of any window can be\nreconstructed by simply replaying the tuples that arrived in that\nwindow. Stateless operators are more efficient in terms of fault\ntolerance, and cost to achieve SLA.", 
            "title": "Stateless"
        }, 
        {
            "location": "/application_development/#stateful", 
            "text": "A Stateful operator is defined as one where data is needed to be\nstored at the end of a window for computations occurring in later\nwindow; a common example is the computation of a sum of values in the\ninput tuples.", 
            "title": "Stateful"
        }, 
        {
            "location": "/application_development/#operator-api", 
            "text": "The Operator API consists of methods that operator developers may\nneed to override. In this section we will discuss the Operator APIs from\nthe point of view of an application developer. Knowledge of how an\noperator works internally is critical for writing an application. Those\ninterested in the details should refer to  Malhar Operator Developer Guide.  The APIs are available in three modes, namely Single Streaming\nWindow, Sliding Application Window, and Aggregate Application Window.\nThese are not mutually exclusive, i.e. an operator can use single\nstreaming window as well as sliding application window. A physical\ninstance of an operator is always processing tuples from a single\nwindow. The processing of tuples is guaranteed to be sequential, no\nmatter which input port the tuples arrive on.  In the later part of this section we will evaluate three common\nuses of streaming windows by applications. They have different\ncharacteristics and implications on optimization and recovery mechanisms\n(i.e. algorithm used to recover a node after outage) as discussed later\nin the section.", 
            "title": "Operator API"
        }, 
        {
            "location": "/application_development/#streaming-window", 
            "text": "Streaming window is atomic micro-batch computation period. The API\nmethods relating to a streaming window are as follows  public void process( tuple_type  tuple) // Called on the input port on which the tuple arrives\npublic void beginWindow(long windowId) // Called at the start of the window as soon as the first begin_window tuple arrives\npublic void endWindow() // Called at the end of the window after end_window tuples arrive on all input ports\npublic void setup(OperatorContext context) // Called once during initialization of the operator\npublic void teardown() // Called once when the operator is being shutdown  A tuple can be emitted in any of the three streaming run-time\ncalls, namely beginWindow, process, and endWindow but not in setup or\nteardown.", 
            "title": "Streaming Window"
        }, 
        {
            "location": "/application_development/#aggregate-application-window", 
            "text": "An operator with an aggregate window is stateful within the\napplication window timeframe and possibly stateless at the end of that\napplication window. An size of an aggregate application window is an\noperator attribute and is defined as a multiple of the streaming window\nsize. The platform recognizes this attribute and optimizes the operator.\nThe beginWindow, and endWindow calls are not invoked for those streaming\nwindows that do not align with the application window. For example in\ncase of streaming window of 0.5 second and application window of 5\nminute, an application window spans 600 streaming windows (5*60*2 =\n600). At the start of the sequence of these 600 atomic streaming\nwindows, a beginWindow gets invoked, and at the end of these 600\nstreaming windows an endWindow gets invoked. All the intermediate\nstreaming windows do not invoke beginWindow or endWindow. Bookkeeping,\nnode recovery, stats, UI, etc. continue to work off streaming windows.\nFor example if operators are being checkpointed say on an average every\n30th window, then the above application window would have about 20\ncheckpoints.", 
            "title": "Aggregate Application Window"
        }, 
        {
            "location": "/application_development/#sliding-application-window", 
            "text": "A sliding window is computations that requires previous N\nstreaming windows. After each streaming window the Nth past window is\ndropped and the new window is added to the computation. An operator with\nsliding window is a stateful operator at end of any window. The sliding\nwindow period is an attribute and is a multiple of streaming window. The\nplatform recognizes this attribute and leverages it during bookkeeping.\nA sliding aggregate window with tolerance to data loss does not have a\nvery high bookkeeping cost. The cost of all three recovery mechanisms,\n at most once\u00a0(data loss tolerant),\nat least once\u00a0(data loss\nintolerant), and exactly once\u00a0(data\nloss intolerant and no extra computations) is same as recovery\nmechanisms based on streaming window. STRAM is not able to leverage this\noperator for any extra optimization.", 
            "title": "Sliding Application Window"
        }, 
        {
            "location": "/application_development/#single-vs-multi-input-operator", 
            "text": "A single-input operator by definition has a single upstream\noperator, since there can only be one writing port for a stream. \u00a0If an\noperator has a single upstream operator, then the beginWindow on the\nupstream also blocks the beginWindow of the single-input operator. For\nan operator to start processing any window at least one upstream\noperator has to start processing that window. A multi-input operator\nreads from more than one upstream ports. Such an operator would start\nprocessing as soon as the first begin_window event arrives. However the\nwindow would not close (i.e. invoke endWindow) till all ports receive\nend_window events for that windowId. Thus the end of a window is a\nblocking event. As we saw earlier, a multi-input operator is also the\npoint in the DAG where windows of all upstream operators are\nsynchronized. The windows (atomic micro-batches) from a faster (or just\nahead in processing) upstream operators are queued up till the slower\nupstream operator catches up. STRAM monitors such bottlenecks and takes\ncorrective actions. The platform ensures minimal delay, i.e processing\nstarts as long as at least one upstream operator has started\nprocessing.", 
            "title": "Single vs Multi-Input Operator"
        }, 
        {
            "location": "/application_development/#recovery-mechanisms", 
            "text": "Application developers can set any of the recovery mechanisms\nbelow to deal with node outage. In general, the cost of recovery depends\non the state of the operator, while data integrity is dependant on the\napplication. The mechanisms are per window as the platform treats\nwindows as atomic compute units. Three recovery mechanisms are\nsupported, namely   At-least-once: All atomic batches are processed at least once.\n    No data loss occurs.  At-most-once: All atomic batches are processed at most once.\n    Data loss is possible; this is the most efficient setting.  Exactly-once: All atomic batches are processed exactly once.\n    No data loss occurs; this is the least efficient setting since\n    additional work is needed to ensure proper semantics.   At-least-once is the default. During a recovery event, the\noperator connects to the upstream buffer server and asks for windows to\nbe replayed. At-least-once and exactly-once mechanisms start from its\ncheckpointed state. At-most-once starts from the next begin-window\nevent.  Recovery mechanisms can be specified per Operator while writing\nthe application as shown below.  Operator o = dag.addOperator(\u201coperator\u201d, \u2026);\ndag.setAttribute(o,  OperatorContext.PROCESSING_MODE,  ProcessingMode.AT_MOST_ONCE);  Also note that once an operator is attributed to AT_MOST_ONCE,\nall the operators downstream to it have to be AT_MOST_ONCE. The client\nwill give appropriate warnings or errors if that\u2019s not the case.  Details are explained in the chapter on Fault Tolerance below.", 
            "title": "Recovery Mechanisms"
        }, 
        {
            "location": "/application_development/#streams", 
            "text": "A stream\u00a0is a connector\n(edge) abstraction, and is a fundamental building block of the platform.\nA stream consists of tuples that flow from one port (called the\noutput\u00a0port) to one or more ports\non other operators (called  input\u00a0ports) another -- so note a potentially\nconfusing aspect of this terminology: tuples enter a stream through its\noutput port and leave via one or more input ports. A stream has the\nfollowing characteristics   Tuples are always delivered in the same order in which they\n    were emitted.  Consists of a sequence of windows one after another. Each\n    window being a collection of in-order tuples.  A stream that connects two containers passes through a\n    buffer server.  All streams can be persisted (by default in HDFS).  Exactly one output port writes to the stream.  Can be read by one or more input ports.  Connects operators within an application, not outside\n    an application.  Has an unique name within an application.  Has attributes which act as hints to STRAM.   Streams have four modes, namely in-line, in-node, in-rack,\n    and other. Modes may be overruled (for example due to lack\n    of containers). They are defined as follows:   THREAD_LOCAL: In the same thread, uses thread\n    stack (intra-thread). This mode can only be used for a downstream\n    operator which has only one input port connected; also called\n    in-line.  CONTAINER_LOCAL: In the same container (intra-process); also\n    called in-container.  NODE_LOCAL: In the same Hadoop node (inter processes, skips\n    NIC); also called in-node.  RACK_LOCAL: On nodes in the same rack; also called\n    in-rack.  unspecified: No guarantee. Could be anywhere within the\n    cluster     An example of a stream declaration is given below  DAG dag = new DAG();\n \u2026\ndag.addStream( views , viewAggregate.sum, cost.data).setLocality(CONTAINER_LOCAL); // A container local  stream\ndag.addStream(\u201cclicks\u201d, clickAggregate.sum, rev.data); // An example of unspecified locality  The platform guarantees in-order delivery of tuples in a stream.\nSTRAM views each stream as collection of ordered windows. Since no tuple\ncan exist outside a window, a replay of a stream consists of replay of a\nset of windows. When multiple input ports read the same stream, the\nexecution plan of a stream ensures that each input port is logically not\nblocked by the reading of another input port. The schema of a stream is\nsame as the schema of the tuple.  In a stream all tuples emitted by an operator in a window belong\nto that window. A replay of this window would consists of an in-order\nreplay of all the tuples. Thus the tuple order within a stream is\nguaranteed. However since an operator may receive multiple streams (for\nexample an operator with two input ports), the order of arrival of two\ntuples belonging to different streams is not guaranteed. In general in\nan asynchronous distributed architecture this is expected. Thus the\noperator (specially one with multiple input ports) should not depend on\nthe tuple order from two streams. One way to cope with this\nindeterminate order, if necessary, is to wait to get all the tuples of a\nwindow and emit results in endWindow call. All operator templates\nprovided as part of Malhar operator library follow these principles.  A logical stream gets partitioned into physical streams each\nconnecting the partition to the upstream operator. If two different\nattributes are needed on the same stream, it should be split using\nStreamDuplicator\u00a0operator.  Modes of the streams are critical for performance. An in-line\nstream is the most optimal as it simply delivers the tuple as-is without\nserialization-deserialization. Streams should be marked\ncontainer_local, specially in case where there is a large tuple volume\nbetween two operators which then on drops significantly. Since the\nsetLocality call merely provides a hint, STRAM may ignore it. An In-node\nstream is not as efficient as an in-line one, but it is clearly better\nthan going off-node since it still avoids the potential bottleneck of\nthe network card.  THREAD_LOCAL and CONTAINER_LOCAL streams do not use a buffer\nserver as this stream is in a single process. The other two do.", 
            "title": "Streams"
        }, 
        {
            "location": "/application_development/#affinity-rules", 
            "text": "Affinity Rules in Apex provide a way to specify hints on how operators should be deployed in a cluster. Sometimes you may want to allocate certain operators on the same or different nodes for performance or other reasons. Affinity rules can be used in such cases to make sure these considerations are honored by platform.  There can be two types of rules: Affinity and anti-affinity rules. Affinity rule indicates that the group of operators in the rule should be allocated together.  On the other hand, anti-affinity rule indicates that the group of operators should be allocated separately.", 
            "title": "Affinity Rules"
        }, 
        {
            "location": "/application_development/#specifying-affinity-rules", 
            "text": "A list of Affinity Rules can be specified for an application by setting attribute: DAGContext.AFFINITY_RULES_SET.\nHere is an example for setting Affinity rules for an application in the populateDag method:  AffinityRulesSet ruleSet = new AffinityRulesSet();\nList AffinityRule  rules = new ArrayList ();\n// Add Affinity rules as per requirement\nrules.add(new AffinityRule(Type.ANTI_AFFINITY, Locality.NODE_LOCAL, false,  rand ,  operator1 ,  operator2 ));\nrules.add(new AffinityRule(Type.AFFINITY, Locality.CONTAINER_LOCAL, false,  console ,  rand ));\nruleSet.setAffinityRules(rules);\ndag.setAttribute(DAGContext.AFFINITY_RULES_SET, ruleSet);  As shown in the example above, each rule has a type AFFINITY or ANTI_AFFINITY indicating whether operators in the group should be allocated together or separate. These can be applied on any operators in DAG.  The operators for rule can be provided either as a list of 2 or more operator names or as a regular expression. The regex should match at least two operators in DAG to be considered a valid rule. Here is example of rule with regex to allocate all the operators in DAG on the same node:  // Allocate all operators starting with console on same container\nrules.add(new AffinityRule(Type.AFFINITY,  *  , Locality.NODE_LOCAL, false));  Likewise, operators for rule can also be added as a list:  // Rule for Operators rand, operator1 and operator2 should not be allocated on same node\nrules.add(new AffinityRule(Type.ANTI_AFFINITY, Locality.NODE_LOCAL, false,  rand ,  operator1 ,  operator2 ));  To indicate affinity or anti-affinity between partitions of a single operator, list should contain the same operator name twice, as shown in the example below for  'TestOperator'. This will ensure that platform will allocate physical partitions of this operator on different nodes.  // Rule for Partitions of TestOperator should not be allocated on the same node\nrules.add(new AffinityRule(Type.ANTI_AFFINITY, Locality.NODE_LOCAL, false,  TestOperator ,  TestOperator ));  Another important parameter to indicate affinity rule is the locality constraint. Similar to stream locality, the affinity rule will be applied at either THREAD_LOCAL, CONTAINER_LOCAL or NODE_LOCAL level. Support for RACK_LOCAL is not added yet.  The last configurable parameter for affinity rules is strict or preferred rule. A true value for this parameter indicates that the rule should be relaxed in case sufficient resources are not available.", 
            "title": "Specifying Affinity Rules"
        }, 
        {
            "location": "/application_development/#specifying-affinity-rules-from-properties", 
            "text": "The same set of rules can also be added from properties.xml by setting value for attribute DAGContext.AFFINITY_RULES_SET as JSON string.  For example:  property \n     name apex.application.AffinityRulesSampleApplication.attr.AFFINITY_RULES_SET /name \n     value \n    {\n       affinityRules : [\n        {\n           operatorRegex :  console* ,\n           locality :  CONTAINER_LOCAL ,\n           type :  AFFINITY ,\n           relaxLocality : false\n        },\n        {\n          operatorsList : [\n           rand ,\n           passThru \n          ],\n           locality :  NODE_LOCAL ,\n           type :  ANTI_AFFINITY ,\n           relaxLocality : false\n        },\n        {\n           operatorsList : [\n           passThru ,\n           passThru \n          ],\n           locality :  NODE_LOCAL ,\n           type :  ANTI_AFFINITY ,\n           relaxLocality : false\n        }\n      ]\n    } /property   Affinity rules which conflict with stream locality or host preference are validated during DAG validation phase.", 
            "title": "Specifying affinity rules from properties"
        }, 
        {
            "location": "/application_development/#validating-an-application", 
            "text": "The platform provides various ways of validating the application\nspecification and data input. An understanding of these checks is very\nimportant for an application developer since it affects productivity.\nValidation of an application is done in three phases, namely   Compile Time: Caught during application development, and is\n    most cost effective. These checks are mainly done on declarative\n    objects and leverages the Java compiler. An example is checking that\n    the schemas specified on all ports of a stream are\n    mutually compatible.  Initialization Time: When the application is being\n    initialized, before submitting to Hadoop. These checks are related\n    to configuration/context of an application, and are done by the\n    logical DAG builder implementation. An example is the checking that\n    all non-optional ports are connected to other ports.  Run Time: Validations done when the application is running.\n    This is the costliest of all checks. These are checks that can only\n    be done at runtime as they involve data. For example divide by 0\n    check as part of business logic.", 
            "title": "Validating an Application"
        }, 
        {
            "location": "/application_development/#compile-time", 
            "text": "Compile time validations apply when an application is specified in\nJava code and include all checks that can be done by Java compiler in\nthe development environment (including IDEs like NetBeans or Eclipse).\nExamples include   Schema Validation: The tuples on ports are POJO (plain old\n    java objects) and compiler checks to ensure that all the ports on a\n    stream have the same schema.  Stream Check: Single Output Port and at least one Input port\n    per stream. A stream can only have one output port writer. This is\n    part of the addStream api. This\n    check ensures that developers only connect one output port to\n    a stream. The same signature also ensures that there is at least one\n    input port for a stream  Naming: Compile time checks ensures that applications\n    components operators, streams are named", 
            "title": "Compile Time"
        }, 
        {
            "location": "/application_development/#initializationinstantiation-time", 
            "text": "Initialization time validations include various checks that are\ndone post compile, and before the application starts running in a\ncluster (or local mode). These are mainly configuration/contextual in\nnature. These checks are as critical to proper functionality of the\napplication as the compile time validations.  Examples include    JavaBeans Validation :\n    Examples include   @Max(): Value must be less than or equal to the number  @Min(): Value must be greater than or equal to the\n    number  @NotNull: The value of the field or property must not be\n    null  @Pattern(regexp = \u201c....\u201d): Value must match the regular\n    expression  Input port connectivity: By default, every non-optional input\n    port must be connected. A port can be declared optional by using an\n    annotation: \u00a0 \u00a0 @InputPortFieldAnnotation(name = \"...\", optional\n    = true)  Output Port Connectivity: Similar. The annotation here is: \u00a0 \u00a0\n    @OutputPortFieldAnnotation(name = \"...\", optional = true)     Unique names in application scope: Operators, streams, must have\n    unique names.   Cycles in the dag: DAG cannot have a cycle.  Unique names in operator scope: Ports, properties, annotations\n    must have unique names.  One stream per port: A port can connect to only one stream.\n    This check applies to input as well as output ports even though an\n    output port can technically write to two streams. If you must have\n    two streams originating from a single output port, use \u00a0a\u00a0streamDuplicator operator.  Application Window Period: Has to be an integral multiple the\n    streaming window period.", 
            "title": "Initialization/Instantiation Time"
        }, 
        {
            "location": "/application_development/#run-time", 
            "text": "Run time checks are those that are done when the application is\nrunning. The real-time streaming platform provides rich run time error\nhandling mechanisms. The checks are exclusively done by the application\nbusiness logic, but the platform allows applications to count and audit\nthese. Some of these features are in the process of development (backend\nand UI) and this section will be updated as they are developed. Upon\ncompletion examples will be added to demos to illustrate these.  Error ports are output ports with error annotations. Since they\nare normal ports, they can be monitored and tuples counted, persisted\nand counts shown in the UI.", 
            "title": "Run Time"
        }, 
        {
            "location": "/application_development/#multi-tenancy-and-security", 
            "text": "Hadoop is a multi-tenant distributed operating system. Security is\nan intrinsic element of multi-tenancy as without it a cluster cannot be\nreasonably be shared among enterprise applications. Streaming\napplications follow all multi-tenancy security models used in Hadoop as\nthey are native Hadoop applications.", 
            "title": "Multi-Tenancy and Security"
        }, 
        {
            "location": "/application_development/#security", 
            "text": "The platform includes Kerberos support. Both access points, namely\nSTRAM and Bufferserver are secure. STRAM passes the token over to\nStreamingContainer, which then gives it to the Bufferserver. The most\nimportant aspect for an application developer is to note that STRAM is\nthe single point of access to ensure security measures are taken by all\ncomponents of the platform.", 
            "title": "Security"
        }, 
        {
            "location": "/application_development/#resource-limits", 
            "text": "Hadoop enforces quotas on resources. This includes hard-disk (name\nspace and total disk quota) as well as priority queues for schedulers.\nThe platform uses Hadoop resource limits to manage a streaming\napplication. In addition network I/O quotas can be enforced. An operator\ncan be dynamically partitioned if it reaches its resource limits; these\nlimits may be expressed in terms of throughput, latency, or just\naggregate resource utilization of a container.", 
            "title": "Resource Limits"
        }, 
        {
            "location": "/application_development/#scalability-and-partitioning", 
            "text": "Scalability is a foundational element of this platform and is a\nbuilding block for an eco-system where big-data meets real-time.\nEnterprises need to continually meet SLA as data grows. Without the\nability to scale as load grows, or new applications with higher loads\ncome to fruition, enterprise grade SLA cannot be met. A big issue with\nthe streaming application space is that, it is not just about high load,\nbut also the fluctuations in it. There is no way to guarantee future\nload requirements and there is a big difference between high and low\nload within a day for the same feed. Traditional streaming platforms\nsolve these two cases by simply throwing more hardware at the\nproblem.  Daily spikes are managed by ensuring enough hardware for peak\nload, which then idles during low load, and future needs are handled by\na very costly re-architecture, or investing heavily in building a\nscalable distributed operating system. Another salient and often\noverlooked cost is the need to manage SLA -- let\u2019s call it  buffer capacity. Since this means computing the\npeak load within required time, that translates to allocating enough\nresources over and above peak load as daily peaks fluctuate. For example\nan average peak load of 100 resource units (cpu and/or memory and/or\nnetwork) may mean allocating about 200 resource units to be safe. A\ndistributed cluster that cannot dynamically scale up and down, in effect\npays buffer capacity per application. Another big aspect of streaming\napplications is that the load is not just ingestion rate, more often\nthan not, the internal operators produce lot more events than the\ningestion rate. For example a dimensional data (with, say  d\u00a0dimensions) computation needs 2*d -1\u00a0computations per ingested event. A lot\nof applications have over 10 dimensions, i.e over 1000 computations per\nincoming event and these need to be distributed across the cluster,\nthereby causing an explosion in the throughput (events/sec) that needs\nto be managed.  The platform is designed to handle such cases at a very low cost.\nThe platform scales linearly with Hadoop. If applications need more\nresources, the enterprise can simply add more commodity nodes to Hadoop\nwithout any downtime, and the Hadoop native platform will take care of\nthe rest. If some nodes go bad, these can be removed without downtime.\nThe daily peaks and valleys in the load are managed by the platform by\ndynamically scaling at the peak and then giving the resources back to\nHadoop during low load. This means that a properly designed Hadoop\ncluster does several things for enterprises: (a) reduces the cost of\nhardware due to use of commodity hardware (b) shares buffer capacity\nacross all applications as peaks of all applications may not align and\n(c) raises the average CPU usage on a 24x7 basis. As a general design\nthis is similar to scale that a map-reduce application can deliver. In\nthe following sections of this chapter we will see how this is\ndone.", 
            "title": "Scalability and Partitioning"
        }, 
        {
            "location": "/application_development/#partitioning", 
            "text": "If all tuples sent through the stream(s) that are connected to the\ninput port(s) of an operator in the DAG are received by a single\nphysical instance of that operator, that operator can become a\nperformance bottleneck. This leads to scalability issues when\nthroughput, memory, or CPU needs exceed the processing capacity of that\nsingle instance.  To address the problem, the platform offers the capability to\npartition the inflow of data so that it is divided across multiple\nphysical instances of a logical operator in the DAG. There are two\nfunctional ways to partition   Load balance: Incoming load is simply partitioned\n    into stream(s) that go to separate instances of physical operators\n    and scalability is achieved via adding more physical operators. Each\n    tuple is sent to physical operator (partition) based on a\n    round-robin or other similar algorithm. This scheme scales linearly.\n    A lot of key based computations can load balance in the platform due\n    to the ability to insert  Unifiers. For many computations, the\n    endWindow and Unifier setup is similar to the combiner and reducer\n    mechanism in a Map-Reduce computation.  Sticky Key: The key assertion is that distribution of tuples\n    are sticky, i.e the data with\n    same key will always be processed by the same physical operator, no\n    matter how many times it is sent through the stream. This stickiness\n    will continue even if the number of partitions grows dynamically and\n    can eventually be leveraged for advanced features like\n    bucket testing. How this is accomplished and what is required to\n    develop compliant operators will be explained below.   We plan to add more partitioning mechanisms proactively to the\nplatform over time as needed by emerging usage patterns. The aim is to\nallow enterprises to be able to focus on their business logic, and\nsignificantly reduce the cost of operability. As an enabling technology\nfor managing high loads, this platform provides enterprises with a\nsignificant innovative edge. Scalability and Partitioning is a\nfoundational building block for this platform.", 
            "title": "Partitioning"
        }, 
        {
            "location": "/application_development/#sticky-partition-vs-round-robin", 
            "text": "As noted above, partitioning via sticky key is data aware but\nround-robin partitioning is not. An example for non-sticky load\nbalancing would be round robin distribution over multiple instances,\nwhere for example a tuple stream of  A, A,\nA with 3 physical operator\ninstances would result in processing of a single A by each of the instances, In contrast, sticky\npartitioning means that exactly one instance of the operators will\nprocess all of the  Atuples if they\nfall into the same bucket, while B\nmay be processed by another operator. Data aware mapping of\ntuples to partitions (similar to distributed hash table) is accomplished\nvia Stream Codecs. In later sections we would show how these two\napproaches can be used in combination.", 
            "title": "Sticky Partition vs Round Robin"
        }, 
        {
            "location": "/application_development/#stream-codec", 
            "text": "The platform does not make assumptions about the tuple\ntype, it could be any Java object. The operator developer knows what\ntuple type an input port expects and is capable of processing. Each\ninput port has a stream codec \u00a0associated that defines how data is serialized when transmitted over a socket\nstream; it also defines another\nfunction that computes the partition hash key for the tuple. The engine\nuses that key to determine which physical instance(s) \u00a0(for a\npartitioned operator) receive that \u00a0tuple. For this to work, consistent hashing is required.\nThe default codec uses the Java Object#hashCode function, which is\nsufficient for basic types such as Integer, String etc. It will also\nwork with custom tuple classes as long as they implement hashCode\nappropriately. Reliance on hashCode may not work when generic containers\nare used that do not hash the actual data, such as standard collection\nclasses (HashMap etc.), in which case a custom stream codec must be\nassigned to the input port.", 
            "title": "Stream Codec"
        }, 
        {
            "location": "/application_development/#static-partitioning", 
            "text": "DAG designers can specify at design time how they would like\ncertain operators to be partitioned. STRAM then instantiates the DAG\nwith the physical plan which adheres to the partitioning scheme defined\nby the design. This plan is the initial partition of the application. In\nother words, Static Partitioning is used to tell STRAM to compute the\nphysical DAG from a logical DAG once, without taking into consideration\nruntime states or loads of various operators.", 
            "title": "Static Partitioning"
        }, 
        {
            "location": "/application_development/#dynamic-partitioning", 
            "text": "In streaming applications the load changes during the day, thus\ncreating situations where the number of partitioned operator instances\nneeds to adjust dynamically. The load can be measured in terms of\nprocessing within the DAG based on throughput, or latency, or\nconsiderations in external system components (time based etc.) that the\nplatform may not be aware of. Whatever the trigger, the resource\nrequirement for the current processing needs to be adjusted at run-time.\nThe platform may detect that operator instances are over or under\nutilized and may need to dynamically adjust the number of instances on\nthe fly. More instances of a logical operator may be required (partition\nsplit) or underutilized operator instances may need decommissioning\n(partition merge). We refer to either of the changes as dynamic\npartitioning. The default partitioning scheme supports split and merge\nof partitions, but without state transfer. The contract of the\nPartitioner\u00a0interface allows the operator\ndeveloper to implement split/merge and the associated state transfer, if\nnecessary.  Since partitioning is a key scalability measure, our goal is to\nmake it as simple as possible without removing the flexibility needed\nfor sophisticated applications. Basic partitioning can be enabled at\ncompile time through the DAG specification. A slightly involved\npartitioning involves writing custom codecs to calculate data aware\npartitioning scheme. More complex partitioning cases may require users\nto provide a custom implementation of Partitioner, which gives the\ndeveloper full control over state transfer between multiple instances of\nthe partitioned operator.", 
            "title": "Dynamic Partitioning"
        }, 
        {
            "location": "/application_development/#default-partitioning", 
            "text": "The platform provides a default partitioning implementation that\ncan be enabled without implementing Partitioner\u00a0(or writing any other extra Java\ncode), which is designed to support simple sticky partitioning out of\nthe box for operators with logic agnostic to the partitioning scheme\nthat can be enabled by means of DAG construction alone.  Typically an operator that can work with the default partitioning\nscheme would have a single input port. If there are multiple input\nports, only one port will be partitioned (the port first connected in\nthe DAG). The number of partitions will be calculated based on the\ninitial partition count - set as attribute on the operator in the DAG\n(if the attribute is not present, partitioning is off). Each partition\nwill handle tuples based on matching the lower bits of the hash code.\nFor example, if the tuple type was Integer and 2 partitions requested,\nall even numbers would go to one operator instance and all odd numbers\nto the other.", 
            "title": "Default Partitioning"
        }, 
        {
            "location": "/application_development/#default-dynamic-partitioning", 
            "text": "Triggering partition load evaluation and repartitioning action\nitself are separate concerns. Triggers are not specified further here,\nwe are planning to support it in a customizable fashion that, for\nexample, allows latency or SLA based implementations. Triggers calculate\na load indicator (signed number) that tells the framework that a given\npartition is either underutilized, operating normally within the\nexpected thresholds or overloaded and becoming a bottleneck. The\nindicator is then presented to the partitioning logic (default or custom\nimplementation of Partitioner) to provide the opportunity to make any\nneeded adjustments.  The default partitioning logic divides the key space\naccording to the lower bits of the hash codes that are generated by the\nstream codec, by assigning each partitioned operator instance via a bit\nmask and the respective value. For example, the operator may have\ninitially two partitions,  0and 1, each\nwith a bit mask of 1.\nIn the case where load evaluation flags partition\n0  as over utilized\n(most data tuples processed yield a hash code with lowest bit cleared),\napartition split\u00a0occurs, resulting in 00\nand  10with mask 11. Operator instance 0 will be replaced with 2 new instances and partition\n1  remains unchanged,\nresulting in three active partitions. The same process could repeat if\nmost tuples fall into the01 partition, leading to a split into 001  and101\nwith mask 111, etc.  Should load decrease in two sibling partitions, a\npartition merge\u00a0could\nreverse the split, reducing the mask length and replacing two operators\nwith one. Should only one of two sibling partitions be underutilized,\n it cannot be\u00a0merged.\nInstead, the platform can attempt to deploy the affected operator\ninstance along with other operator instances for resource sharing\namongst underutilized partitions (not implemented yet). Keeping separate\noperator instances allows\u00a0us  to\npin load increases directly to the affected instance with a single\nspecific partition key, which would not be the case had we assigned a\nshared instance to handle multiple keys.", 
            "title": "Default Dynamic Partitioning"
        }, 
        {
            "location": "/application_development/#nxm-partitions", 
            "text": "When two consecutive logical operators are partitioned a special\noptimization is done. Technically the output of the first operator\nshould be unified and streamed to the next logical node. But that can\ncreate a network bottleneck. The platform optimizes this by partitioning\nthe output stream of each partition of the first operator as per the\npartitions needed by the next operator. For example if the first\noperator has N partitions and the second operator has M partitions then\neach of the N partitions would send out M streams. The first of each of\nthese M streams would be unified and routed to the first of the M\npartitions, and so on. Such an optimization allows for higher\nscalability and eliminates a network bottleneck (one unifier in between\nthe two operators) by having M unifiers. This also enables the\napplication to perform within the resource limits enforced by YARN.\nSTRAM has a much better understanding and estimation of unifier resource\nneeds and is thus able to optimize for resource constraints.  Figure 5 shows a case where we have a 3x2 partition; the single\nintermediate unifier between operator 1\u00a0and 2\u00a0is\noptimized away. The partition computation for operator  2\u00a0is executed on outbound streams of each\npartitions of operator 1. Each\npartition of operator 2\u00a0has its own\nCONTAINER_LOCAL unifier. In such a situation, the in-bound network\ntuple flow is split between containers for  2a\u00a0and 2b\u00a0each of which take half the traffic. STRAM\ndoes this by default since it always has better performance.", 
            "title": "NxM Partitions"
        }, 
        {
            "location": "/application_development/#parallel", 
            "text": "In cases where all the downstream operators use the same\npartitioning scheme and the DAG is network bound an optimization called\nparallel partition\u00a0is very\neffective. In such a scenario all the downstream operators are also\npartitioned to create computation flow per partition. This optimization\nis extremely efficient for network bound streams, In some cases this\noptimization would also apply for CPU or RAM bounded\napplications.  In Figure 6a, operator 1\u00a0is\npartitioned into 1a\u00a0and\n1b. Both the downstream operators\n2\u00a0and  3\u00a0follow the same partition scheme as\n1, however the network I/O between\n1\u00a0and 2, and between 2\u00a0and  3\u00a0is\nhigh. Then users can decide to optimize using parallel partitions. This\nallows STRAM to completely skip the insertion of intermediate Unifier\noperators between 1 and 2 as well as between 2 and 3; a single unifier\njust before operator  4, is\nadequate by which time tuple flow volume is low.  Since operator 4 has sufficient resources to manage the combined\noutput of multiple instances of operator 3, it need not be partitioned. A further\noptimization can be done by declaring operators  1, 2, and\n3\u00a0as THREAD_LOCAL (intra-thread)\nor CONTAINER_LOCAL (intra-process) or NODE_LOCAL (intra-node).\nParallel partition is not used by default, users have to specify it\nexplicitly via an attribute of the input port (reader) of the stream as\nshown below.   The following code shows an example of creating a parallel partition.  dag.addStream( DenormalizedUserId , idAssigner.userid, uniqUserCount.data);\ndag.setInputPortAttribute(uniqUserCount.data, PortContext.PARTITION_PARALLEL, partitionParallel);  Parallel partitions can be used with other partitions, for example\na parallel partition could be sticky key or load balanced.", 
            "title": "Parallel"
        }, 
        {
            "location": "/application_development/#parallel-partitions-with-streams-modes", 
            "text": "Parallel partitions can be further optimized if the parallel\npartitions are combined with streams being in-line or in-node or in-rack\nmode. This is very powerful feature and should be used if operators have\nvery high throughput within them and the outbound merge does an\naggregation. For example in Figure 6b, if operator 3 significantly\nreduces the throughput, which usually is a reason to do parallel\npartition, then making the streams in-line or in-node within nodes\n1- 2 and 2- 3 significantly impacts the performance.  CONTAINER_LOCAL stream has high bandwidth, and can manage to\nconsume massive tuple count without taxing the NIC and networking stack.\nThe downside is that all operators (1,2,3) in this case need to be able\nto fit within the resource limits of CPU and memory enforced on a Hadoop\ncontainer. A way around this is to request RM to provide a big\ncontainer. On a highly used Hadoop grid, getting a bigger container may\nbe a problem, and operational complexities of managing a Hadoop cluster\nwith different container sizes may be higher. If THREAD_LOCAL or\nCONTAINER_LOCAL streams are needed to get the throughput, increasing\nthe partition count should be considered. In future STRAM may take this\ndecision automatically. Unless there is a very bad skew and sticky key\npartitioning is in use, the approach to partition till each container\nhas enough resources works well.  A NODE_LOCAL stream has lower bandwidth compared to a\nCONTAINER_LOCAL stream, but it works well with the RM in terms of\nrespecting container size limits. A NODE_LOCAL parallel partition uses\nlocal loop back for streams and is much better than using NIC. Though\nNODE_LOCAL stream fits well with similar size containers, it does need\nRM to be able to deliver two containers on the same Hadoop node. On a\nheavily used Hadoop cluster, this may not always be possible. In future\nSTRAM would do these trade-offs automatically at run-time.  A RACK_LOCAL stream has much lower bandwidth than NODE_LOCAL\nstream, as events go through the NIC. But it still is able to better\nmanage SLA and latency. Moreover RM has much better ability to give a\nrack local container as opposed to the other two.  Parallel partitions with CONTAINER_LOCAL streams can be done by\nsetting all the intermediate streams to CONTAINER_LOCAL. Parallel\npartitions with THREAD_LOCAL streams can be done by setting all the\nintermediate streams to THREAD_LOCAL. Platform supports the following\nvia attributes.   Parallel-Partition  Parallel-Partition with THREAD_LOCAL stream  Parallel-Partition with CONTAINER_LOCAL stream  Parallel-Partition with NODE_LOCAL stream  Parallel-Partition with RACK_LOCAL stream   These attributes would nevertheless be initial starting point and\nSTRAM can improve on them at run time.", 
            "title": "Parallel Partitions with Streams Modes"
        }, 
        {
            "location": "/application_development/#skew-balancing-partition", 
            "text": "Skew balancing partition is useful to manage skews in the stream\nthat is load balanced using a sticky key. Incoming events may have a\nskew, and these may change depending on various factors like time of the\nday or other special circumstances. To manage the uneven load, users can\nset a limit on the ratio of maximum load on a partition to the minimum\nload on a partition. STRAM would use this to dynamically change the\npartitions. For example suppose there are 6 partitions, and the load\nhappens to be distributed as follows: one with 40%, and the rest with\n12% each. The ratio of maximum to minimum is 3.33. If the desired ratio\nis set to 2, STRAM would partition the first instance into two\npartitions, each with 20% load to bring the ratio down to the desired\nlevel. This will be tried repeatedly till partitions are balanced. The\ntime period between each attempt is controlled via an attribute to avoid\nrebalancing too frequently. As mentioned earlier, dynamic operations\ninclude both splitting a partition as well as merging partitions with\nlow load.  Figure 7 shows an example of skew balancing partition. An example\nof 3x1 paritition is shown. Let's say that skew balance is kept at \u201cno\npartition to take up more than 50% load. If in runtime the load type\nchanges to create a skew. For example, consider an application in the US\nthat is processing a website clickstream. At night in the US, the\nmajority of accesses come from the Far East, while in the daytime it\ncomes from the Americas. Similarly, in the early morning, the majority\nof the accesses are from east coast of the US, with the skew shifting to\nthe west coast as the day progresses. Assume operator 1 is partitioned\ninto 1a, 1b, and 1c.  Let's see what happens if the logical operator 1 gets into a 20%,\n20%, 60% skew as shown in Figure 7. This would trigger the skew\nbalancing partition. One example of attaining balance is to merge 1a,\nand 1b to get 1a+1b in a single partition to take the load to 40%; then\nsplit 1c into two partitions 1ca and 1cb to get 30% on each of them.\nThis way STRAM is able to get back to under 50% per partition. As a live\n24x7 application, this kind of skew partitioning can be applied several\ntimes in a day. Skew-balancing at runtime is a critical feature for SLA\ncompliance; it also enables cost savings. This partitioning scheme will\nbe available in later release.", 
            "title": "Skew Balancing Partition"
        }, 
        {
            "location": "/application_development/#skew-unifier-partition", 
            "text": "In this section we would take a look at another way to balance the\nskew. This method is a little less disruptive, but is useful in\naggregate operators. Let us take the same example as in Figure 7 with\nskew 20%, 20%, and 60%. To manage the load we could have either worked\non rebalancing the partition, which involves a merge and split of\npartitions to get to a new distribution or by partitioning  only\u00a0the partition with the big skew. Since the\nbest way to manage skew is to load balance, if possible, this scheme\nattempts to do so. The method is less useful than the others we discusse\n-- the main reason being that if the developer has chosen a sticky key\npartition to start with, it is unlikely that a load balancing scheme can\nhelp. Assuming that it is worthwhile to load balance, a special\none-purpose unifier can be inserted for the skew partition. If the cause\nof resource bottleneck is not the I/O, specially the I/O into the\ndownstream operator, but is the compute (memory, CPU) power of a\npartition, it makes sense to split the skew partition without having to\nchange the in-bound I/O to the upstream operator.  To trigger this users can set a limit on the ratio of maximum load\non a partition to the minimum load on a partition, and ask to use this\nscheme. STRAM would use this to load balance.The time period between\neach attempt is controlled via the same attribute to avoid rebalancing\ntoo frequently.  Figure 8 shows an example of skew load balancing partition with a\ndedicated unifier. The 20%, 20%, and 60% triggers the skew load\nbalancing partition with an unifier. Partition 1c would be split into\ntwo and it would get its own dedicated unifier. Ideally these two\nadditional partitions 1ca and 1cb will get 30% load. This way STRAM is\nable to get back to under 50% per partition. This scheme is very useful\nwhen the number of partitions is very high and we still have a bad\nskew.  In the steady state no physical partition is computing more than\n30% of the load. Memory and CPU resources are thus well distributed. The\nunifier that was inserted has to handle 60% of the load, distributed\nmore evenly, as opposed to the final unifier that had a 60% skew to\nmanage at a much higher total load. This partitioning scheme will be\navailable in later release.", 
            "title": "Skew Unifier Partition"
        }, 
        {
            "location": "/application_development/#cascading-unifier", 
            "text": "Let's take the case of an upstream operator oprU\u00a0that connects to a downstream operator\noprD. Let's assume the application\nis set to scale oprU by load balancing. So this could be either Nx1 or\nNxM partitioning scheme. The upstream operator oprU scales by increasing\nN. An increase in the load triggers more resource needs (CPU, Memory, or\nI/O), which in turn triggers more containers and raises N, the\ndownstream node may be impacted in a lot of situations. In this section\nwe review a method to shield oprD from dynamic changes in the execution\nplan of oprU. On aggregate operators (Sum, Count, Max, Min, Range \u2026) it\nis better to do load balanced partitioning to avoid impact of skew. This\nworks very well as each partition emits tuples at the order of number of\nkeys (range) in the incoming stream per application window. But as N\ngrows the in-bound I/O to the unifier of oprU that runs in the container\nof oprD goes up proportionately as each upstream partition sends tuples\nof the order of unique keys (range). This means that the partitioning\nwould not scale linearly. The platform has mechanisms to manage this and\nget the scale back to being linear.  Cascading unifiers are implemented by inserting a series of\nintermediate unifiers before the final unifier in the container of oprD.\nSince each unifier guarantees that the outbound I/O would be in order of\nthe number of unique keys, the unifier in the oprD container can expect\nto achieve an upper limit on the inbound I/O. The problem is the same\nirrespective of the value of M (1 or more), wherein the amount of\ninbound I/O is proportional to N, not M. Figure 8 illustrates how\ncascading unifier works.   Figure 8 shows an example where a 4x1 partition with single\nunifier is split into three 2x1 partitions to enable the final unifier\nin oprD container to get an upper limit on inbound I/O. This is useful\nto ensure that network I/O to containers is within limits, or within a\nlimit specified by users. The platform allows setting an upper limit of\nfan-in of the stream between oprU and oprD. Let's say that this is F (in\nthe figure F=2). STRAM would plan N/F (let's call it N1) containers,\neach with one unifier. The inbound fan-in to these unifiers is F. If N1  F, another level of unifiers would be inserted. Let's say at some\npoint N/(F1*F2*...Fk)   F, where K is the level of unifiers. The\noutbound I/O of each unifier is guaranteed to be under F, specially the\nunifier for oprD. This ensures that the application scales linearly as\nthe load grows. The downside is the additional latency imposed by each\nunifier level (a few milliseconds), but the SLA is maintained, and the\napplication is able to run within the resource limits imposed by YARN.\nThe value of F can be derived from any of the following   I/O limit on containers to allow proper behavior in an\n    multi-tenant environment  Load on oprD instance  Buffer server limits on fan-in, fan-out  Size of reservoir buffer for inbound fan-in   A more intriguing optimization comes when cascading unifiers are\ncombined with node-local execution plan, in which the bounds of two or\nmore containers are used and much higher local loopback limits are\nleveraged. In general the first level fan-in limit (F1) and the last\nstage fan-in limit (Fk) need not be same. In fact a much open and better\nleveraged execution plan may indeed have F1 != F2 != \u2026 != Fk, as Fk\ndetermines the fan-in for oprD, while F1, \u2026 Fk-1 are fan-ins for\nunifier-only containers. The platform will have these schemes in later\nversions.", 
            "title": "Cascading Unifier"
        }, 
        {
            "location": "/application_development/#sla", 
            "text": "A Service Level Agreement translates to guaranteeing that the\napplication would meet the requirements X% of the time. For example six\nsigma X is\u00a099.99966%. For\nreal-time streaming applications this translates to requirements for\nlatency, throughput, uptime, data loss etc. and that in turn indirectly\nleads to various resource requirements, recovery mechanisms, etc. The\nplatform is designed to handle these and features would be released in\nfuture as they get developed. At a top level, STRAM monitors throughput\nper operator, computes latency per operator, manages uptime and supports\nvarious recovery mechanisms to handle data loss. A lot of this decision\nmaking and algorithms will be customizable.", 
            "title": "SLA"
        }, 
        {
            "location": "/application_development/#fault-tolerance", 
            "text": "Fault tolerance in the platform is defined as the ability to\nrecognize the outage of any part of the application, get resources,\nre-initialize the failed operators, and re-compute the lost data. The\ndefault method is to bring the affected part of the DAG \u00a0back to a known\n(checkpointed) state and recompute atomic micro batches from there on.\nThus the default is  at least\nonce\u00a0processing mode. An operator can be configured for\nat most once\u00a0recovery, in which\ncase the re-initialized operator starts from next available window; or\nfor exactly once\u00a0recovery, in which\ncase the operator only recomputes the window it was processing when the\noutage happened.", 
            "title": "Fault Tolerance"
        }, 
        {
            "location": "/application_development/#state-of-the-application", 
            "text": "The state of the application is traditionally defined as the state\nof all operators and streams at any given time. Monitoring state as\nevery tuple is processed asynchronously in a distributed environment\nbecomes a near impossible task, and cost paid to achieve it is very\nhigh. Consequently, in the platform, state is not saved per tuple, but\nrather at window boundaries. The platform treats windows as atomic micro\nbatches. The state saving task is delegated by STRAM to the individual\noperator or container. This ensures that the bookkeeping cost is very\nlow and works in a distributed way. Thus, the state of the application\nis defined as the collection of states of every operator and the set of\nall windows stored in the buffer server. This allows STRAM to rebuild\nany part of the application from the last saved state of the impacted\noperators and the windows retained by the buffer server. The state of an\noperator is intrinsically associated with a window id. Since operators\ncan override the default checkpointing period, operators may save state\nat the end of different windows. This works because the buffer server\nsaves all windows for as long as they are needed (state in the buffer\nserver is purged once STRAM determines that it is not longer needed\nbased on checkpointing in downstream operators).  Operators can be stateless or stateful. A stateless operator\nretains no data between windows. All results of all computations done in\na window are emitted in that window. Variables in such an operator are\neither transient or are cleared by an end_window event. Such operators\nneed no state restoration after an outage. A stateful operator retains\ndata between windows and has data in checkpointed state. This data\n(state) is used for computation in future windows. Such an operator\nneeds its state restored after an outage. By default the platform\nassumes the operator is stateful. In order to optimize recovery (skip\nprocessing related to state recovery) for a stateless operator, the\noperator needs to be declared as stateless to STRAM. Operators can\nexplicitly mark themselves stateless via an annotation or an\nattribute.  Recovery mechanisms are explained later in this section. Operator\ndevelopers have to ensure that there is no dependency on the order of\ntuples between two different streams. As mentioned earlier in this\ndocument, the platform guarantees in-order tuple delivery within a\nsingle stream, For operators with multiple input ports, a replay may\nresult in a different relative order of tuples among the different input\nports. If the output tuple computation is affected by this relative\norder, the operator may have to wait for the endWindow call (at which\npoint it would have seen all the tuples from all input ports in the\ncurrent window), perform order-dependent computations correctly and\nfinally, emit results.", 
            "title": "State of the Application"
        }, 
        {
            "location": "/application_development/#checkpointing", 
            "text": "STRAM provides checkpointing parameters to StreamingContainer\nduring initialization. A checkpoint period is given to the containers\nthat have the window generators. A control tuple is sent at the end of\ncheckpoint interval. This tuple traverses through the data path via\nstreams and triggers each StreamingContainer in the path to instrument a\ncheckpoint of the operator that receives this tuple. This ensures that\nall the operators checkpoint at exactly the same window boundary (except\nin those cases where a different checkpoint interval was configured for\nan operator by the user).  The only delay is the latency of the control tuple to reach all\nthe operators. Checkpoint is thus done between the endWindow call of a\nwindow and the beginWindow call of the next window. Since most operators\nare computing in parallel (with the exception of those connected by\nTHREAD_LOCAL streams) they each checkpoint as and when they are ready\nto process the \u201ccheckpoint\u201d control tuple. The asynchronous design of\nthe platform means that there is no guarantee that two operators would\ncheckpoint at exactly the same time, but there is a guarantee that by\ndefault they would checkpoint at the same window boundary. This feature\nalso ensures that purge of old data can be efficiently done: Once the\ncheckpoint window tuple is done traversing the DAG, the checkpoint state\nof the entire DAG increments to this window id at which point prior\ncheckpoint data can be discarded.  In case of an operator that has an application window size that is\nlarger than the size of the streaming window, the checkpointing by\ndefault still happens at same intervals as with other operators. To\nalign checkpointing with application window boundary, the application\ndeveloper should set the attribute \u201cCHECKPOINT_WINDOW_COUNT\u201d to\n\u201cAPPLICATION_WINDOW_COUNT\u201d. This ensures that the checkpoint happens\nat the  end\u00a0of the application\nwindow and not within\u00a0that window.\nSuch operators now treat the application window as an atomic computation\nunit. The downside is that it does need the upstream buffer server to\nkeep tuples for the entire application window.  If an operator is completely stateless, i.e. an outbound tuple is\nonly emitted in the process\u00a0call\nand only depends on the tuple of that call, there is no need to align\ncheckpointing with application window end. If the operator is stateful\nonly within a window, the operator developer should strongly consider\ncheckpointing only on the application window boundary.  Checkpointing involves pausing an operator, serializing the state\nto persistent storage and then resuming the operator. Thus checkpointing\nhas a latency cost that can negatively affect computational throughput;\nto minimize that impact, it is important to ensure that checkpointing is\ndone with minimal required objects. This means, as mentioned earlier,\nall data that is not part of the operator state should be declared as\ntransient so that it is not persisted.  An operator developer can also create a stateless operator (marked\nwith the Stateless annotation). Stateless operators are not\ncheckpointed. Obviously, in such an operator, computation should not\ndepend on state from a previous window.  The serialized \u00a0state of an operator is stored as a file, and is\nthe state to which that the operator is restored if an outage happens\nbefore the next checkpoint. The id of the last completed window (per\noperator) is sent back to STRAM in the next heartbeat. The default\nimplementation for serialization uses KRYO. Multiple past checkpoints\nare kept per operator. Depending on the downstream checkpoint, one of\nthese are chosen for recovery. Checkpoints and buffer server state are\npurged once STRAM sees windows as fully processed in the DAG.  A complete recovery of an operator needs the operator to be\ncreated, its checkpointed state restored and then all the lost atomic\nwindows replayed by the upstream buffer server(s). The above design\nkeeps the bookkeeping cost low with quick catch up time. In the next\nsection we will see how this simple abstraction allows applications to\nrecover under different requirements.", 
            "title": "Checkpointing"
        }, 
        {
            "location": "/application_development/#recovery-mechanisms_1", 
            "text": "Recovery mechanism are ways to recover from a container (or an\noperator) outage. In this section we discuss a single container outage.\nMultiple container outages are handled as independent events. Recovery\nrequires the upstream buffer server to replay windows and it would\nsimply go one more level upstream if the immediate upstream container\nhas also failed. If multiple operators are in a container (THREAD_LOCAL\nor CONTAINER_LOCAL stream) the container recovery treats each operator\nas an independent object when figuring out the recovery steps.\nApplication developers can set any of the recovery mechanisms discussed\nbelow for node outage.  In general, the cost of recovery depends on the state of the\noperator and the recovery mechanism selected, while data loss tolerance\nis specified by the application. For example a data-loss tolerant\napplication would prefer at most\nonce\u00a0recovery. All recovery mechanisms treat a streaming\nwindow as an atomic computation unit. In all three recovery mechanisms\nthe new operator connects to the upstream buffer server and asks for\ndata from a particular window onwards. Thus all recovery methods\ntranslate to deciding which atomic units to re-compute and which state\nthe new operator resumes from. A partially computed micro-batch is\nalways dropped. Such micro-batches are re-computed in at-least-once or\nexactly-once mode and skipped in at-most-once mode. The notiion of an\natomic micro-batch is a critical guiding principle as it enables very\nlow bookkeeping costs, high throughput, low recovery times, and high\nscalability. Within an application each operator can have its own\nrecovery mechanism.", 
            "title": "Recovery Mechanisms"
        }, 
        {
            "location": "/application_development/#at-least-once", 
            "text": "At least once recovery is the default recovery mechanism, i.e it\nis used when no mechanism is specified. In this method, the lost\noperator is brought back to its latest viable checkpointed state and the\nupstream buffer server is asked to replay all subsequent windows. There\nis no data loss in recovery. The viable checkpoint state is defined as\nthe one whose window id is in the past as compared to all the\ncheckpoints of all the downstream operators. All downstream operators\nare restarted at their checkpointed state. They ignore all incoming data\nthat belongs to windows prior their checkpointed window. The lost\nwindows are thus recomputed and the application catches up with live\nincoming data. This is called \" at least\nonce\"\u00a0because lost windows are recomputed. For example if\nthe streaming window is 0.5 seconds and checkpointing is being done\nevery 30 seconds, then upon node outage all windows since the last\ncheckpoint (up to 60 windows) need to be re-processed. If the\napplication can handle loss of data, then this is not the most optimal\nrecovery mechanism.  In general for this recovery mode, the average time lag on a node\noutage is  = (CP/2*SW)*T + HC  where   CP \u00a0\u00a0- Checkpointing period (default value is 30 seconds)  SW \u00a0\u00a0- Streaming window period (default value is 0.5 seconds)  T \u00a0\u00a0\u00a0- \u00a0Time taken to re-compute one lost window from data in memory  HC \u00a0\u00a0- Time it takes to get a new Hadoop Container, or make do with the current ones   A lower CP is a trade off between cost of checkpointing and the\nneed to have to use it in case of outage. Input adapters cannot use\nat-least-once recovery without the support from sources outside Hadoop.\nFor an output adapter care may needed if the external system cannot\nhandle re-write of the same data.", 
            "title": "At Least Once"
        }, 
        {
            "location": "/application_development/#at-most-once", 
            "text": "This recovery mechanism is for applications that can tolerate\ndata-loss; they get the quickest recovery in return. The restarted node\nconnects to the upstream buffer server, subscribing to data from the\nstart of the next window. It then starts processing that window. The\ndownstream operators ignore the lost windows and continue to process\nincoming data normally. Thus, this mechanism forces all downstream\noperators to follow.  For multiple inputs, the operator waits for all ports with the\nat-most-once attribute to get responses from their respective buffer\nservers. Then, the operator starts processing till the end window of the\nlatest window id on each input port is reached. In this case the end\nwindow tuple is non-blocking till the common window id is reached. At\nthis point the input ports are now properly synchronized. Upstream nodes\nreconnect under  at most\nonce\u00a0paradigm in same way. \u00a0For example, assume an operator\nhas ports in1\u00a0and in2\u00a0and a checkpointed window of 95. Assume further that the buffer servers of\noperators upstream of  in1\u00a0and\nin2\u00a0respond with window id 100 and\n102 respectively. Then port in1\u00a0would continue to process till end window of\n101, while port  in2\u00a0will wait for in1\nto catch up to 102.\nFrom \u00a0then on, both ports process their tuples normally. So windows from\n96 to  99are lost. Window 100\nand 101 has only\nin1 active, and 102 onwards both ports are active. The other\nports of upstream nodes would also catch up till  102in a similar fashion. This operator may not\nneed to be checkpointed. Currently the option to not do checkpoint in\nsuch cases is not available.  In general, in this recovery mode, the average time lag on a node\noutage is  = SW/2 + HC  where    SW \u00a0- Streaming window period (default value is 0.5\nseconds)    HC \u00a0- Time it takes to get a new Hadoop Container, or make\ndo with the current ones", 
            "title": "At Most Once"
        }, 
        {
            "location": "/application_development/#exactly-once", 
            "text": "This recovery mechanism is for applications that require no\ndata-loss as well are no recomputation. Since a window is an atomic\ncompute unit, exactly once applies to the window as a whole. In this\nrecovery mode, the operator is brought back to the start of the window\nin which the outage happened and the window is recomputed. The window is\nconsidered closed when all the data computations are done and end window\ntuple is emitted. \u00a0Exactly once requires every window to be\ncheckpointed. From then on, the operator asks the upstream buffer server\nto send data from the last checkpoint. The upstream node behaves the\nsame as in at-most-once recovery. Checkpointing after every streaming\nwindow is very costly, but users would most often do exactly once per\napplication window; if the application window size is substantially\nlarger than the streaming window size (which typically is the case) the\ncost of running an operator in this recovery mode may not be as\nhigh.", 
            "title": "Exactly Once"
        }, 
        {
            "location": "/application_development/#speculative-execution", 
            "text": "In future we looking at possibility of adding speculative execution for the applications. This would be enabled in multiple ways.    At an operator level: The upstream operator would emit to\n    two copies. The downstream operator would receive from both copies\n    and pick a winner. The winner (primary) would be picked in either of\n    the following ways   Statically as dictated by STRAM  Dynamically based on whose tuple arrives first. This mode\n    needs both copies to guarantee that the computation result would\n    have identical functionality     At a sub-query level: A part of the application DAG would be\n    run in parallel and all upstream operators would feed to two copies\n    and all downstream operators would receive from both copies. The\n    winners would again be picked in a static or dynamic manner   Entire DAG: Another copy of the application would be run by\n    STRAM and the winner would be decided outside the application. In\n    this mode the output adapters would both be writing\n    the result.   In all cases the two copies would run on different Hadoop nodes.\nSpeculative execution is under development and\nis not yet available.", 
            "title": "Speculative Execution"
        }, 
        {
            "location": "/application_development/#dynamic-application-modifications", 
            "text": "Dynamic application modifications are being worked on and most of\nthe features discussed here are now available. The platform supports the\nability to modify the DAG of the application as per inputs as well as\nset constraints, and will continue to provide abilities to deepen\nfeatures based on this ability. All these changes have one thing in\ncommon and that is the application does not need to be restarted as\nSTRAM will instrument the changes and the streaming will catch-up and\ncontinue.  Some examples are   Dynamic Partitioning:\u00a0Automatic\n    changes in partitioning of computations to match constraints on a\n    run time basis. Examples includes STRAM adding resource during spike\n    in streams and returning them once spike is gone. Scale up and scale\n    down is done automatically without human intervention.  Modification via constraints: Attributes can be changed via\n    Webservices and STRAM would adapt the execution plan to meet these.\n    Examples include operations folks asking STRAM to reduce container\n    count, or changing network resource restrictions.  Modification via properties: Properties of operators can be\n    changed in run time. This enables application developers to trigger\n    a new behavior as need be. Examples include triggering an alert ON.\n    The platform supports changes to any property of an operator that\n    has a setter function defined.  Modification of DAG structure: Operators and streams can be\n    added to or removed from a running DAG, provided the code of the\n    operator being added is already in the classpath of the running\n    application master. \u00a0This enables application developers to add or\n    remove processing pipelines on the fly without having to restart\n    the application.  Query Insertion: Addition of sub-queries to currently\n    running application. This query would take current streams as inputs\n    and start computations as per their specs. Examples insertion of\n    SQL-queries on live data streams, dynamic query submission and\n    result from STRAM (not yet available).   Dynamic modifications to applications are foundational part of the\nplatform. They enable users to build layers over the applications. Users\ncan also save all the changes done since the application launch, and\ntherefore predictably get the application to its current state. For\ndetails refer to   Configuration Guide \n.", 
            "title": "Dynamic Application Modifications"
        }, 
        {
            "location": "/application_development/#demos", 
            "text": "The source code for the demos is available in the open-source Apache Apex-Malhar repository .\nAll of these do computations in real-time. Developers are encouraged to\nreview them as they use various features of the platform and provide an\nopportunity for quick learning.", 
            "title": "Demos"
        }, 
        {
            "location": "/application_packages/", 
            "text": "Apache Apex Packages\n\n\nApplication Packages\n\n\nAn Apache Apex Application Package is a zip file that contains all the\nnecessary files to launch an application in Apache Apex. It is the\nstandard way for assembling and sharing an Apache Apex application.\n\n\nRequirements\n\n\nYou will need have the following installed:\n\n\n\n\nApache Maven 3.0 or later (for assembling the App Package)\n\n\nApache Apex 3.6.0 or later (for launching the App Package in your cluster)\n\n\n\n\nCreating Your First Apex App Package\n\n\nYou can create an Apex Application Package using your Linux command\nline, or using your favorite IDE.\n\n\nUsing Command Line\n\n\nFirst, change to the directory where you put your projects, and create\nan Apex application project using Maven by running the following\ncommand.  Replace \"com.example\", \"myapp\" and \"1.0-SNAPSHOT\" with the\nappropriate values (make sure this is all on one line). You can also\nreplace \"RELEASE\" with a specific Apex version number (like \"3.6.0\")\nif you don't want to use the most recent release:\n\n\n$ mvn archetype:generate \\\n -DarchetypeGroupId=org.apache.apex \\\n -DarchetypeArtifactId=apex-app-archetype -DarchetypeVersion=RELEASE \\\n -DgroupId=com.example -Dpackage=com.example.myapp -DartifactId=myapp \\\n -Dversion=1.0-SNAPSHOT\n\n\n\nThis creates a Maven project named \"myapp\". Open it with your favorite\nIDE (e.g. NetBeans, Eclipse, IntelliJ IDEA). In the project, there is a\nsample DAG that generates a number of tuples with a random number and\nprints out \"hello world\" and the random number in the tuples.  The code\nthat builds the DAG is in\nsrc/main/java/com/example/myapp/Application.java, and the code that\nruns the unit test for the DAG is in\nsrc/test/java/com/example/myapp/ApplicationTest.java. Try it out by\nrunning the following command:\n\n\n$cd myapp; mvn package\n\n\n\nThis builds the App Package runs the unit test of the DAG.  You should\nbe getting test output similar to this:\n\n\n -------------------------------------------------------\n  TESTS\n -------------------------------------------------------\n\n Running com.example.myapp.ApplicationTest\n hello world: 0.8015370953286478\n hello world: 0.9785359225545481\n hello world: 0.6322611586644047\n hello world: 0.8460953663451775\n hello world: 0.5719372906929072\n hello world: 0.6361174312337172\n hello world: 0.14873007534816318\n hello world: 0.8866986277418261\n hello world: 0.6346526809866057\n hello world: 0.48587295703904465\n hello world: 0.6436832429676687\n\n ...\n\n Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.863\n sec\n\n Results :\n\n Tests run: 1, Failures: 0, Errors: 0, Skipped: 0\n\n\n\n\nThe \"mvn package\" command creates the App Package file in target\ndirectory as target/myapp-1.0-SNAPSHOT.apa. You will be able to use\nthat App Package file to launch this sample application in your actual\nApex installation.\n\n\nAlternatively you can perform the same steps within your IDE (IDEA IntelliJ, Eclipse, NetBeans all support it). Please check the IDE documentation for details.\n\n\nGroup ID: org.apache.apex\nArtifact ID: apex-app-archetype\nVersion: 3.6.0 (or any later version)\n\n\nWriting Your Own App Package\n\n\nPlease refer to the [Application Developer Guide][application_development.md] on the basics on how to write an Apache Apex application. In your AppPackage project, you can add custom operators (refer to \nOperator Development Guide\n, project dependencies, default and required configuration properties, pre-set configurations and other metadata. Note that you can also specify the DAG using Java, JSON or properties files. \n\n\nAdding (and removing) project dependencies\n\n\nUnder the project, you can add project dependencies in pom.xml, or do it\nthrough your IDE.  Here\u2019s the section that describes the dependencies in\nthe default pom.xml:\n\n\n  \ndependencies\n\n    \n!-- add your dependencies here --\n\n    \ndependency\n\n      \ngroupId\norg.apache.apex\n/groupId\n\n      \nartifactId\nmalhar-library\n/artifactId\n\n      \nversion\n${apex.version}\n/version\n\n      \n!--\n           If you know your application do not need the transitive dependencies that are pulled in by malhar-library,\n           Uncomment the following to reduce the size of your app package.\n      --\n\n      \n!--\n      \nexclusions\n\n        \nexclusion\n\n          \ngroupId\n*\n/groupId\n\n          \nartifactId\n*\n/artifactId\n\n        \n/exclusion\n\n      \n/exclusions\n\n      --\n\n    \n/dependency\n\n    \ndependency\n\n      \ngroupId\norg.apache.apex\n/groupId\n\n      \nartifactId\napex-engine\n/artifactId\n\n      \nversion\n${apex.version}\n/version\n\n      \nscope\nprovided\n/scope\n\n    \n/dependency\n\n    \ndependency\n\n      \ngroupId\njunit\n/groupId\n\n      \nartifactId\njunit\n/artifactId\n\n      \nversion\n4.10\n/version\n\n      \nscope\ntest\n/scope\n\n    \n/dependency\n\n  \n/dependencies\n\n\n\n\n\nBy default, as shown above, the default dependencies include\nmalhar-library in compile scope, apex-engine in provided scope, and junit\nin test scope.  Do not remove these three dependencies since they are\nnecessary for any Apex application.  You can, however, exclude\ntransitive dependencies from malhar-library to reduce the size of your\nApp Package, provided that none of the operators in malhar-library that\nneed the transitive dependencies will be used in your application.\n\n\nIn the sample application, it is safe to remove the transitive\ndependencies from malhar-library, by uncommenting the \"exclusions\"\nsection.  It will reduce the size of the sample App Package from 8MB to\n700KB.\n\n\nNote that if we exclude *, in some versions of Maven, you may get\nwarnings similar to the following:\n\n\n\n [WARNING] 'dependencies.dependency.exclusions.exclusion.groupId' for\n org.apache.apex:malhar-library:jar with value '*' does not match a\n valid id pattern.\n\n [WARNING]\n [WARNING] It is highly recommended to fix these problems because they\n threaten the stability of your build.\n [WARNING]\n [WARNING] For this reason, future Maven versions might no longer support\n building such malformed projects.\n [WARNING]\n\n\n\n\n\nThis is a bug in early versions of Maven 3.  The dependency exclusion is\nstill valid and it is safe to ignore these warnings.\n\n\nApplication Configuration\n\n\nA configuration file can be used to configure an application.  Different\nkinds of configuration parameters can be specified. They are application\nattributes, operator attributes and properties, port attributes, stream\nproperties and application specific properties. They are all specified\nas name value pairs, in XML format, like the following.\n\n\n?xml version=\n1.0\n?\n\n\nconfiguration\n\n  \nproperty\n\n    \nname\nsome_name_1\n/name\n\n    \nvalue\nsome_default_value\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\nsome_name_2\n/name\n\n    \nvalue\nsome_default_value\n/value\n\n  \n/property\n\n\n/configuration\n\n\n\n\n\nApplication attributes\n\n\nApplication attributes are used to specify the platform behavior for the\napplication. They can be specified using the parameter\n\napex.attr.\nattribute\n. The prefix \u201capex\u201d is a constant, \u201cattr\u201d is a\nconstant denoting an attribute is being specified and \nattribute\n\nspecifies the name of the attribute. Below is an example snippet setting\nthe streaming windows size of the application to be 1000 milliseconds.\n\n\n  \nproperty\n\n     \nname\napex.attr.STREAMING_WINDOW_SIZE_MILLIS\n/name\n\n     \nvalue\n1000\n/value\n\n  \n/property\n\n\n\n\n\nThe name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\ncom.datatorrent.api.Context.DAGContext and the different attributes can\nbe specified in the format described above.\n\n\nOperator attributes\n\n\nOperator attributes are used to specify the platform behavior for the\noperator. They can be specified using the parameter\n\napex.operator.\noperator-name\n.attr.\nattribute\n. The prefix \u201capex\u201d is a\nconstant, \u201coperator\u201d is a constant denoting that an operator is being\nspecified, \noperator-name\n denotes the name of the operator, \u201cattr\u201d is\nthe constant denoting that an attribute is being specified and\n\nattribute\n is the name of the attribute. The operator name is the\nsame name that is specified when the operator is added to the DAG using\nthe addOperator method. An example illustrating the specification is\nshown below. It specifies the number of streaming windows for one\napplication window of an operator named \u201cinput\u201d to be 10\n\n\nproperty\n\n  \nname\napex.operator.input.attr.APPLICATION_WINDOW_COUNT\n/name\n\n  \nvalue\n10\n/value\n\n\n/property\n\n\n\n\n\nThe name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\ncom.datatorrent.api.Context.OperatorContext and the different attributes\ncan be specified in the format described above.\n\n\nOperator properties\n\n\nOperators can be configured using operator specific properties. The\nproperties can be specified using the parameter\n\napex.operator.\noperator-name\n.prop.\nproperty-name\n. The difference\nbetween this and the operator attribute specification described above is\nthat the keyword \u201cprop\u201d is used to denote that it is a property and\n\nproperty-name\n specifies the property name.  An example illustrating\nthis is specified below. It specifies the property \u201chostname\u201d of the\nredis server for a \u201credis\u201d output operator.\n\n\n  \nproperty\n\n    \nname\napex.operator.redis.prop.host\n/name\n\n    \nvalue\n127.0.0.1\n/value\n\n  \n/property\n\n\n\n\n\nThe name tag specifies the property and the value specifies the property\nvalue. The property name is converted to a setter method which is called\non the actual operator. The method name is composed by appending the\nword \u201cset\u201d and the property name with the first character of the name\ncapitalized. In the above example the setter method would become\nsetHost. The method is called using JAVA reflection and the property\nvalue is passed as an argument. In the above example the method setHost\nwill be called on the \u201credis\u201d operator with \u201c127.0.0.1\u201d as the argument.\n\n\nProperties that are collection types can also be configured, based on the beanutils\nsyntax. For example, the connection properties of the JDBC store can be accessed\nlike this:\n\n\n  \nproperty\n\n    \nname\napex.operator.jdbc.prop.store.connectionProperties(user)\n/name\n\n    \nvalue\nyour-user-name\n/value\n\n  \n/property\n\n\n\n\n\nPort attributes\n\n\nPort attributes are used to specify the platform behavior for input and\noutput ports. They can be specified using the parameter \napex.operator.\noperator-name\n.inputport.\nport-name\n.attr.\nattribute\n\nfor input port and \napex.operator.\noperator-name\n.outputport.\nport-name\n.attr.\nattribute\n\nfor output port. The keyword \u201cinputport\u201d is used to denote an input port\nand \u201coutputport\u201d to denote an output port. The rest of the specification\nfollows the conventions described in other specifications above. An\nexample illustrating this is specified below. It specifies the queue\ncapacity for an input port named \u201cinput\u201d of an operator named \u201crange\u201d to\nbe 4k.\n\n\nproperty\n\n  \nname\napex.operator.range.inputport.input.attr.QUEUE_CAPACITY\n/name\n\n  \nvalue\n4000\n/value\n\n\n/property\n\n\n\n\n\nThe name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\ncom.datatorrent.api.Context.PortContext and the different attributes can\nbe specified in the format described above.\n\n\nThe attributes for an output port can also be specified in a similar way\nas described above with a change that keyword \u201coutputport\u201d is used\ninstead of \u201cintputport\u201d. A generic keyword \u201cport\u201d can be used to specify\neither an input or an output port. It is useful in the wildcard\nspecification described below.\n\n\nStream properties\n\n\nStreams can be configured using stream properties. The properties can be\nspecified using the parameter\n\napex.stream.\nstream-name\n.prop.\nproperty-name\n  The constant \u201cstream\u201d\nspecifies that it is a stream, \nstream-name\n specifies the name of the\nstream and \nproperty-name\n the name of the property. The name of the\nstream is the same name that is passed when the stream is added to the\nDAG using the addStream method. An example illustrating the\nspecification is shown below. It sets the locality of the stream named\n\u201cstream1\u201d to container local indicating that the operators the stream is\nconnecting be run in the same container.\n\n\n  \nproperty\n\n    \nname\napex.stream.stream1.prop.locality\n/name\n\n    \nvalue\nCONTAINER_LOCAL\n/value\n\n  \n/property\n\n\n\n\n\nThe property name is converted into a set method on the stream in the\nsame way as described in operator properties section above. In this case\nthe method would be setLocality and it will be called in the stream\n\u201cstream1\u201d with the value as the argument.\n\n\nAlong with the above system defined parameters, the applications can\ndefine their own specific parameters they can be specified in the\nconfiguration file. The only condition is that the names of these\nparameters don\u2019t conflict with the system defined parameters or similar\napplication parameters defined by other applications. To this end, it is\nrecommended that the application parameters have the format\n\nfull-application-class-name\n.\nparam-name\n.\n The\nfull-application-class-name is the full JAVA class name of the\napplication including the package path and param-name is the name of the\nparameter within the application. The application will still have to\nstill read the parameter in using the configuration API of the\nconfiguration object that is passed in populateDAG.\n\n\nWildcards\n\n\nWildcards and regular expressions can be used in place of names to\nspecify a group for applications, operators, ports or streams. For\nexample, to specify an attribute for all ports of an operator it can be\ndone as follows\n\n\nproperty\n\n  \nname\napex.operator.range.port.*.attr.QUEUE_CAPACITY\n/name\n\n  \nvalue\n4000\n/value\n\n\n/property\n\n\n\n\n\nThe wildcard \u201c*\u201d was used instead of the name of the port. Wildcard can\nalso be used for operator name, stream name or application name. Regular\nexpressions can also be used for names to specify attributes or\nproperties for a specific set.\n\n\nAdding configuration properties\n\n\nIt is common for applications to require configuration parameters to\nrun.  For example, the address and port of the database, the location of\na file for ingestion, etc.  You can specify them in\nsrc/main/resources/META-INF/properties.xml under the App Package\nproject. The properties.xml may look like:\n\n\n?xml version=\n1.0\n?\n\n\nconfiguration\n\n  \nproperty\n\n    \nname\nsome_name_1\n/name\n\n  \n/property\n\n  \nproperty\n\n    \nname\nsome_name_2\n/name\n\n    \nvalue\nsome_default_value\n/value\n\n  \n/property\n\n\n/configuration\n\n\n\n\n\nThe name of an application-specific property takes the form of:\n\n\napex.operator.{opName}.prop.{propName}\n\n\nThe first represents the property with name propName of operator opName.\n Or you can set the application name at run time by setting this\nproperty:\n\n\n    apex.attr.APPLICATION_NAME\n\n\n\nIn this example, property some_name_1 is a required property which\nmust be set at launch time, or it must be set by a pre-set configuration\n(see next section).  Property some_name_2 is a property that is\nassigned with value some_default_value unless it is overridden at\nlaunch time.\n\n\nAdding pre-set configurations\n\n\nAt build time, you can add pre-set configurations to the App Package by\nadding configuration XML files under \nsrc/site/conf/\nconf\n.xml\nin your\nproject.  You can then specify which configuration to use at launch\ntime.  The configuration XML is of the same format of the properties.xml\nfile.\n\n\nApplication-specific properties file\n\n\nYou can also specify properties.xml per application in the application\npackage.  Just create a file with the name properties-{appName}.xml and\nit will be picked up when you launch the application with the specified\nname within the application package.  In short:\n\n\nproperties.xml: Properties that are global to the Configuration\nPackage\n\n\nproperties-{appName}.xml: Properties that are specific when launching\nan application with the specified appName.\n\n\nProperties source precedence\n\n\nIf properties with the same key appear in multiple sources (e.g. from\napp package default configuration as META-INF/properties.xml, from app\npackage configuration in the conf directory, from launch time defines,\netc), the precedence of sources, from highest to lowest, is as follows:\n\n\n\n\nLaunch time defines (using -D option in CLI)\n\n\nLaunch time specified configuration file in file system (using -conf\n    option in CLI)\n\n\nLaunch time specified package configuration (using -apconf option in\n    CLI)\n\n\nConfiguration from \\$HOME/.dt/dt-site.xml\n\n\nApplication defaults within the package as\n    META-INF/properties-{appname}.xml\n\n\nPackage defaults as META-INF/properties.xml\n\n\ndt-site.xml in local DT installation\n\n\ndt-site.xml stored in HDFS\n\n\n\n\nOther meta-data\n\n\nIn a Apex App Package project, the pom.xml file contains a\nsection that looks like:\n\n\nproperties\n\n  \napex.core.version\n3.6.0\n/apex.core.version\n\n  \napex.apppackage.classpath\\\nlib*.jar\n/apex.apppackage.classpath\n\n\n/properties\n\n\n\n\n\napex.version is the Apache Apex version that are to be used\nwith this Application Package.\n\n\napex.apppackage.classpath is the classpath that is used when\nlaunching the application in the Application Package.  The default is\nlib/*.jar, where lib is where all the dependency jars are kept within\nthe Application Package.  One reason to change this field is when your\nApplication Package needs the classpath in a specific order.\n\n\nLogging configuration\n\n\nJust like other Java projects, you can change the logging configuration\nby having your log4j.properties under src/main/resources.  For example,\nif you have the following in src/main/resources/log4j.properties:\n\n\n log4j.rootLogger=WARN,CONSOLE\n log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\n log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\n log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} [%t] %-5p\n %c{2} %M - %m%n\n\n\n\n\nThe root logger\u2019s level is set to WARN and the output is set to the console (stdout).\n\n\nNote that by default from project created from the maven archetype,\nthere is already a log4j.properties file under src/test/resources and\nthat file is only used for the unit test.\n\n\nZip Structure of Application Package\n\n\nApache Apex Application Package files are zip files.  You can examine the content of any Application Package by using unzip -t on your Linux command line.\n\n\nThere are four top level directories in an Application Package:\n\n\n\n\n\"app\" contains the jar files of the DAG code and any custom operators, and any JSON or properties files that specify a DAG.\n\n\n\"lib\" contains all dependency jars\n\n\n\"conf\" contains all the pre-set configuration XML files.\n\n\n\"META-INF\" contains the MANIFEST.MF file and the properties.xml file.\n\n\n\u201cresources\u201d contains any other files\n\n\n\n\nExamining and Launching Application Packages Through CLI\n\n\nIf you are working with Application Packages in the local filesystem, you can use the Apex Command Line Interface (apex).\n\n\nGetting Application Package Meta Information\n\n\nYou can get the meta information about the Application Package using\nthis Apex CLI command.\n\n\n apex\n get-app-package-info \napp-package-file\n\n\n\n\n\nGetting Available Operators In Application Package\n\n\nYou can get the list of available operators in the Application Package\nusing this command.\n\n\n apex\n get-app-package-operators \napp-package-file\n \npackage-prefix\n\n [parent-class]\n\n\n\n\nGetting Properties of Operators in Application Package\n\n\nYou can get the list of properties of any operator in the Application\nPackage using this command.\n\n\napex\n get-app-package-operator-properties \n \n\n\nLaunching an Application Package\n\n\nYou can launch an application within an Application Package.\n\n\napex\n launch [-D property-name=property-value, ...] [-conf config-name]\n [-apconf config-file-within-app-package] \napp-package-file\n\n [matching-app-name]\n\n\n\n\nNote that -conf expects a configuration file in the file system, while -apconf expects a configuration file within the app package.\n\n\nConfiguration Packages\n\n\nSometimes just a configuration file is not enough for launching an application package. If a configuration requires\nadditional files to be packaged, you can use an Apex Configuration Package.\n\n\nCreating Configuration Packages\n\n\nCreating Configuration Packages is similar to creating Application Packages. You can create a configuration \npackage project using Maven by running the following command. Replace \"com.example\", \"myconfig\" and \"1.0-SNAPSHOT\" with the appropriate values:\n\n\n$ mvn archetype:generate -DarchetypeGroupId=org.apache.apex \\\n  -DarchetypeArtifactId=apex-conf-archetype -DarchetypeVersion=RELEASE \\\n  -DgroupId=com.example -Dpackage=com.example.myconfig -DartifactId=myconfig \\\n  -Dversion=1.0-SNAPSHOT\n\n\n\n\nAnd create the configuration package file by running:\n\n\n$ mvn package\n\n\n\n\nThe \"mvn package\" command creates the Config Package file in target\ndirectory as target/myconfig.apc. You will be able to use that\nConfiguration Package file to launch an Apache Apex application.\n\n\nAssembling your own configuration package\n\n\nInside the project created by the archetype, these are the files that\nyou should know about when assembling your own configuration package:\n\n\n./pom.xml\n./src/main/resources/classpath\n./src/main/resources/files\n./src/main/resources/META-INF/properties.xml\n./src/main/resources/META-INF/properties-{appname}.xml\n\n\n\npom.xml\n\n\nExample:\n\n\n  \ngroupId\ncom.example\n/groupId\n\n  \nversion\n1.0.0\n/version\n\n  \nartifactId\nmyconfig\n/artifactId\n\n  \npackaging\njar\n/packaging\n\n  \n!-- change these to the appropriate values --\n\n  \nname\nMy Apex Application Configuration\n/name\n\n  \ndescription\nMy Custom Application Configuration Description\n/description\n\n  \nproperties\n\n    \napex.apppackage.name\nmyapexapp\n/apex.apppackage.name\n\n    \napex.apppackage.minversion\n1.0.0\n/apex.apppackage.minversion\n\n    \napex.apppackage.maxversion\n1.9999.9999\n/apex.apppackage.maxversion\n\n    \napex.appconf.classpath\nclasspath/*\n/apex.appconf.classpath\n\n    \napex.appconf.files\nfiles/*\n/apex.appconf.files\n\n  \n/properties\n\n\n\n\n\n\nIn pom.xml, you can change the following keys to your desired values\n\n\n\n\ngroupId\n\n\nversion\n\n\nartifactId\n\n\nname\n\n\ndescription\n\n\n\n\nYou can also change the values of\n\n\n\n\napex.apppackage.name\n\n\napex.apppackage.minversion\n\n\napex.apppackage.maxversion\n\n\n\n\nto reflect what Application Packages can be used with this configuration package.  Apex will use this information to check whether a\nconfiguration package is compatible with the Application Package when you issue a launch command.\n\n\n./src/main/resources/classpath\n\n\nPlace any file in this directory that you\u2019d like to be copied to the\ncompute machines when launching an application and included in the\nclasspath of the application.  Example of such files are Java properties\nfiles and jar files.\n\n\n./src/main/resources/files\n\n\nPlace any file in this directory that you\u2019d like to be copied to the\ncompute machines when launching an application but not included in the\nclasspath of the application.\n\n\nProperties XML file\n\n\nA properties xml file consists of a set of key-value pairs.  The set of\nkey-value pairs specifies the configuration options the application\nshould be launched with.\n\n\nExample:\n\n\nconfiguration\n\n  \nproperty\n\n    \nname\nsome-property-name\n/name\n\n    \nvalue\nsome-property-value\n/value\n\n  \n/property\n\n   ...\n\n/configuration\n\n\n\n\n\nNames of properties XML file:\n\n\n\n\nproperties.xml:\n Properties that are global to the Configuration\nPackage\n\n\nproperties-{appName}.xml:\n Properties that are specific when launching\nan application with the specified appName within the Application\nPackage.\n\n\n\n\nAfter you are done with the above, remember to do mvn package to\ngenerate a new configuration package, which will be located in the\ntarget directory in your project.\n\n\nZip structure of configuration package\n\n\nApex Application Configuration Package files are zip files.  You\ncan examine the content of any Application Configuration Package by\nusing unzip -t on your Linux command line.  The structure of the zip\nfile is as follow:\n\n\nMETA-INF\n  MANIFEST.MF\n  properties.xml\n  properties-{appname}.xml\nclasspath\n  {classpath files}\nfiles\n  {files}\n\n\n\n\nLaunching with CLI\n\n\n-conf\n option of the launch command in CLI supports specifying configuration package in the local filesystem.  Example:\n\n\napex\\\n launch myapp-1.0.0.apa -conf myconfig.apc\n\n\n\nThis command expects both the application package and the configuration package to be in the local file system.", 
            "title": "Packages"
        }, 
        {
            "location": "/application_packages/#apache-apex-packages", 
            "text": "", 
            "title": "Apache Apex Packages"
        }, 
        {
            "location": "/application_packages/#application-packages", 
            "text": "An Apache Apex Application Package is a zip file that contains all the\nnecessary files to launch an application in Apache Apex. It is the\nstandard way for assembling and sharing an Apache Apex application.", 
            "title": "Application Packages"
        }, 
        {
            "location": "/application_packages/#requirements", 
            "text": "You will need have the following installed:   Apache Maven 3.0 or later (for assembling the App Package)  Apache Apex 3.6.0 or later (for launching the App Package in your cluster)", 
            "title": "Requirements"
        }, 
        {
            "location": "/application_packages/#creating-your-first-apex-app-package", 
            "text": "You can create an Apex Application Package using your Linux command\nline, or using your favorite IDE.", 
            "title": "Creating Your First Apex App Package"
        }, 
        {
            "location": "/application_packages/#using-command-line", 
            "text": "First, change to the directory where you put your projects, and create\nan Apex application project using Maven by running the following\ncommand.  Replace \"com.example\", \"myapp\" and \"1.0-SNAPSHOT\" with the\nappropriate values (make sure this is all on one line). You can also\nreplace \"RELEASE\" with a specific Apex version number (like \"3.6.0\")\nif you don't want to use the most recent release:  $ mvn archetype:generate \\\n -DarchetypeGroupId=org.apache.apex \\\n -DarchetypeArtifactId=apex-app-archetype -DarchetypeVersion=RELEASE \\\n -DgroupId=com.example -Dpackage=com.example.myapp -DartifactId=myapp \\\n -Dversion=1.0-SNAPSHOT  This creates a Maven project named \"myapp\". Open it with your favorite\nIDE (e.g. NetBeans, Eclipse, IntelliJ IDEA). In the project, there is a\nsample DAG that generates a number of tuples with a random number and\nprints out \"hello world\" and the random number in the tuples.  The code\nthat builds the DAG is in\nsrc/main/java/com/example/myapp/Application.java, and the code that\nruns the unit test for the DAG is in\nsrc/test/java/com/example/myapp/ApplicationTest.java. Try it out by\nrunning the following command:  $cd myapp; mvn package  This builds the App Package runs the unit test of the DAG.  You should\nbe getting test output similar to this:   -------------------------------------------------------\n  TESTS\n -------------------------------------------------------\n\n Running com.example.myapp.ApplicationTest\n hello world: 0.8015370953286478\n hello world: 0.9785359225545481\n hello world: 0.6322611586644047\n hello world: 0.8460953663451775\n hello world: 0.5719372906929072\n hello world: 0.6361174312337172\n hello world: 0.14873007534816318\n hello world: 0.8866986277418261\n hello world: 0.6346526809866057\n hello world: 0.48587295703904465\n hello world: 0.6436832429676687\n\n ...\n\n Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.863\n sec\n\n Results :\n\n Tests run: 1, Failures: 0, Errors: 0, Skipped: 0  The \"mvn package\" command creates the App Package file in target\ndirectory as target/myapp-1.0-SNAPSHOT.apa. You will be able to use\nthat App Package file to launch this sample application in your actual\nApex installation.  Alternatively you can perform the same steps within your IDE (IDEA IntelliJ, Eclipse, NetBeans all support it). Please check the IDE documentation for details.  Group ID: org.apache.apex\nArtifact ID: apex-app-archetype\nVersion: 3.6.0 (or any later version)", 
            "title": "Using Command Line"
        }, 
        {
            "location": "/application_packages/#writing-your-own-app-package", 
            "text": "Please refer to the [Application Developer Guide][application_development.md] on the basics on how to write an Apache Apex application. In your AppPackage project, you can add custom operators (refer to  Operator Development Guide , project dependencies, default and required configuration properties, pre-set configurations and other metadata. Note that you can also specify the DAG using Java, JSON or properties files.", 
            "title": "Writing Your Own App Package"
        }, 
        {
            "location": "/application_packages/#adding-and-removing-project-dependencies", 
            "text": "Under the project, you can add project dependencies in pom.xml, or do it\nthrough your IDE.  Here\u2019s the section that describes the dependencies in\nthe default pom.xml:     dependencies \n     !-- add your dependencies here -- \n     dependency \n       groupId org.apache.apex /groupId \n       artifactId malhar-library /artifactId \n       version ${apex.version} /version \n       !--\n           If you know your application do not need the transitive dependencies that are pulled in by malhar-library,\n           Uncomment the following to reduce the size of your app package.\n      -- \n       !--\n       exclusions \n         exclusion \n           groupId * /groupId \n           artifactId * /artifactId \n         /exclusion \n       /exclusions \n      -- \n     /dependency \n     dependency \n       groupId org.apache.apex /groupId \n       artifactId apex-engine /artifactId \n       version ${apex.version} /version \n       scope provided /scope \n     /dependency \n     dependency \n       groupId junit /groupId \n       artifactId junit /artifactId \n       version 4.10 /version \n       scope test /scope \n     /dependency \n   /dependencies   By default, as shown above, the default dependencies include\nmalhar-library in compile scope, apex-engine in provided scope, and junit\nin test scope.  Do not remove these three dependencies since they are\nnecessary for any Apex application.  You can, however, exclude\ntransitive dependencies from malhar-library to reduce the size of your\nApp Package, provided that none of the operators in malhar-library that\nneed the transitive dependencies will be used in your application.  In the sample application, it is safe to remove the transitive\ndependencies from malhar-library, by uncommenting the \"exclusions\"\nsection.  It will reduce the size of the sample App Package from 8MB to\n700KB.  Note that if we exclude *, in some versions of Maven, you may get\nwarnings similar to the following:  \n [WARNING] 'dependencies.dependency.exclusions.exclusion.groupId' for\n org.apache.apex:malhar-library:jar with value '*' does not match a\n valid id pattern.\n\n [WARNING]\n [WARNING] It is highly recommended to fix these problems because they\n threaten the stability of your build.\n [WARNING]\n [WARNING] For this reason, future Maven versions might no longer support\n building such malformed projects.\n [WARNING]  This is a bug in early versions of Maven 3.  The dependency exclusion is\nstill valid and it is safe to ignore these warnings.", 
            "title": "Adding (and removing) project dependencies"
        }, 
        {
            "location": "/application_packages/#application-configuration", 
            "text": "A configuration file can be used to configure an application.  Different\nkinds of configuration parameters can be specified. They are application\nattributes, operator attributes and properties, port attributes, stream\nproperties and application specific properties. They are all specified\nas name value pairs, in XML format, like the following.  ?xml version= 1.0 ?  configuration \n   property \n     name some_name_1 /name \n     value some_default_value /value \n   /property \n   property \n     name some_name_2 /name \n     value some_default_value /value \n   /property  /configuration", 
            "title": "Application Configuration"
        }, 
        {
            "location": "/application_packages/#application-attributes", 
            "text": "Application attributes are used to specify the platform behavior for the\napplication. They can be specified using the parameter apex.attr. attribute . The prefix \u201capex\u201d is a constant, \u201cattr\u201d is a\nconstant denoting an attribute is being specified and  attribute \nspecifies the name of the attribute. Below is an example snippet setting\nthe streaming windows size of the application to be 1000 milliseconds.     property \n      name apex.attr.STREAMING_WINDOW_SIZE_MILLIS /name \n      value 1000 /value \n   /property   The name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\ncom.datatorrent.api.Context.DAGContext and the different attributes can\nbe specified in the format described above.", 
            "title": "Application attributes"
        }, 
        {
            "location": "/application_packages/#operator-attributes", 
            "text": "Operator attributes are used to specify the platform behavior for the\noperator. They can be specified using the parameter apex.operator. operator-name .attr. attribute . The prefix \u201capex\u201d is a\nconstant, \u201coperator\u201d is a constant denoting that an operator is being\nspecified,  operator-name  denotes the name of the operator, \u201cattr\u201d is\nthe constant denoting that an attribute is being specified and attribute  is the name of the attribute. The operator name is the\nsame name that is specified when the operator is added to the DAG using\nthe addOperator method. An example illustrating the specification is\nshown below. It specifies the number of streaming windows for one\napplication window of an operator named \u201cinput\u201d to be 10  property \n   name apex.operator.input.attr.APPLICATION_WINDOW_COUNT /name \n   value 10 /value  /property   The name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\ncom.datatorrent.api.Context.OperatorContext and the different attributes\ncan be specified in the format described above.", 
            "title": "Operator attributes"
        }, 
        {
            "location": "/application_packages/#operator-properties", 
            "text": "Operators can be configured using operator specific properties. The\nproperties can be specified using the parameter apex.operator. operator-name .prop. property-name . The difference\nbetween this and the operator attribute specification described above is\nthat the keyword \u201cprop\u201d is used to denote that it is a property and property-name  specifies the property name.  An example illustrating\nthis is specified below. It specifies the property \u201chostname\u201d of the\nredis server for a \u201credis\u201d output operator.     property \n     name apex.operator.redis.prop.host /name \n     value 127.0.0.1 /value \n   /property   The name tag specifies the property and the value specifies the property\nvalue. The property name is converted to a setter method which is called\non the actual operator. The method name is composed by appending the\nword \u201cset\u201d and the property name with the first character of the name\ncapitalized. In the above example the setter method would become\nsetHost. The method is called using JAVA reflection and the property\nvalue is passed as an argument. In the above example the method setHost\nwill be called on the \u201credis\u201d operator with \u201c127.0.0.1\u201d as the argument.  Properties that are collection types can also be configured, based on the beanutils\nsyntax. For example, the connection properties of the JDBC store can be accessed\nlike this:     property \n     name apex.operator.jdbc.prop.store.connectionProperties(user) /name \n     value your-user-name /value \n   /property", 
            "title": "Operator properties"
        }, 
        {
            "location": "/application_packages/#port-attributes", 
            "text": "Port attributes are used to specify the platform behavior for input and\noutput ports. They can be specified using the parameter  apex.operator. operator-name .inputport. port-name .attr. attribute \nfor input port and  apex.operator. operator-name .outputport. port-name .attr. attribute \nfor output port. The keyword \u201cinputport\u201d is used to denote an input port\nand \u201coutputport\u201d to denote an output port. The rest of the specification\nfollows the conventions described in other specifications above. An\nexample illustrating this is specified below. It specifies the queue\ncapacity for an input port named \u201cinput\u201d of an operator named \u201crange\u201d to\nbe 4k.  property \n   name apex.operator.range.inputport.input.attr.QUEUE_CAPACITY /name \n   value 4000 /value  /property   The name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\ncom.datatorrent.api.Context.PortContext and the different attributes can\nbe specified in the format described above.  The attributes for an output port can also be specified in a similar way\nas described above with a change that keyword \u201coutputport\u201d is used\ninstead of \u201cintputport\u201d. A generic keyword \u201cport\u201d can be used to specify\neither an input or an output port. It is useful in the wildcard\nspecification described below.", 
            "title": "Port attributes"
        }, 
        {
            "location": "/application_packages/#stream-properties", 
            "text": "Streams can be configured using stream properties. The properties can be\nspecified using the parameter apex.stream. stream-name .prop. property-name   The constant \u201cstream\u201d\nspecifies that it is a stream,  stream-name  specifies the name of the\nstream and  property-name  the name of the property. The name of the\nstream is the same name that is passed when the stream is added to the\nDAG using the addStream method. An example illustrating the\nspecification is shown below. It sets the locality of the stream named\n\u201cstream1\u201d to container local indicating that the operators the stream is\nconnecting be run in the same container.     property \n     name apex.stream.stream1.prop.locality /name \n     value CONTAINER_LOCAL /value \n   /property   The property name is converted into a set method on the stream in the\nsame way as described in operator properties section above. In this case\nthe method would be setLocality and it will be called in the stream\n\u201cstream1\u201d with the value as the argument.  Along with the above system defined parameters, the applications can\ndefine their own specific parameters they can be specified in the\nconfiguration file. The only condition is that the names of these\nparameters don\u2019t conflict with the system defined parameters or similar\napplication parameters defined by other applications. To this end, it is\nrecommended that the application parameters have the format full-application-class-name . param-name .  The\nfull-application-class-name is the full JAVA class name of the\napplication including the package path and param-name is the name of the\nparameter within the application. The application will still have to\nstill read the parameter in using the configuration API of the\nconfiguration object that is passed in populateDAG.", 
            "title": "Stream properties"
        }, 
        {
            "location": "/application_packages/#wildcards", 
            "text": "Wildcards and regular expressions can be used in place of names to\nspecify a group for applications, operators, ports or streams. For\nexample, to specify an attribute for all ports of an operator it can be\ndone as follows  property \n   name apex.operator.range.port.*.attr.QUEUE_CAPACITY /name \n   value 4000 /value  /property   The wildcard \u201c*\u201d was used instead of the name of the port. Wildcard can\nalso be used for operator name, stream name or application name. Regular\nexpressions can also be used for names to specify attributes or\nproperties for a specific set.", 
            "title": "Wildcards"
        }, 
        {
            "location": "/application_packages/#adding-configuration-properties", 
            "text": "It is common for applications to require configuration parameters to\nrun.  For example, the address and port of the database, the location of\na file for ingestion, etc.  You can specify them in\nsrc/main/resources/META-INF/properties.xml under the App Package\nproject. The properties.xml may look like:  ?xml version= 1.0 ?  configuration \n   property \n     name some_name_1 /name \n   /property \n   property \n     name some_name_2 /name \n     value some_default_value /value \n   /property  /configuration   The name of an application-specific property takes the form of:  apex.operator.{opName}.prop.{propName}  The first represents the property with name propName of operator opName.\n Or you can set the application name at run time by setting this\nproperty:      apex.attr.APPLICATION_NAME  In this example, property some_name_1 is a required property which\nmust be set at launch time, or it must be set by a pre-set configuration\n(see next section).  Property some_name_2 is a property that is\nassigned with value some_default_value unless it is overridden at\nlaunch time.", 
            "title": "Adding configuration properties"
        }, 
        {
            "location": "/application_packages/#adding-pre-set-configurations", 
            "text": "At build time, you can add pre-set configurations to the App Package by\nadding configuration XML files under  src/site/conf/ conf .xml in your\nproject.  You can then specify which configuration to use at launch\ntime.  The configuration XML is of the same format of the properties.xml\nfile.", 
            "title": "Adding pre-set configurations"
        }, 
        {
            "location": "/application_packages/#application-specific-properties-file", 
            "text": "You can also specify properties.xml per application in the application\npackage.  Just create a file with the name properties-{appName}.xml and\nit will be picked up when you launch the application with the specified\nname within the application package.  In short:  properties.xml: Properties that are global to the Configuration\nPackage  properties-{appName}.xml: Properties that are specific when launching\nan application with the specified appName.", 
            "title": "Application-specific properties file"
        }, 
        {
            "location": "/application_packages/#properties-source-precedence", 
            "text": "If properties with the same key appear in multiple sources (e.g. from\napp package default configuration as META-INF/properties.xml, from app\npackage configuration in the conf directory, from launch time defines,\netc), the precedence of sources, from highest to lowest, is as follows:   Launch time defines (using -D option in CLI)  Launch time specified configuration file in file system (using -conf\n    option in CLI)  Launch time specified package configuration (using -apconf option in\n    CLI)  Configuration from \\$HOME/.dt/dt-site.xml  Application defaults within the package as\n    META-INF/properties-{appname}.xml  Package defaults as META-INF/properties.xml  dt-site.xml in local DT installation  dt-site.xml stored in HDFS", 
            "title": "Properties source precedence"
        }, 
        {
            "location": "/application_packages/#other-meta-data", 
            "text": "In a Apex App Package project, the pom.xml file contains a\nsection that looks like:  properties \n   apex.core.version 3.6.0 /apex.core.version \n   apex.apppackage.classpath\\ lib*.jar /apex.apppackage.classpath  /properties   apex.version is the Apache Apex version that are to be used\nwith this Application Package.  apex.apppackage.classpath is the classpath that is used when\nlaunching the application in the Application Package.  The default is\nlib/*.jar, where lib is where all the dependency jars are kept within\nthe Application Package.  One reason to change this field is when your\nApplication Package needs the classpath in a specific order.", 
            "title": "Other meta-data"
        }, 
        {
            "location": "/application_packages/#logging-configuration", 
            "text": "Just like other Java projects, you can change the logging configuration\nby having your log4j.properties under src/main/resources.  For example,\nif you have the following in src/main/resources/log4j.properties:   log4j.rootLogger=WARN,CONSOLE\n log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\n log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\n log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} [%t] %-5p\n %c{2} %M - %m%n  The root logger\u2019s level is set to WARN and the output is set to the console (stdout).  Note that by default from project created from the maven archetype,\nthere is already a log4j.properties file under src/test/resources and\nthat file is only used for the unit test.", 
            "title": "Logging configuration"
        }, 
        {
            "location": "/application_packages/#zip-structure-of-application-package", 
            "text": "Apache Apex Application Package files are zip files.  You can examine the content of any Application Package by using unzip -t on your Linux command line.  There are four top level directories in an Application Package:   \"app\" contains the jar files of the DAG code and any custom operators, and any JSON or properties files that specify a DAG.  \"lib\" contains all dependency jars  \"conf\" contains all the pre-set configuration XML files.  \"META-INF\" contains the MANIFEST.MF file and the properties.xml file.  \u201cresources\u201d contains any other files", 
            "title": "Zip Structure of Application Package"
        }, 
        {
            "location": "/application_packages/#examining-and-launching-application-packages-through-cli", 
            "text": "If you are working with Application Packages in the local filesystem, you can use the Apex Command Line Interface (apex).", 
            "title": "Examining and Launching Application Packages Through CLI"
        }, 
        {
            "location": "/application_packages/#getting-application-package-meta-information", 
            "text": "You can get the meta information about the Application Package using\nthis Apex CLI command.   apex  get-app-package-info  app-package-file", 
            "title": "Getting Application Package Meta Information"
        }, 
        {
            "location": "/application_packages/#getting-available-operators-in-application-package", 
            "text": "You can get the list of available operators in the Application Package\nusing this command.   apex  get-app-package-operators  app-package-file   package-prefix \n [parent-class]", 
            "title": "Getting Available Operators In Application Package"
        }, 
        {
            "location": "/application_packages/#getting-properties-of-operators-in-application-package", 
            "text": "You can get the list of properties of any operator in the Application\nPackage using this command.  apex  get-app-package-operator-properties", 
            "title": "Getting Properties of Operators in Application Package"
        }, 
        {
            "location": "/application_packages/#launching-an-application-package", 
            "text": "You can launch an application within an Application Package.  apex  launch [-D property-name=property-value, ...] [-conf config-name]\n [-apconf config-file-within-app-package]  app-package-file \n [matching-app-name]  Note that -conf expects a configuration file in the file system, while -apconf expects a configuration file within the app package.", 
            "title": "Launching an Application Package"
        }, 
        {
            "location": "/application_packages/#configuration-packages", 
            "text": "Sometimes just a configuration file is not enough for launching an application package. If a configuration requires\nadditional files to be packaged, you can use an Apex Configuration Package.", 
            "title": "Configuration Packages"
        }, 
        {
            "location": "/application_packages/#creating-configuration-packages", 
            "text": "Creating Configuration Packages is similar to creating Application Packages. You can create a configuration \npackage project using Maven by running the following command. Replace \"com.example\", \"myconfig\" and \"1.0-SNAPSHOT\" with the appropriate values:  $ mvn archetype:generate -DarchetypeGroupId=org.apache.apex \\\n  -DarchetypeArtifactId=apex-conf-archetype -DarchetypeVersion=RELEASE \\\n  -DgroupId=com.example -Dpackage=com.example.myconfig -DartifactId=myconfig \\\n  -Dversion=1.0-SNAPSHOT  And create the configuration package file by running:  $ mvn package  The \"mvn package\" command creates the Config Package file in target\ndirectory as target/myconfig.apc. You will be able to use that\nConfiguration Package file to launch an Apache Apex application.", 
            "title": "Creating Configuration Packages"
        }, 
        {
            "location": "/application_packages/#assembling-your-own-configuration-package", 
            "text": "Inside the project created by the archetype, these are the files that\nyou should know about when assembling your own configuration package:  ./pom.xml\n./src/main/resources/classpath\n./src/main/resources/files\n./src/main/resources/META-INF/properties.xml\n./src/main/resources/META-INF/properties-{appname}.xml", 
            "title": "Assembling your own configuration package"
        }, 
        {
            "location": "/application_packages/#pomxml", 
            "text": "Example:     groupId com.example /groupId \n   version 1.0.0 /version \n   artifactId myconfig /artifactId \n   packaging jar /packaging \n   !-- change these to the appropriate values -- \n   name My Apex Application Configuration /name \n   description My Custom Application Configuration Description /description \n   properties \n     apex.apppackage.name myapexapp /apex.apppackage.name \n     apex.apppackage.minversion 1.0.0 /apex.apppackage.minversion \n     apex.apppackage.maxversion 1.9999.9999 /apex.apppackage.maxversion \n     apex.appconf.classpath classpath/* /apex.appconf.classpath \n     apex.appconf.files files/* /apex.appconf.files \n   /properties   In pom.xml, you can change the following keys to your desired values   groupId  version  artifactId  name  description   You can also change the values of   apex.apppackage.name  apex.apppackage.minversion  apex.apppackage.maxversion   to reflect what Application Packages can be used with this configuration package.  Apex will use this information to check whether a\nconfiguration package is compatible with the Application Package when you issue a launch command.", 
            "title": "pom.xml"
        }, 
        {
            "location": "/application_packages/#srcmainresourcesclasspath", 
            "text": "Place any file in this directory that you\u2019d like to be copied to the\ncompute machines when launching an application and included in the\nclasspath of the application.  Example of such files are Java properties\nfiles and jar files.", 
            "title": "./src/main/resources/classpath"
        }, 
        {
            "location": "/application_packages/#srcmainresourcesfiles", 
            "text": "Place any file in this directory that you\u2019d like to be copied to the\ncompute machines when launching an application but not included in the\nclasspath of the application.", 
            "title": "./src/main/resources/files"
        }, 
        {
            "location": "/application_packages/#properties-xml-file", 
            "text": "A properties xml file consists of a set of key-value pairs.  The set of\nkey-value pairs specifies the configuration options the application\nshould be launched with.  Example:  configuration \n   property \n     name some-property-name /name \n     value some-property-value /value \n   /property \n   ... /configuration   Names of properties XML file:   properties.xml:  Properties that are global to the Configuration\nPackage  properties-{appName}.xml:  Properties that are specific when launching\nan application with the specified appName within the Application\nPackage.   After you are done with the above, remember to do mvn package to\ngenerate a new configuration package, which will be located in the\ntarget directory in your project.", 
            "title": "Properties XML file"
        }, 
        {
            "location": "/application_packages/#zip-structure-of-configuration-package", 
            "text": "Apex Application Configuration Package files are zip files.  You\ncan examine the content of any Application Configuration Package by\nusing unzip -t on your Linux command line.  The structure of the zip\nfile is as follow:  META-INF\n  MANIFEST.MF\n  properties.xml\n  properties-{appname}.xml\nclasspath\n  {classpath files}\nfiles\n  {files}", 
            "title": "Zip structure of configuration package"
        }, 
        {
            "location": "/application_packages/#launching-with-cli", 
            "text": "-conf  option of the launch command in CLI supports specifying configuration package in the local filesystem.  Example:  apex\\  launch myapp-1.0.0.apa -conf myconfig.apc  This command expects both the application package and the configuration package to be in the local file system.", 
            "title": "Launching with CLI"
        }, 
        {
            "location": "/operator_development/", 
            "text": "Operator Development Guide\n\n\nOperators are basic building blocks of an application built to run on\nApache Apex\u00a0platform. An application may consist of one or more\noperators each of which define some logical operation to be done on the\ntuples arriving at the operator. These operators are connected together\nusing streams forming a Directed Acyclic Graph (DAG). In other words, a streaming\napplication is represented by a DAG that consists of operations (called operators) and\ndata flow (called streams).\n\n\nIn this document we will discuss details on how an operator works and\nits internals. This document is intended to serve the following purposes\n\n\n\n\nApache Apex Operators\n\u00a0- Introduction to operator terminology and concepts.\n\n\nWriting Custom Operators\n\u00a0- Designing, coding and testing new operators from scratch.  Includes code examples.\n\n\nOperator Reference\n - Details of operator internals, lifecycle, and best practices and optimizations.\n\n\nAdvanced Features\n - Advanced features in operator development and its capabilities.\n\n\n\n\n\n\nApache Apex Operators \n\n\nOperators - \u201cWhat\u201d in a nutshell\n\n\nOperators are independent units of logical operations which can\ncontribute in executing the business logic of a use case. For example,\nin an ETL workflow, a filtering operation can be represented by a single\noperator. This filtering operator will be responsible for doing just one\ntask in the ETL pipeline, i.e. filter incoming tuples. Operators do not\nimpose any restrictions on what can or cannot be done as part of a\noperator. An operator may as well contain the entire business logic.\nHowever, it is recommended, that the operators are light weight\nindependent tasks, in\norder to take advantage of the distributed framework that Apache Apex\nprovides.\u00a0The structure of a streaming application shares resemblance\nwith the way CPU pipelining works. CPU pipelining breaks down the\ncomputation engine into different stages viz. instruction fetch,\ninstruction decode, etc. so that each of them can perform their task on\ndifferent instructions\nparallely. Similarly,\nApache Apex APIs allow the user to break down their tasks into different\nstages so that all of the tasks can be executed on different tuples\nparallely.\n\n\n\n\nOperators - \u201cHow\u201d in a nutshell\n\n\nAn Apache Apex application runs as a YARN application. Hence, each of\nthe operators that the application DAG contains, runs in one of the\ncontainers provisioned by YARN.\u00a0Further, Apache Apex exposes APIs to\nallow the user to request bundling multiple operators in a single node,\na single container or even a single thread. We shall look at these calls\nin the reference sections [cite reference sections]. For now, consider\nan operator as some piece of code that runs on some machine of a YARN\ncluster.\n\n\nTypes of Operators\n\n\nAn operator works on one tuple at a time. These tuples may be supplied\nby other operators in the application or by external sources,\nsuch as a database or a message bus. Similarly, after the tuples are\nprocessed, these may be passed on to other operators, or stored into an external system.\nThere are 3 type of operators based on function:\n\n\n\n\nInput Adapter\n - This is one of the starting points in\n    the\u00a0application DAG and is responsible for getting tuples from an\n    external system. At the same time, such data may also be generated\n    by the operator itself, without interacting with the outside\n    world.\u00a0These input tuples will form the initial universe of\n    data\u00a0that the application works on.\n\n\nGeneric Operator\n - This type of operator accepts input tuples from\n    the previous operators and passes\u00a0them on to the following operators\n    in the DAG.\n\n\nOutput Adapter\n - This is one of the ending points in the application\n    DAG and is responsible for writing the data out to some external\n    system.\n\n\n\n\nNote: There can be multiple operators of all types in an application\nDAG.\n\n\nOperators Position in a DAG\n\n\nWe may refer to operators depending on their position with respect to\none another. For any operator opr (see image below), there are two types of operators.\n\n\n\n\nUpstream operators\n - These are the operators from which there is a\n    directed path to opr\u00a0in the application DAG.\n\n\nDownstream operators\n - These are the operators to which there is a\n    directed path from opr\u00a0in the application DAG.\n\n\n\n\nNote that there are no cycles formed in the application\u00a0DAG.\n\n\n\n\nPorts\n\n\nOperators in a DAG are connected together via directed flows\ncalled streams. Each\u00a0stream\u00a0has end-points located on the operators\ncalled ports. Therea are 2 types of ports.\n\n\n\n\nInput Port\n - This is a\u00a0port through which an operator accepts input\n    tuples\u00a0from an upstream operator.\n\n\nOutput port\n - This is a\u00a0port through which an operator passes on the\n    processed data to downstream operators.\n\n\n\n\nLooking at the number of input ports, an Input Adapter is an operator\nwith no input ports, a Generic operator has both input and output ports,\nwhile an Output Adapter has no output ports. At the same time, note that\nan operator may act as an Input Adapter while at the same time have an\ninput port. In such cases, the operator is getting data from two\ndifferent sources, viz.\u00a0the input stream from the input port and an\nexternal source.\n\n\n\n\n\n\nHow Operator Works\n\n\nAn operator passes through various stages during its lifetime. Each\nstage is an API call that the Streaming Application Master makes for an\noperator. \u00a0The following figure illustrates the stages through which an\noperator passes.\n\n\n\n\n\n\nThe \nsetup()\n call initializes the operator and prepares itself to\n    start processing tuples.\n\n\nThe \nbeginWindow()\n call marks the beginning\u00a0of an\u00a0application window\n    and allows for any processing to be done before a window starts.\n\n\nThe \nprocess()\n call belongs to the \nInputPort\n and gets triggered when\n    any tuple arrives at the Input port of the operator. This call is\n    specific only to Generic and Output adapters, since Input Adapters\n    do not have an input port. This is made for all the tuples at the\n    input port until the end window marker tuple is received on the\n    input port.\n\n\nThe \nemitTuples()\n is the counterpart of \nprocess()\n call for Input\n    Adapters.\n    This call is used by Input adapters to emit any tuples that are\n    fetched from the external systems, or generated by the operator.\n    This method is called continuously until the pre-configured window\n    time is elapsed, at which the end window marker tuple is sent out on\n    the output port.\n\n\nThe \nendWindow()\n call marks the end of the window and allows for any\n    processing to be done after the window ends.\n\n\nThe \nteardown()\n call is used for gracefully shutting down the\n    operator and releasing any resources held by the operator.\n\n\n\n\nDeveloping Custom Operators \n\n\nAbout this tutorial\n\n\nThis tutorial will guide the user towards developing a operator from\nscratch. It includes all aspects of writing an operator including\ndesign, code and unit testing.\n\n\nIntroduction\n\n\nIn this tutorial, we will design and write, from scratch, an operator\ncalled Word Count. This operator will accept tuples of type String,\ncount the number of occurrences for each word appearing in the tuple and\nsend out the updated counts for all the words encountered in the tuple.\nFurther, the operator will also accept a file path on HDFS which will\ncontain the stop-words which need to be ignored when counting\noccurrences.\n\n\nDesign\n\n\nDesign of the operator must be finalized before starting to write an\noperator. Many aspects including the functionality, the data sources,\nthe types involved etc. need to be first finalized before writing the\noperator. Let us dive into each of these while considering the Word\nCount\u00a0operator.\n\n\nFunctionality\n\n\nWe can define the scope of operator functionality using the following\ntasks:\n\n\n\n\nParse the input tuple to identify the words in the tuple\n\n\nIdentify the stop-words in the tuple by looking up the stop-word\n    file as configured\n\n\nFor each non-stop-word in the tuple, count the occurrences in that\n    tuple and add it to a global counts\n\n\n\n\nLet\u2019s consider an example. Suppose we have the following tuples flow\ninto the Word Count operator.\n\n\n\n\nHumpty dumpty sat on a wall\n\n\nHumpty dumpty had a great fall\n\n\n\n\nInitially counts for all words\u00a0is 0. Once the first tuple is processed,\nthe counts that must be emitted are:\n\n\nhumpty - 1\ndumpty - 1\nsat - 1\nwall - 1\n\n\n\n\nNote that we are ignoring the stop-words, \u201con\u201d and \u201ca\u201d in this case.\nAlso note that as a rule, we\u2019ll ignore the case of the words when\ncounting occurrences.\n\n\nSimilarly, after the second tuple is processed, the counts that must be\nemitted are:\n\n\nhumpty - 2\ndumpty - 2\ngreat - 1\nfall - 1\n\n\n\n\nAgain, we ignore the words \n\u201chad\u201d\n and \n\u201ca\u201d\n since these are stop-words.\n\n\nNote that the most recent count for any word is correct count for that\nword. In other words, any new output for a word, invalidated all the\nprevious counts for that word.\n\n\nInputs\n\n\nAs seen from the example\u00a0above, the following\u00a0inputs are expected for\nthe operator:\n\n\n\n\nInput stream whose tuple type is String\n\n\nInput HDFS file path, pointing to a file containing stop-words\n\n\n\n\nOnly one input port is needed. The stop-word file will be small enough\nto be read completely in a single read. In addition this will be a one\ntime activity for the lifetime of the operator. This does not need a\nseparate input port.\n\n\n\n\nOutputs\n\n\nWe can define the output for this operator in multiple ways.\n\n\n\n\nThe operator may send out the set of counts for which the counts\n    have changed after processing each tuple.\n\n\nSome applications might not need an update after every tuple, but\n    only after\u00a0a certain time duration.\n\n\n\n\nLet us try and implement both these options depending on the\nconfiguration. Let us define a\u00a0boolean configuration parameter\n\n\u201csendPerTuple\u201d\n. The value of this parameter will indicate whether the\nupdated counts for words need to be emitted after processing each\ntuple\u00a0(true)\u00a0or after a certain time duration (false).\n\n\nThe type of information the operator will be sending out on the output\nport is the same for all the cases. This will be a \n key, value \n\u00a0pair,\nwhere the key\u00a0is the word while, the value is the latest count for that\nword. This means we just need one output port on which this information\nwill go out.\n\n\n\n\nConfiguration\n\n\nWe have the following configuration parameters:\n\n\n\n\nstopWordFilePath\n\u00a0- This parameter will store the path to the stop\n    word file on HDFS as configured by the user.\n\n\nsendPerTuple\n\u00a0- This parameter decides whether we send out the\n    updated counts after processing each tuple or at the end of a\n    window. When set to true, the operator will send out the updated\n    counts after each tuple, else it will send at the end of\n    each\u00a0window.\n\n\n\n\nCode\n\n\nThe source code for the tutorial can be found here:\n\n\nhttps://github.com/DataTorrent/examples/tree/master/tutorials/operatorTutorial\n\n\nOperator Reference \n\n\nThe Operator Class\n\n\nThe operator will exist physically as a class which implements the\nOperator\u00a0interface. This interface will require implementations for the\nfollowing method calls:\n\n\n\n\nsetup(OperatorContext context)\n\n\nbeginWindow(long windowId)\n\n\nendWindow()\n\n\ntearDown()\n\n\n\n\nIn order to simplify the creation of an operator, Apache\u00a0Apex\nlibrary also provides a base class \u201cBaseOperator\u201d which has empty\nimplementations for these methods. Please refer to the \nApex Operators\n\u00a0section and the\n\nReference\n\u00a0section for details on these.\n\n\nWe extend the class \u201cBaseOperator\u201d to create our own operator\n\u201cWordCountOperator\u201d.\n\n\npublic class WordCountOperator extends BaseOperator\n{\n}\n\n\n\n\nClass (Operator) properties\n\n\nWe define the following class variables:\n\n\n\n\nsendPerTuple\n\u00a0- Configures the output frequency from the operator\n\n\n\n\nprivate boolean sendPerTuple = true; // default\n\n\n\n\n\n\nstopWordFilePath\n\u00a0- Stores the path to the stop words file on HDFS\n\n\n\n\nprivate String stopWordFilePath;\u00a0// no default\n\n\n\n\n\n\nstopWords\n\u00a0- Stores the stop words read from the configured file\n\n\n\n\nprivate transient String[] stopWords;\n\n\n\n\n\n\nglobalCounts\n\u00a0- A Map which stores the counts of all the words\n    encountered so far. Note that this variable is non transient, which\n    means that this variable is saved as part of the checkpoint and can be recovered in event of a crash.\n\n\n\n\nprivate Map\nString, Long\n globalCounts;\n\n\n\n\n\n\nupdatedCounts\n\u00a0- A Map which stores the counts for only the most\n    recent tuple(s). sendPerTuple configuration determines whether to store the most recent or the recent\n    window worth of tuples.\n\n\n\n\nprivate transient Map\nString, Long\n updatedCounts;\n\n\n\n\n\n\ninput\n - The input port for the operator. The type of this input port\n    is String\u00a0which means it will only accept tuples of type String. The\n    definition of an input port requires implementation of a method\n    called process(String tuple), which should\u00a0have the processing logic\n    for the input tuple which \u00a0arrives at this input port. We delegate\n    this task to another method called processTuple(String tuple). This\n    helps in keeping the operator classes extensible by overriding the\n    processing logic for the input tuples.\n\n\n\n\npublic transient DefaultInputPort\nString\n input = new \u00a0 \u00a0\nDefaultInputPort\nString\n()\n{\n\u00a0\u00a0\u00a0\u00a0@Override\n\u00a0\u00a0\u00a0\u00a0public void process(String tuple)\n\u00a0\u00a0\u00a0\u00a0{\n    \u00a0\u00a0\u00a0\u00a0processTuple(tuple);\n\u00a0\u00a0\u00a0\u00a0}\n};\n\n\n\n\n\n\noutput - The output port for the operator. The type of this port is\n    Entry \n String, Long \n, which means the operator will emit \n word,\n    count \n pairs for the updated counts.\n\n\n\n\npublic transient DefaultOutputPort \nEntry\nString, Long\n output = new\nDefaultOutputPort\nEntry\nString,Long\n();\n\n\n\n\nThe Constructor\n\n\nThe constructor is the place where we initialize the non-transient data\nstructures,\u00a0since\nconstructor is called just once per activation of an operator. With regards to Word Count\u00a0operator, we initialize the globalCounts variable in the constructor.\n\n\nglobalCounts = Maps.newHashMap();\n\n\n\n\nSetup call\n\n\nThe setup method is called only once during an operator lifetime and its purpose is to allow\nthe operator to set itself up for processing incoming streams. Transient objects in the operator are\nnot serialized and checkpointed. Hence, it is essential that such objects initialized in the setup call.\nIn case of operator failure, the operator will be redeployed (most likely on a different container). The setup method called by the Apache Apex engine allows the operator to prepare for execution in the new container.\n\n\nThe following tasks are executed as part of the setup call:\n\n\n\n\nRead the stop-word list from HDFS and store it in the\n    stopWords\u00a0array\n\n\nInitialize updatedCounts\u00a0variable. This will store the updated\n    counts for words in most recent tuples processed by the operator.\n    As a transient variable, the value will be lost when operator fails.\n\n\n\n\nBegin Window call\n\n\nThe begin window call signals the start of an application window. With\nregards to Word Count Operator, we are expecting updated counts for the most recent window of\ndata if the sendPerTuple\u00a0is set to false. Hence, we clear the updatedCounts\u00a0variable in the begin window\ncall and start accumulating the counts till the end window call.\n\n\nProcess Tuple call\n\n\nThe processTuple\u00a0method is called by the process\u00a0method of the input\nport, input. This method defines the processing logic for the current\ntuple that is received at the input port. As part of this method, we\nidentify the words in the current tuple and update the globalCounts\u00a0and\nthe updatedCounts\u00a0variables. In addition, if the sendPerTuple\u00a0variable\nis set to true, we also emit the words\u00a0and corresponding counts in\nupdatedCounts\u00a0to the output port. Note\u00a0that in this case (sendPerTuple =\ntrue), we clear the updatedCounts\u00a0variable in every call to\nprocessTuple.\n\n\nEnd Window call\n\n\nThis call signals the end of an application window. With regards to Word\nCount Operator, we emit the updatedCounts\u00a0to the output port if the\nsendPerTuple\u00a0flag is set to false.\n\n\nTeardown call\n\n\nThis method allows the operator to gracefully shut down itself after\nreleasing the resources that it has acquired. With regards to our operator,\nwe call the shutDown\u00a0method which shuts down the operator along with any\ndownstream operators.\n\n\nTesting your Operator\n\n\nAs part of testing our operator, we test the following two facets:\n\n\n\n\nTest output of the operator after processing a single tuple\n\n\nTest output of the operator after processing of a window of tuples\n\n\n\n\nThe unit tests for the WordCount operator are available in the class\nWordCountOperatorTest.java. We simulate the behavior of the engine by\nusing the test utilities provided by Apache Apex libraries. We simulate\nthe setup, beginWindow, process\u00a0method of the input port and\nendWindow\u00a0calls and compare the output received at the simulated output\nports.\n\n\n\n\nInvoke constructor; non-transients initialized.\n\n\nCopy state from checkpoint -- initialized values from step 1 are\nreplaced.\n\n\n\n\nAdvanced Features \n\n\nControl Tuple Support\n\n\nOperators now also have the capability to emit control tuples. These control tuples are different from the control tuples used by the engine like BEGIN_WINDOW and END_WINDOW tuples. Operators can create and emit their own control tuples which can be used to communicate to the down stream operators regarding some event. Examples of such events can be BEGIN_FILE, or END_FILE.\nMore details can be found at \nControl Tuples\n\n\nMalhar Operator Library\n\n\nTo see the full list of Apex Malhar operators along with related documentation, visit \nApex Malhar on Github", 
            "title": "Operators"
        }, 
        {
            "location": "/operator_development/#operator-development-guide", 
            "text": "Operators are basic building blocks of an application built to run on\nApache Apex\u00a0platform. An application may consist of one or more\noperators each of which define some logical operation to be done on the\ntuples arriving at the operator. These operators are connected together\nusing streams forming a Directed Acyclic Graph (DAG). In other words, a streaming\napplication is represented by a DAG that consists of operations (called operators) and\ndata flow (called streams).  In this document we will discuss details on how an operator works and\nits internals. This document is intended to serve the following purposes   Apache Apex Operators \u00a0- Introduction to operator terminology and concepts.  Writing Custom Operators \u00a0- Designing, coding and testing new operators from scratch.  Includes code examples.  Operator Reference  - Details of operator internals, lifecycle, and best practices and optimizations.  Advanced Features  - Advanced features in operator development and its capabilities.", 
            "title": "Operator Development Guide"
        }, 
        {
            "location": "/operator_development/#apache-apex-operators", 
            "text": "", 
            "title": "Apache Apex Operators "
        }, 
        {
            "location": "/operator_development/#operators-what-in-a-nutshell", 
            "text": "Operators are independent units of logical operations which can\ncontribute in executing the business logic of a use case. For example,\nin an ETL workflow, a filtering operation can be represented by a single\noperator. This filtering operator will be responsible for doing just one\ntask in the ETL pipeline, i.e. filter incoming tuples. Operators do not\nimpose any restrictions on what can or cannot be done as part of a\noperator. An operator may as well contain the entire business logic.\nHowever, it is recommended, that the operators are light weight\nindependent tasks, in\norder to take advantage of the distributed framework that Apache Apex\nprovides.\u00a0The structure of a streaming application shares resemblance\nwith the way CPU pipelining works. CPU pipelining breaks down the\ncomputation engine into different stages viz. instruction fetch,\ninstruction decode, etc. so that each of them can perform their task on\ndifferent instructions\nparallely. Similarly,\nApache Apex APIs allow the user to break down their tasks into different\nstages so that all of the tasks can be executed on different tuples\nparallely.", 
            "title": "Operators - \u201cWhat\u201d in a nutshell"
        }, 
        {
            "location": "/operator_development/#operators-how-in-a-nutshell", 
            "text": "An Apache Apex application runs as a YARN application. Hence, each of\nthe operators that the application DAG contains, runs in one of the\ncontainers provisioned by YARN.\u00a0Further, Apache Apex exposes APIs to\nallow the user to request bundling multiple operators in a single node,\na single container or even a single thread. We shall look at these calls\nin the reference sections [cite reference sections]. For now, consider\nan operator as some piece of code that runs on some machine of a YARN\ncluster.", 
            "title": "Operators - \u201cHow\u201d in a nutshell"
        }, 
        {
            "location": "/operator_development/#types-of-operators", 
            "text": "An operator works on one tuple at a time. These tuples may be supplied\nby other operators in the application or by external sources,\nsuch as a database or a message bus. Similarly, after the tuples are\nprocessed, these may be passed on to other operators, or stored into an external system.\nThere are 3 type of operators based on function:   Input Adapter  - This is one of the starting points in\n    the\u00a0application DAG and is responsible for getting tuples from an\n    external system. At the same time, such data may also be generated\n    by the operator itself, without interacting with the outside\n    world.\u00a0These input tuples will form the initial universe of\n    data\u00a0that the application works on.  Generic Operator  - This type of operator accepts input tuples from\n    the previous operators and passes\u00a0them on to the following operators\n    in the DAG.  Output Adapter  - This is one of the ending points in the application\n    DAG and is responsible for writing the data out to some external\n    system.   Note: There can be multiple operators of all types in an application\nDAG.", 
            "title": "Types of Operators"
        }, 
        {
            "location": "/operator_development/#operators-position-in-a-dag", 
            "text": "We may refer to operators depending on their position with respect to\none another. For any operator opr (see image below), there are two types of operators.   Upstream operators  - These are the operators from which there is a\n    directed path to opr\u00a0in the application DAG.  Downstream operators  - These are the operators to which there is a\n    directed path from opr\u00a0in the application DAG.   Note that there are no cycles formed in the application\u00a0DAG.", 
            "title": "Operators Position in a DAG"
        }, 
        {
            "location": "/operator_development/#ports", 
            "text": "Operators in a DAG are connected together via directed flows\ncalled streams. Each\u00a0stream\u00a0has end-points located on the operators\ncalled ports. Therea are 2 types of ports.   Input Port  - This is a\u00a0port through which an operator accepts input\n    tuples\u00a0from an upstream operator.  Output port  - This is a\u00a0port through which an operator passes on the\n    processed data to downstream operators.   Looking at the number of input ports, an Input Adapter is an operator\nwith no input ports, a Generic operator has both input and output ports,\nwhile an Output Adapter has no output ports. At the same time, note that\nan operator may act as an Input Adapter while at the same time have an\ninput port. In such cases, the operator is getting data from two\ndifferent sources, viz.\u00a0the input stream from the input port and an\nexternal source.", 
            "title": "Ports"
        }, 
        {
            "location": "/operator_development/#how-operator-works", 
            "text": "An operator passes through various stages during its lifetime. Each\nstage is an API call that the Streaming Application Master makes for an\noperator. \u00a0The following figure illustrates the stages through which an\noperator passes.    The  setup()  call initializes the operator and prepares itself to\n    start processing tuples.  The  beginWindow()  call marks the beginning\u00a0of an\u00a0application window\n    and allows for any processing to be done before a window starts.  The  process()  call belongs to the  InputPort  and gets triggered when\n    any tuple arrives at the Input port of the operator. This call is\n    specific only to Generic and Output adapters, since Input Adapters\n    do not have an input port. This is made for all the tuples at the\n    input port until the end window marker tuple is received on the\n    input port.  The  emitTuples()  is the counterpart of  process()  call for Input\n    Adapters.\n    This call is used by Input adapters to emit any tuples that are\n    fetched from the external systems, or generated by the operator.\n    This method is called continuously until the pre-configured window\n    time is elapsed, at which the end window marker tuple is sent out on\n    the output port.  The  endWindow()  call marks the end of the window and allows for any\n    processing to be done after the window ends.  The  teardown()  call is used for gracefully shutting down the\n    operator and releasing any resources held by the operator.", 
            "title": "How Operator Works"
        }, 
        {
            "location": "/operator_development/#developing-custom-operators", 
            "text": "", 
            "title": "Developing Custom Operators "
        }, 
        {
            "location": "/operator_development/#about-this-tutorial", 
            "text": "This tutorial will guide the user towards developing a operator from\nscratch. It includes all aspects of writing an operator including\ndesign, code and unit testing.", 
            "title": "About this tutorial"
        }, 
        {
            "location": "/operator_development/#introduction", 
            "text": "In this tutorial, we will design and write, from scratch, an operator\ncalled Word Count. This operator will accept tuples of type String,\ncount the number of occurrences for each word appearing in the tuple and\nsend out the updated counts for all the words encountered in the tuple.\nFurther, the operator will also accept a file path on HDFS which will\ncontain the stop-words which need to be ignored when counting\noccurrences.", 
            "title": "Introduction"
        }, 
        {
            "location": "/operator_development/#design", 
            "text": "Design of the operator must be finalized before starting to write an\noperator. Many aspects including the functionality, the data sources,\nthe types involved etc. need to be first finalized before writing the\noperator. Let us dive into each of these while considering the Word\nCount\u00a0operator.", 
            "title": "Design"
        }, 
        {
            "location": "/operator_development/#functionality", 
            "text": "We can define the scope of operator functionality using the following\ntasks:   Parse the input tuple to identify the words in the tuple  Identify the stop-words in the tuple by looking up the stop-word\n    file as configured  For each non-stop-word in the tuple, count the occurrences in that\n    tuple and add it to a global counts   Let\u2019s consider an example. Suppose we have the following tuples flow\ninto the Word Count operator.   Humpty dumpty sat on a wall  Humpty dumpty had a great fall   Initially counts for all words\u00a0is 0. Once the first tuple is processed,\nthe counts that must be emitted are:  humpty - 1\ndumpty - 1\nsat - 1\nwall - 1  Note that we are ignoring the stop-words, \u201con\u201d and \u201ca\u201d in this case.\nAlso note that as a rule, we\u2019ll ignore the case of the words when\ncounting occurrences.  Similarly, after the second tuple is processed, the counts that must be\nemitted are:  humpty - 2\ndumpty - 2\ngreat - 1\nfall - 1  Again, we ignore the words  \u201chad\u201d  and  \u201ca\u201d  since these are stop-words.  Note that the most recent count for any word is correct count for that\nword. In other words, any new output for a word, invalidated all the\nprevious counts for that word.", 
            "title": "Functionality"
        }, 
        {
            "location": "/operator_development/#inputs", 
            "text": "As seen from the example\u00a0above, the following\u00a0inputs are expected for\nthe operator:   Input stream whose tuple type is String  Input HDFS file path, pointing to a file containing stop-words   Only one input port is needed. The stop-word file will be small enough\nto be read completely in a single read. In addition this will be a one\ntime activity for the lifetime of the operator. This does not need a\nseparate input port.", 
            "title": "Inputs"
        }, 
        {
            "location": "/operator_development/#outputs", 
            "text": "We can define the output for this operator in multiple ways.   The operator may send out the set of counts for which the counts\n    have changed after processing each tuple.  Some applications might not need an update after every tuple, but\n    only after\u00a0a certain time duration.   Let us try and implement both these options depending on the\nconfiguration. Let us define a\u00a0boolean configuration parameter \u201csendPerTuple\u201d . The value of this parameter will indicate whether the\nupdated counts for words need to be emitted after processing each\ntuple\u00a0(true)\u00a0or after a certain time duration (false).  The type of information the operator will be sending out on the output\nport is the same for all the cases. This will be a   key, value  \u00a0pair,\nwhere the key\u00a0is the word while, the value is the latest count for that\nword. This means we just need one output port on which this information\nwill go out.", 
            "title": "Outputs"
        }, 
        {
            "location": "/operator_development/#configuration", 
            "text": "We have the following configuration parameters:   stopWordFilePath \u00a0- This parameter will store the path to the stop\n    word file on HDFS as configured by the user.  sendPerTuple \u00a0- This parameter decides whether we send out the\n    updated counts after processing each tuple or at the end of a\n    window. When set to true, the operator will send out the updated\n    counts after each tuple, else it will send at the end of\n    each\u00a0window.", 
            "title": "Configuration"
        }, 
        {
            "location": "/operator_development/#code", 
            "text": "The source code for the tutorial can be found here:  https://github.com/DataTorrent/examples/tree/master/tutorials/operatorTutorial", 
            "title": "Code"
        }, 
        {
            "location": "/operator_development/#operator-reference", 
            "text": "", 
            "title": "Operator Reference "
        }, 
        {
            "location": "/operator_development/#the-operator-class", 
            "text": "The operator will exist physically as a class which implements the\nOperator\u00a0interface. This interface will require implementations for the\nfollowing method calls:   setup(OperatorContext context)  beginWindow(long windowId)  endWindow()  tearDown()   In order to simplify the creation of an operator, Apache\u00a0Apex\nlibrary also provides a base class \u201cBaseOperator\u201d which has empty\nimplementations for these methods. Please refer to the  Apex Operators \u00a0section and the Reference \u00a0section for details on these.  We extend the class \u201cBaseOperator\u201d to create our own operator\n\u201cWordCountOperator\u201d.  public class WordCountOperator extends BaseOperator\n{\n}", 
            "title": "The Operator Class"
        }, 
        {
            "location": "/operator_development/#class-operator-properties", 
            "text": "We define the following class variables:   sendPerTuple \u00a0- Configures the output frequency from the operator   private boolean sendPerTuple = true; // default   stopWordFilePath \u00a0- Stores the path to the stop words file on HDFS   private String stopWordFilePath;\u00a0// no default   stopWords \u00a0- Stores the stop words read from the configured file   private transient String[] stopWords;   globalCounts \u00a0- A Map which stores the counts of all the words\n    encountered so far. Note that this variable is non transient, which\n    means that this variable is saved as part of the checkpoint and can be recovered in event of a crash.   private Map String, Long  globalCounts;   updatedCounts \u00a0- A Map which stores the counts for only the most\n    recent tuple(s). sendPerTuple configuration determines whether to store the most recent or the recent\n    window worth of tuples.   private transient Map String, Long  updatedCounts;   input  - The input port for the operator. The type of this input port\n    is String\u00a0which means it will only accept tuples of type String. The\n    definition of an input port requires implementation of a method\n    called process(String tuple), which should\u00a0have the processing logic\n    for the input tuple which \u00a0arrives at this input port. We delegate\n    this task to another method called processTuple(String tuple). This\n    helps in keeping the operator classes extensible by overriding the\n    processing logic for the input tuples.   public transient DefaultInputPort String  input = new \u00a0 \u00a0\nDefaultInputPort String ()\n{\n\u00a0\u00a0\u00a0\u00a0@Override\n\u00a0\u00a0\u00a0\u00a0public void process(String tuple)\n\u00a0\u00a0\u00a0\u00a0{\n    \u00a0\u00a0\u00a0\u00a0processTuple(tuple);\n\u00a0\u00a0\u00a0\u00a0}\n};   output - The output port for the operator. The type of this port is\n    Entry   String, Long  , which means the operator will emit   word,\n    count   pairs for the updated counts.   public transient DefaultOutputPort  Entry String, Long  output = new\nDefaultOutputPort Entry String,Long ();", 
            "title": "Class (Operator) properties"
        }, 
        {
            "location": "/operator_development/#the-constructor", 
            "text": "The constructor is the place where we initialize the non-transient data\nstructures,\u00a0since\nconstructor is called just once per activation of an operator. With regards to Word Count\u00a0operator, we initialize the globalCounts variable in the constructor.  globalCounts = Maps.newHashMap();", 
            "title": "The Constructor"
        }, 
        {
            "location": "/operator_development/#setup-call", 
            "text": "The setup method is called only once during an operator lifetime and its purpose is to allow\nthe operator to set itself up for processing incoming streams. Transient objects in the operator are\nnot serialized and checkpointed. Hence, it is essential that such objects initialized in the setup call.\nIn case of operator failure, the operator will be redeployed (most likely on a different container). The setup method called by the Apache Apex engine allows the operator to prepare for execution in the new container.  The following tasks are executed as part of the setup call:   Read the stop-word list from HDFS and store it in the\n    stopWords\u00a0array  Initialize updatedCounts\u00a0variable. This will store the updated\n    counts for words in most recent tuples processed by the operator.\n    As a transient variable, the value will be lost when operator fails.", 
            "title": "Setup call"
        }, 
        {
            "location": "/operator_development/#begin-window-call", 
            "text": "The begin window call signals the start of an application window. With\nregards to Word Count Operator, we are expecting updated counts for the most recent window of\ndata if the sendPerTuple\u00a0is set to false. Hence, we clear the updatedCounts\u00a0variable in the begin window\ncall and start accumulating the counts till the end window call.", 
            "title": "Begin Window call"
        }, 
        {
            "location": "/operator_development/#process-tuple-call", 
            "text": "The processTuple\u00a0method is called by the process\u00a0method of the input\nport, input. This method defines the processing logic for the current\ntuple that is received at the input port. As part of this method, we\nidentify the words in the current tuple and update the globalCounts\u00a0and\nthe updatedCounts\u00a0variables. In addition, if the sendPerTuple\u00a0variable\nis set to true, we also emit the words\u00a0and corresponding counts in\nupdatedCounts\u00a0to the output port. Note\u00a0that in this case (sendPerTuple =\ntrue), we clear the updatedCounts\u00a0variable in every call to\nprocessTuple.", 
            "title": "Process Tuple call"
        }, 
        {
            "location": "/operator_development/#end-window-call", 
            "text": "This call signals the end of an application window. With regards to Word\nCount Operator, we emit the updatedCounts\u00a0to the output port if the\nsendPerTuple\u00a0flag is set to false.", 
            "title": "End Window call"
        }, 
        {
            "location": "/operator_development/#teardown-call", 
            "text": "This method allows the operator to gracefully shut down itself after\nreleasing the resources that it has acquired. With regards to our operator,\nwe call the shutDown\u00a0method which shuts down the operator along with any\ndownstream operators.", 
            "title": "Teardown call"
        }, 
        {
            "location": "/operator_development/#testing-your-operator", 
            "text": "As part of testing our operator, we test the following two facets:   Test output of the operator after processing a single tuple  Test output of the operator after processing of a window of tuples   The unit tests for the WordCount operator are available in the class\nWordCountOperatorTest.java. We simulate the behavior of the engine by\nusing the test utilities provided by Apache Apex libraries. We simulate\nthe setup, beginWindow, process\u00a0method of the input port and\nendWindow\u00a0calls and compare the output received at the simulated output\nports.   Invoke constructor; non-transients initialized.  Copy state from checkpoint -- initialized values from step 1 are\nreplaced.", 
            "title": "Testing your Operator"
        }, 
        {
            "location": "/operator_development/#advanced-features", 
            "text": "", 
            "title": "Advanced Features "
        }, 
        {
            "location": "/operator_development/#control-tuple-support", 
            "text": "Operators now also have the capability to emit control tuples. These control tuples are different from the control tuples used by the engine like BEGIN_WINDOW and END_WINDOW tuples. Operators can create and emit their own control tuples which can be used to communicate to the down stream operators regarding some event. Examples of such events can be BEGIN_FILE, or END_FILE.\nMore details can be found at  Control Tuples", 
            "title": "Control Tuple Support"
        }, 
        {
            "location": "/operator_development/#malhar-operator-library", 
            "text": "To see the full list of Apex Malhar operators along with related documentation, visit  Apex Malhar on Github", 
            "title": "Malhar Operator Library"
        }, 
        {
            "location": "/autometrics/", 
            "text": "Apache Apex AutoMetrics\n\n\nIntroduction\n\n\nMetrics collect various statistical information about a process which can be very useful for diagnosis. Auto Metrics in Apex can help monitor operators in a running application.  The goal of \nAutoMetric\n API is to enable operator developer to define relevant metrics for an operator in a simple way which the platform collects and reports automatically.\n\n\nSpecifying AutoMetrics in an Operator\n\n\nAn \nAutoMetric\n can be any object. It can be of a primitive type - int, long, etc. or a complex one. A field or a \nget\n method in an operator can be annotated with \n@AutoMetric\n to specify that its value is a metric. After every application end window, the platform collects the values of these fields/methods in a map and sends it to application master.\n\n\n\n\npublic class LineReceiver extends BaseOperator\n{\n @AutoMetric\n long length;\n\n @AutoMetric\n long count;\n\n public final transient DefaultInputPort\nString\n input = new DefaultInputPort\nString\n()\n {\n   @Override\n   public void process(String s)\n   {\n     length += s.length();\n     count++;\n   }\n };\n\n @Override\n public void beginWindow(long windowId)\n {\n   length = 0;\n   count = 0;\n }\n}\n\n\n\n\nThere are 2 auto-metrics declared in the \nLineReceiver\n. At the end of each application window, the platform will send a map with 2 entries - \n[(length, 100), (count, 10)]\n to the application master.\n\n\nAggregating AutoMetrics across Partitions\n\n\nWhen an operator is partitioned, it is useful to aggregate the values of auto-metrics across all its partitions every window to get a logical view of these metrics. The application master performs these aggregations using metrics aggregators.\n\n\nThe AutoMetric API helps to achieve this by providing an interface for writing aggregators- \nAutoMetric.Aggregator\n. Any implementation of \nAutoMetric.Aggregator\n can be set as an operator attribute - \nMETRICS_AGGREGATOR\n for a particular operator which in turn is used for aggregating physical metrics.\n\n\nDefault aggregators\n\n\nMetricsAggregator\n is a simple implementation of \nAutoMetric.Aggregator\n that platform uses as a default for summing up primitive types - int, long, float and double.\n\n\nMetricsAggregator\n is just a collection of \nSingleMetricAggregator\ns. There are multiple implementations of \nSingleMetricAggregator\n that perform sum, min, max, avg which are present in Apex core and Apex malhar.\n\n\nFor the \nLineReceiver\n operator, the application developer need not specify any aggregator. The platform will automatically inject an instance of \nMetricsAggregator\n that contains two \nLongSumAggregator\ns - one for \nlength\n and one for \ncount\n. This aggregator will report sum of length and sum of count across all the partitions of \nLineReceiver\n.\n\n\nBuilding custom aggregators\n\n\nPlatform cannot perform any meaningful aggregations for non-numeric metrics. In such cases, the operator or application developer can write custom aggregators. Let\u2019s say, if the \nLineReceiver\n was modified to have a complex metric as shown below.\n\n\npublic class AnotherLineReceiver extends BaseOperator\n{\n  @AutoMetric\n  final LineMetrics lineMetrics = new LineMetrics();\n\n  public final transient DefaultInputPort\nString\n input = new DefaultInputPort\nString\n()\n  {\n    @Override\n    public void process(String s)\n    {\n      lineMetrics.length += s.length();\n      lineMetrics.count++;\n    }\n  };\n\n  @Override\n  public void beginWindow(long windowId)\n  {\n    lineMetrics.length = 0;\n    lineMetrics.count = 0;\n  }\n\n  public static class LineMetrics implements Serializable\n  {\n    long length;\n    long count;\n\n    private static final long serialVersionUID = 201511041908L;\n  }\n}\n\n\n\n\nBelow is a custom aggregator that can calculate average line length across all partitions of \nAnotherLineReceiver\n.\n\n\npublic class AvgLineLengthAggregator implements AutoMetric.Aggregator\n{\n\n  Map\nString, Object\n result = Maps.newHashMap();\n\n  @Override\n  public Map\nString, Object\n aggregate(long l, Collection\nAutoMetric.PhysicalMetricsContext\n collection)\n  {\n    long totalLength = 0;\n    long totalCount = 0;\n    for (AutoMetric.PhysicalMetricsContext pmc : collection) {\n      AnotherLineReceiver.LineMetrics lm = (AnotherLineReceiver.LineMetrics)pmc.getMetrics().get(\nlineMetrics\n);\n      totalLength += lm.length;\n      totalCount += lm.count;\n    }\n    result.put(\navgLineLength\n, totalLength/totalCount);\n    return result;\n  }\n}\n\n\n\n\nAn instance of above aggregator can be specified as the \nMETRIC_AGGREGATOR\n for \nAnotherLineReceiver\n while creating the DAG as shown below.\n\n\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    ...\n    AnotherLineReceiver lineReceiver = dag.addOperator(\nLineReceiver\n, new AnotherLineReceiver());\n    dag.setAttribute(lineReceiver, Context.OperatorContext.METRICS_AGGREGATOR, new AvgLineLengthAggregator());\n    ...\n  }\n\n\n\n\nRetrieving AutoMetrics\n\n\nThere are two options for retrieving the AutoMetrics:\n\n\n\n\nThrought DataTorrent Gateway REST API\n\n\nThrough REST service on the port of the running STRAM\n\n\n\n\nThe Gateway REST API provides a way to retrieve the latest AutoMetrics for each logical operator.  For example:\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}\n{\n    ...\n    \nautoMetrics\n: {\n       \ncount\n: \n71314\n,\n       \nlength\n: \n27780706\n\n    },\n    \nclassName\n: \ncom.datatorrent.autometric.LineReceiver\n,\n    ...\n}\n\n\n\n\nSystem Metrics\n\n\nSystem metrics are standard operator metrics provided by the system.  Examples include:\n\n\n\n\nprocessed tuples per second\n\n\nemitted tuples per second\n\n\ntotal tuples processed\n\n\ntotal tuples emitted\n\n\nlatency\n\n\nCPU percentage\n\n\nfailure count\n\n\ncheckpoint elapsed time\n\n\n\n\nThe Gateway REST API provides a way to retrieve the latest values for all of the above for each of the logical operators in the application.\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}\n{\n    ...\n    \ncpuPercentageMA\n: \n{cpuPercentageMA}\n,\n    \nfailureCount\n: \n{failureCount}\n,\n    \nlatencyMA\n: \n{latencyMA}\n,  \n    \ntotalTuplesEmitted\n: \n{totalTuplesEmitted}\n,\n    \ntotalTuplesProcessed\n: \n{totalTuplesProcessed}\n,\n    \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n    \ntuplesProcessedPSMA\n: \n{tuplesProcessedPSMA}\n,\n    ...\n}\n\n\n\n\nHowever, just like AutoMetrics, the Gateway only provides the latest metrics.  For historical metrics, we will need the help of \nApp Data Tracker\n.", 
            "title": "AutoMetric API"
        }, 
        {
            "location": "/autometrics/#apache-apex-autometrics", 
            "text": "", 
            "title": "Apache Apex AutoMetrics"
        }, 
        {
            "location": "/autometrics/#introduction", 
            "text": "Metrics collect various statistical information about a process which can be very useful for diagnosis. Auto Metrics in Apex can help monitor operators in a running application.  The goal of  AutoMetric  API is to enable operator developer to define relevant metrics for an operator in a simple way which the platform collects and reports automatically.", 
            "title": "Introduction"
        }, 
        {
            "location": "/autometrics/#specifying-autometrics-in-an-operator", 
            "text": "An  AutoMetric  can be any object. It can be of a primitive type - int, long, etc. or a complex one. A field or a  get  method in an operator can be annotated with  @AutoMetric  to specify that its value is a metric. After every application end window, the platform collects the values of these fields/methods in a map and sends it to application master.   public class LineReceiver extends BaseOperator\n{\n @AutoMetric\n long length;\n\n @AutoMetric\n long count;\n\n public final transient DefaultInputPort String  input = new DefaultInputPort String ()\n {\n   @Override\n   public void process(String s)\n   {\n     length += s.length();\n     count++;\n   }\n };\n\n @Override\n public void beginWindow(long windowId)\n {\n   length = 0;\n   count = 0;\n }\n}  There are 2 auto-metrics declared in the  LineReceiver . At the end of each application window, the platform will send a map with 2 entries -  [(length, 100), (count, 10)]  to the application master.", 
            "title": "Specifying AutoMetrics in an Operator"
        }, 
        {
            "location": "/autometrics/#aggregating-autometrics-across-partitions", 
            "text": "When an operator is partitioned, it is useful to aggregate the values of auto-metrics across all its partitions every window to get a logical view of these metrics. The application master performs these aggregations using metrics aggregators.  The AutoMetric API helps to achieve this by providing an interface for writing aggregators-  AutoMetric.Aggregator . Any implementation of  AutoMetric.Aggregator  can be set as an operator attribute -  METRICS_AGGREGATOR  for a particular operator which in turn is used for aggregating physical metrics.", 
            "title": "Aggregating AutoMetrics across Partitions"
        }, 
        {
            "location": "/autometrics/#default-aggregators", 
            "text": "MetricsAggregator  is a simple implementation of  AutoMetric.Aggregator  that platform uses as a default for summing up primitive types - int, long, float and double.  MetricsAggregator  is just a collection of  SingleMetricAggregator s. There are multiple implementations of  SingleMetricAggregator  that perform sum, min, max, avg which are present in Apex core and Apex malhar.  For the  LineReceiver  operator, the application developer need not specify any aggregator. The platform will automatically inject an instance of  MetricsAggregator  that contains two  LongSumAggregator s - one for  length  and one for  count . This aggregator will report sum of length and sum of count across all the partitions of  LineReceiver .", 
            "title": "Default aggregators"
        }, 
        {
            "location": "/autometrics/#building-custom-aggregators", 
            "text": "Platform cannot perform any meaningful aggregations for non-numeric metrics. In such cases, the operator or application developer can write custom aggregators. Let\u2019s say, if the  LineReceiver  was modified to have a complex metric as shown below.  public class AnotherLineReceiver extends BaseOperator\n{\n  @AutoMetric\n  final LineMetrics lineMetrics = new LineMetrics();\n\n  public final transient DefaultInputPort String  input = new DefaultInputPort String ()\n  {\n    @Override\n    public void process(String s)\n    {\n      lineMetrics.length += s.length();\n      lineMetrics.count++;\n    }\n  };\n\n  @Override\n  public void beginWindow(long windowId)\n  {\n    lineMetrics.length = 0;\n    lineMetrics.count = 0;\n  }\n\n  public static class LineMetrics implements Serializable\n  {\n    long length;\n    long count;\n\n    private static final long serialVersionUID = 201511041908L;\n  }\n}  Below is a custom aggregator that can calculate average line length across all partitions of  AnotherLineReceiver .  public class AvgLineLengthAggregator implements AutoMetric.Aggregator\n{\n\n  Map String, Object  result = Maps.newHashMap();\n\n  @Override\n  public Map String, Object  aggregate(long l, Collection AutoMetric.PhysicalMetricsContext  collection)\n  {\n    long totalLength = 0;\n    long totalCount = 0;\n    for (AutoMetric.PhysicalMetricsContext pmc : collection) {\n      AnotherLineReceiver.LineMetrics lm = (AnotherLineReceiver.LineMetrics)pmc.getMetrics().get( lineMetrics );\n      totalLength += lm.length;\n      totalCount += lm.count;\n    }\n    result.put( avgLineLength , totalLength/totalCount);\n    return result;\n  }\n}  An instance of above aggregator can be specified as the  METRIC_AGGREGATOR  for  AnotherLineReceiver  while creating the DAG as shown below.    @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    ...\n    AnotherLineReceiver lineReceiver = dag.addOperator( LineReceiver , new AnotherLineReceiver());\n    dag.setAttribute(lineReceiver, Context.OperatorContext.METRICS_AGGREGATOR, new AvgLineLengthAggregator());\n    ...\n  }", 
            "title": "Building custom aggregators"
        }, 
        {
            "location": "/autometrics/#retrieving-autometrics", 
            "text": "There are two options for retrieving the AutoMetrics:   Throught DataTorrent Gateway REST API  Through REST service on the port of the running STRAM   The Gateway REST API provides a way to retrieve the latest AutoMetrics for each logical operator.  For example:  GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}\n{\n    ...\n     autoMetrics : {\n        count :  71314 ,\n        length :  27780706 \n    },\n     className :  com.datatorrent.autometric.LineReceiver ,\n    ...\n}", 
            "title": "Retrieving AutoMetrics"
        }, 
        {
            "location": "/autometrics/#system-metrics", 
            "text": "System metrics are standard operator metrics provided by the system.  Examples include:   processed tuples per second  emitted tuples per second  total tuples processed  total tuples emitted  latency  CPU percentage  failure count  checkpoint elapsed time   The Gateway REST API provides a way to retrieve the latest values for all of the above for each of the logical operators in the application.  GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}\n{\n    ...\n     cpuPercentageMA :  {cpuPercentageMA} ,\n     failureCount :  {failureCount} ,\n     latencyMA :  {latencyMA} ,  \n     totalTuplesEmitted :  {totalTuplesEmitted} ,\n     totalTuplesProcessed :  {totalTuplesProcessed} ,\n     tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n     tuplesProcessedPSMA :  {tuplesProcessedPSMA} ,\n    ...\n}  However, just like AutoMetrics, the Gateway only provides the latest metrics.  For historical metrics, we will need the help of  App Data Tracker .", 
            "title": "System Metrics"
        }, 
        {
            "location": "/control_tuples/", 
            "text": "User Defined Control Tuples\n\n\nIntroduction\n\n\nCustom control tuple support in Apache Apex gives the user the capability to insert user defined control tuples in the data flow. For analogy, the engine already supports a few pre-defined control tuples like BEGIN_WINDOW, END_WINDOW, etc. Until now, we did not have the support for applications to insert their own control tuples.\n\n\nTerminology\n\n\nAll discussion in this document is related to Control Tuples generated by user defined logic. The document may refer to these tuples as \nControl Tuples\n, \nUser Defined Control Tuples\n or \nCustom Control Tuples\n interchangeably.\n\n\nDefinition\n\n\nA user defined control tuple could be any user defined object which implements a ControlTuple interface.\n\n\nSee \nDelivery Semantics\n for details on DeliveryType\n\n\npublic interface ControlTuple\n{\n  DeliveryType getDeliveryType();\n\n  enum DeliveryType\n  {\n    IMMEDIATE,\n    END_WINDOW\n  }\n}\n\n\n\n\n\nExample user defined control tuple:\n\n\npublic class TestControlTuple implements ControlTuple\n{\n  public long data;\n  public boolean immediate;\n\n  // For Kryo\n  public TestControlTuple()\n  {\n    data = 0;\n  }\n\n  // Constructor\n  public TestControlTuple(long data, boolean immediate)\n  {\n    this.data = data;\n    this.immediate = immediate;\n  }\n\n  @Override\n  public DeliveryType getDeliveryType()\n  {\n    if (immediate) {\n      return DeliveryType.IMMEDIATE;\n    } else {\n      return DeliveryType.END_WINDOW;\n    }\n  }\n}\n\n\n\n\nUse cases\n\n\nA control tuple may be used in an application to trigger some sort of action in a downstream operator. For example, the source operator might want to notify the last operator that it has emitted all the data in a file and that the file has now ended. Let's call this an \nEnd-Of-File\n control tuple. Once the last operator gets the \nEnd-Of-File\n tuple, it would, say, close the destination file it was writing and create a new file.\n\n\nMore use cases which were discussed during the requirements of this feature are as follows:\n\n\n\n\nBatch support\n - We need to tell all operators of the physical DAG when a\nbatch starts and ends, so the operators can do whatever is needed upon\nthe start or the end of a batch.\n\n\nWatermark\n - To support the concepts of event time windowing, the\nwatermark control tuple is needed to identify late windows.\n\n\nChanging operator properties\n - We do have the support of changing\noperator properties on the fly, but with a custom control tuple, the\ncommand to change operator properties can be window aligned for all\npartitions and also across the DAG. In other words, the properties of \nall\n physical partitions can be aligned to a particular window. In case the behavior of the application needs to change, we may also be able to change properties of multiple logical operators aligned to a particular window.\n\n\nRecording tuples\n - Like changing operator properties, we do have this\nsupport now but only at the individual physical operator level, and without\ncontrol of which window to record tuples for. With a custom control tuple,\nbecause a control tuple must belong to a window, all operators in the DAG\ncan start (and stop) recording for the same windows.\n\n\n\n\nUsage\n\n\nGenerating a Control Tuple\n\n\nThere is no restriction on which operator in the DAG can or can not generate a control tuple. The operator which needs to generate a control tuple should declare a port whose type is \nControlAwareDefaultOutputPort\n; the user could simply call the \nemitControl(ControlTuple t)\n method on this port.\n\n\nExample: In the code snippet below, the \nGenerator\n operator declares a \nControlAwareDefaultOutputPort\n called \noutput\n which can emit a data tuple as well as a control tuple.\n\n\npublic class Generator extends BaseOperator implements InputOperator\n{\n  private long data;\n  private long count;\n\n  public final transient ControlAwareDefaultOutputPort\nDouble\n output =\n      new ControlAwareDefaultOutputPort\n();\n\n  @Override\n  public void emitTuples()\n  {\n    // Can emit a data tuple using output.emit()\n    output.emit(data++);\n    count++;\n  }\n\n  @Override\n  public void endWindow()\n  {\n    // Can also emit a control tuple using output.emitControl()\n    output.emitControl(new TestControlTuple(count, immediate));\n  }\n}\n\n\n\n\nNote\n - User defined control tuples and control aware ports can only be used in operators which use the apex-core dependency which has control tuple support, viz. 3.6.0 or above. Previous versions of apex-core would not be able to support an application which uses user defined control tuples or control aware ports and would crash at launch time.\n\n\nReceiving a Control Tuple\n\n\nAny downstream operator which wants to receive a user defined control tuple, should declare an input port which is \nControl Aware\n. A \nControlAwareDefaultInputPort\n would have the necessary capability to process a control tuple in addition to a regular data tuple.\n\n\nExample: Below code snippet illustrates the use of \nprocessControl\n method of \nControlAwareDefaultInputPort\n to receive / handle user defined control tuples.\n\n\npublic final transient ControlAwareDefaultInputPort\nDouble\n input =\n    new ControlAwareDefaultInputPort\nDouble\n()\n{\n  // Process a data tuple\n  @Override\n  public void process(Double tuple)\n  {\n    output.emit(tuple);\n  }\n\n  // Process a control tuple\n  @Override\n  public boolean processControl(ControlTuple userControlTuple)\n  {\n    // process control tuple here\n    return false;\n    // indicates whether or not the engine\n    // should propagate the tuple automatically to downstream operators\n    // Discussed in later sections\n  }\n};\n\n\n\n\n\nNote that the pre-defined control tuples like \nBEGIN_WINDOW\n and \nEND_WINDOW\n would not be handled by the \nprocessControl()\n method since these used only by the engine and are not meant to be delivered to user logic in operators. Custom control tuples on the other hand are generated by the operators and need to be delivered to downstream operators.\n\n\nReturn value of \nprocessControl\n\n\nFollowing are the semantics:\n\n\n\n\ntrue - Operator would handle propagation explicitly\n\n\nfalse - Operator would not handle propagation. Engine will automatically forward.\n\n\n\n\nSee \nPropagation of Control Tuples\n for more details\n\n\nSerialization requirements\n\n\nA control tuple generated by some operator of the application needs to traverse the same path as that traversed by other data tuples transmitted by the application. For this reason, similar to the other data tuples, the control tuple needs to be Kryo serializable since the default serializer used by the platform is Kryo.\n\n\nPropagation of Control Tuples\n\n\nA control tuple emitted by an operator can be propagated downstream automatically. This is in line with the automatic propagation of other pre-defined control tuples in the engine. However, some use cases require that the control tuple need not be propagated further in the DAG. We support this behavior for user defined control tuples.\n\n\nOnce the control tuple is processed in the \nprocessControl\n method, a return value is expected by the engine. This return value indicates whether or not the operator wishes to handle the propagation of the control tuple or let the engine proceed with the default auto-propagation of the control tuple.\n\n\nThe \nprocessControl\n method of the \nControlAwareDefaultInputPort\n returns a boolean return value.\n\n\n@Override\npublic boolean processControl(ControlTuple userControlTuple)\n{\n  // process userControlTuple here\n  // return true if operator wants to propagate explicitly or block propagation\n  // return false if operator wants engine to propagate automatically\n}\n\n\n\n\nNon - \nControl Aware\n ports\n\n\nFor operators without \nControl Aware\n ports, the platform will forward the control tuples to the downstream operators automatically. The application writer / user does not have to worry about handling a Control tuple which is generated upstream. Only operators with \nControl Aware\n ports would be delivered the control tuple via the \nprocessControl\n method.\nThis also allows the existing operators to be backward compatible.\n\n\nDelivery Semantics\n\n\nDelivery mechanism refer to the time wrt. the processing window when a control tuple is delivered to the operator. An operator has various call backs like \nsetup\n, \nbeginWindow\n, \nendWindow\n, etc.  \n\n\nDeliveryType IMMEDIATE\n\n\nAs the name implies, the control tuple is immediately delivered to the next  downstream operator (if the operator is control aware), else it is forwarded to the next downstream operator.\n\n\n\n\n\n\nCase: Downstream is partitioned\n\nWhen the downstream is partitioned, the control tuple with \nIMMEDIATE\n delivery type would go to all the downstream partitions. This holds, irrespective of whether or not the control tuple was generated by the immediately upstream operator or even further upstream.\n\n\n\n\n\n\nCase: Upstream is partitioned\n\nWhen the upstream is partitioned and the control tuple is generated in any subset of the partitions the downstream operator would receive the control tuple immediately and would not wait till the end of the current window. In case the source for the control tuple was a single source further upstream and multiple copies were generated by the intermediate partitions, the duplicate copies of the control tuple would be filtered out at the downstream operator. Thus only unique control tuples are delivered to the downstream operator. Further, in case of \nIMMEDIATE\n delivery, the first instance of the control tuple is delivered to the operator and the duplicates filtered out.\n\n\n\n\n\n\nDeliveryType END_WINDOW\n\n\nThis delivery type only delivers the control tuple to the operator after all data tuples have been delivered to the operator. In the operator lifecycle, this would mean that the control tuples would be delivered just before the \nendWindow\n call.\n\n\n\n\n\n\nCase: Downstream is partitioned\n\n  When the downstream is partitioned, the control tuple emitted by the upstream would be broadcast to downstream operators and buffered in the downstream partitions until the end of the window and is delivered to the operator just before the \nendWindow\n call.\n\n\n\n\n\n\nCase: Upstream is partitioned\n\n  If the control tuples are generated in any subset of the partitions, then each control tuple is unique and are delivered to the downstream operator before the \nendWindow\n call. However, if the source for the control tuple is a source further upstream, then the downstream operator would filter out duplicates as and when each control tuple arrive at the operator, and finally all unique control tuples are delivered to the operator just before the \nendWindow\n call.\n\n\n\n\n\n\nAssumptions\n\n\nAll the user defined control tuples used in the application are cached in the memory of the operator for the duration of a window. For this reason, it is imperative that the size as well as the number of control tuples emitted within a window is small as compared to the number of data tuples.\n\n\nJIRA\n\n\n\n\nAPEXCORE-579\n points to the top level JIRA issue for control tuple support.", 
            "title": "Custom Control Tuples"
        }, 
        {
            "location": "/control_tuples/#user-defined-control-tuples", 
            "text": "", 
            "title": "User Defined Control Tuples"
        }, 
        {
            "location": "/control_tuples/#introduction", 
            "text": "Custom control tuple support in Apache Apex gives the user the capability to insert user defined control tuples in the data flow. For analogy, the engine already supports a few pre-defined control tuples like BEGIN_WINDOW, END_WINDOW, etc. Until now, we did not have the support for applications to insert their own control tuples.", 
            "title": "Introduction"
        }, 
        {
            "location": "/control_tuples/#terminology", 
            "text": "All discussion in this document is related to Control Tuples generated by user defined logic. The document may refer to these tuples as  Control Tuples ,  User Defined Control Tuples  or  Custom Control Tuples  interchangeably.", 
            "title": "Terminology"
        }, 
        {
            "location": "/control_tuples/#definition", 
            "text": "A user defined control tuple could be any user defined object which implements a ControlTuple interface.  See  Delivery Semantics  for details on DeliveryType  public interface ControlTuple\n{\n  DeliveryType getDeliveryType();\n\n  enum DeliveryType\n  {\n    IMMEDIATE,\n    END_WINDOW\n  }\n}  Example user defined control tuple:  public class TestControlTuple implements ControlTuple\n{\n  public long data;\n  public boolean immediate;\n\n  // For Kryo\n  public TestControlTuple()\n  {\n    data = 0;\n  }\n\n  // Constructor\n  public TestControlTuple(long data, boolean immediate)\n  {\n    this.data = data;\n    this.immediate = immediate;\n  }\n\n  @Override\n  public DeliveryType getDeliveryType()\n  {\n    if (immediate) {\n      return DeliveryType.IMMEDIATE;\n    } else {\n      return DeliveryType.END_WINDOW;\n    }\n  }\n}", 
            "title": "Definition"
        }, 
        {
            "location": "/control_tuples/#use-cases", 
            "text": "A control tuple may be used in an application to trigger some sort of action in a downstream operator. For example, the source operator might want to notify the last operator that it has emitted all the data in a file and that the file has now ended. Let's call this an  End-Of-File  control tuple. Once the last operator gets the  End-Of-File  tuple, it would, say, close the destination file it was writing and create a new file.  More use cases which were discussed during the requirements of this feature are as follows:   Batch support  - We need to tell all operators of the physical DAG when a\nbatch starts and ends, so the operators can do whatever is needed upon\nthe start or the end of a batch.  Watermark  - To support the concepts of event time windowing, the\nwatermark control tuple is needed to identify late windows.  Changing operator properties  - We do have the support of changing\noperator properties on the fly, but with a custom control tuple, the\ncommand to change operator properties can be window aligned for all\npartitions and also across the DAG. In other words, the properties of  all  physical partitions can be aligned to a particular window. In case the behavior of the application needs to change, we may also be able to change properties of multiple logical operators aligned to a particular window.  Recording tuples  - Like changing operator properties, we do have this\nsupport now but only at the individual physical operator level, and without\ncontrol of which window to record tuples for. With a custom control tuple,\nbecause a control tuple must belong to a window, all operators in the DAG\ncan start (and stop) recording for the same windows.", 
            "title": "Use cases"
        }, 
        {
            "location": "/control_tuples/#usage", 
            "text": "", 
            "title": "Usage"
        }, 
        {
            "location": "/control_tuples/#generating-a-control-tuple", 
            "text": "There is no restriction on which operator in the DAG can or can not generate a control tuple. The operator which needs to generate a control tuple should declare a port whose type is  ControlAwareDefaultOutputPort ; the user could simply call the  emitControl(ControlTuple t)  method on this port.  Example: In the code snippet below, the  Generator  operator declares a  ControlAwareDefaultOutputPort  called  output  which can emit a data tuple as well as a control tuple.  public class Generator extends BaseOperator implements InputOperator\n{\n  private long data;\n  private long count;\n\n  public final transient ControlAwareDefaultOutputPort Double  output =\n      new ControlAwareDefaultOutputPort ();\n\n  @Override\n  public void emitTuples()\n  {\n    // Can emit a data tuple using output.emit()\n    output.emit(data++);\n    count++;\n  }\n\n  @Override\n  public void endWindow()\n  {\n    // Can also emit a control tuple using output.emitControl()\n    output.emitControl(new TestControlTuple(count, immediate));\n  }\n}  Note  - User defined control tuples and control aware ports can only be used in operators which use the apex-core dependency which has control tuple support, viz. 3.6.0 or above. Previous versions of apex-core would not be able to support an application which uses user defined control tuples or control aware ports and would crash at launch time.", 
            "title": "Generating a Control Tuple"
        }, 
        {
            "location": "/control_tuples/#receiving-a-control-tuple", 
            "text": "Any downstream operator which wants to receive a user defined control tuple, should declare an input port which is  Control Aware . A  ControlAwareDefaultInputPort  would have the necessary capability to process a control tuple in addition to a regular data tuple.  Example: Below code snippet illustrates the use of  processControl  method of  ControlAwareDefaultInputPort  to receive / handle user defined control tuples.  public final transient ControlAwareDefaultInputPort Double  input =\n    new ControlAwareDefaultInputPort Double ()\n{\n  // Process a data tuple\n  @Override\n  public void process(Double tuple)\n  {\n    output.emit(tuple);\n  }\n\n  // Process a control tuple\n  @Override\n  public boolean processControl(ControlTuple userControlTuple)\n  {\n    // process control tuple here\n    return false;\n    // indicates whether or not the engine\n    // should propagate the tuple automatically to downstream operators\n    // Discussed in later sections\n  }\n};  Note that the pre-defined control tuples like  BEGIN_WINDOW  and  END_WINDOW  would not be handled by the  processControl()  method since these used only by the engine and are not meant to be delivered to user logic in operators. Custom control tuples on the other hand are generated by the operators and need to be delivered to downstream operators.", 
            "title": "Receiving a Control Tuple"
        }, 
        {
            "location": "/control_tuples/#return-value-of-processcontrol", 
            "text": "Following are the semantics:   true - Operator would handle propagation explicitly  false - Operator would not handle propagation. Engine will automatically forward.   See  Propagation of Control Tuples  for more details", 
            "title": "Return value of processControl"
        }, 
        {
            "location": "/control_tuples/#serialization-requirements", 
            "text": "A control tuple generated by some operator of the application needs to traverse the same path as that traversed by other data tuples transmitted by the application. For this reason, similar to the other data tuples, the control tuple needs to be Kryo serializable since the default serializer used by the platform is Kryo.", 
            "title": "Serialization requirements"
        }, 
        {
            "location": "/control_tuples/#propagation-of-control-tuples", 
            "text": "A control tuple emitted by an operator can be propagated downstream automatically. This is in line with the automatic propagation of other pre-defined control tuples in the engine. However, some use cases require that the control tuple need not be propagated further in the DAG. We support this behavior for user defined control tuples.  Once the control tuple is processed in the  processControl  method, a return value is expected by the engine. This return value indicates whether or not the operator wishes to handle the propagation of the control tuple or let the engine proceed with the default auto-propagation of the control tuple.  The  processControl  method of the  ControlAwareDefaultInputPort  returns a boolean return value.  @Override\npublic boolean processControl(ControlTuple userControlTuple)\n{\n  // process userControlTuple here\n  // return true if operator wants to propagate explicitly or block propagation\n  // return false if operator wants engine to propagate automatically\n}", 
            "title": "Propagation of Control Tuples"
        }, 
        {
            "location": "/control_tuples/#non-control-aware-ports", 
            "text": "For operators without  Control Aware  ports, the platform will forward the control tuples to the downstream operators automatically. The application writer / user does not have to worry about handling a Control tuple which is generated upstream. Only operators with  Control Aware  ports would be delivered the control tuple via the  processControl  method.\nThis also allows the existing operators to be backward compatible.", 
            "title": "Non - Control Aware ports"
        }, 
        {
            "location": "/control_tuples/#delivery-semantics", 
            "text": "Delivery mechanism refer to the time wrt. the processing window when a control tuple is delivered to the operator. An operator has various call backs like  setup ,  beginWindow ,  endWindow , etc.", 
            "title": "Delivery Semantics"
        }, 
        {
            "location": "/control_tuples/#deliverytype-immediate", 
            "text": "As the name implies, the control tuple is immediately delivered to the next  downstream operator (if the operator is control aware), else it is forwarded to the next downstream operator.    Case: Downstream is partitioned \nWhen the downstream is partitioned, the control tuple with  IMMEDIATE  delivery type would go to all the downstream partitions. This holds, irrespective of whether or not the control tuple was generated by the immediately upstream operator or even further upstream.    Case: Upstream is partitioned \nWhen the upstream is partitioned and the control tuple is generated in any subset of the partitions the downstream operator would receive the control tuple immediately and would not wait till the end of the current window. In case the source for the control tuple was a single source further upstream and multiple copies were generated by the intermediate partitions, the duplicate copies of the control tuple would be filtered out at the downstream operator. Thus only unique control tuples are delivered to the downstream operator. Further, in case of  IMMEDIATE  delivery, the first instance of the control tuple is delivered to the operator and the duplicates filtered out.", 
            "title": "DeliveryType IMMEDIATE"
        }, 
        {
            "location": "/control_tuples/#deliverytype-end_window", 
            "text": "This delivery type only delivers the control tuple to the operator after all data tuples have been delivered to the operator. In the operator lifecycle, this would mean that the control tuples would be delivered just before the  endWindow  call.    Case: Downstream is partitioned \n  When the downstream is partitioned, the control tuple emitted by the upstream would be broadcast to downstream operators and buffered in the downstream partitions until the end of the window and is delivered to the operator just before the  endWindow  call.    Case: Upstream is partitioned \n  If the control tuples are generated in any subset of the partitions, then each control tuple is unique and are delivered to the downstream operator before the  endWindow  call. However, if the source for the control tuple is a source further upstream, then the downstream operator would filter out duplicates as and when each control tuple arrive at the operator, and finally all unique control tuples are delivered to the operator just before the  endWindow  call.", 
            "title": "DeliveryType END_WINDOW"
        }, 
        {
            "location": "/control_tuples/#assumptions", 
            "text": "All the user defined control tuples used in the application are cached in the memory of the operator for the duration of a window. For this reason, it is imperative that the size as well as the number of control tuples emitted within a window is small as compared to the number of data tuples.", 
            "title": "Assumptions"
        }, 
        {
            "location": "/control_tuples/#jira", 
            "text": "APEXCORE-579  points to the top level JIRA issue for control tuple support.", 
            "title": "JIRA"
        }, 
        {
            "location": "/development_best_practices/", 
            "text": "Development Best Practices\n\n\nThis document describes the best practices to follow when developing operators and other application components such as partitoners, stream codecs etc on the Apache Apex platform.\n\n\nOperators\n\n\nThese are general guidelines for all operators that are covered in the current section. The subsequent sections talk about special considerations for input and output operators.\n\n\n\n\nWhen writing a new operator to be used in an application, consider breaking it down into\n\n\nAn abstract operator that encompasses the core functionality but leaves application specific schemas and logic to the implementation.\n\n\nAn optional concrete operator also in the library that extends the abstract operator and provides commonly used schema types such as strings, byte[] or POJOs.\n\n\n\n\n\n\nFollow these conventions for the life cycle methods:\n\n\nDo one time initialization of entities that apply for the entire lifetime of the operator in the \nsetup\n method, e.g., factory initializations. Initializations in \nsetup\n are done in the container where the operator is deployed. Allocating memory for fields in the constructor is not efficient as it would lead to extra garbage in memory for the following reason. The operator is instantiated on the client from where the application is launched, serialized and started one of the Hadoop nodes in a container. So the constructor is first called on the client and if it were to initialize any of the fields, that state would be saved during serialization. In the Hadoop container the operator is deserialized and started. This would invoke the constructor again, which will initialize the fields but their state will get overwritten by the serialized state and the initial values would become garbage in memory.\n\n\nDo one time initialization for live entities in \nactivate\n method, e.g., opening connections to a database server or starting a thread for asynchronous operations. The \nactivate\n method is called right before processing starts so it is a better place for these initializations than at \nsetup\n which can lead to a delay before processing data from the live entity.  \n\n\nPerform periodic tasks based on processing time in application window boundaries.\n\n\nPerform initializations needed for each application window in \nbeginWindow\n.\n\n\nPerform aggregations needed for each application window  in \nendWindow\n.\n\n\nTeardown of live entities (inverse of tasks performed during activate) should be in the \ndeactivate\n method.\n\n\nTeardown of lifetime entities (those initialized in setup method) should happen in the \nteardown\n method.\n\n\nIf the operator implementation is not finalized mark it with the \n@Evolving\n annotation.\n\n\n\n\n\n\nIf the operator needs to perform operations based on event time of the individual tuples and not the processing time, extend and use the \nWindowedOperator\n. Refer to documentation of that operator for details on how to use it.\n\n\nIf an operator needs to do some work when it is not receiving any input, it should implement \nIdleTimeHandler\n interface. This interface contains \nhandleIdleTime\n method which will be called whenever the platform isn\u2019t doing anything else and the operator can do the work in this method. If for any reason the operator does not have any work to do when this method is called, it should sleep for a small amount of time such as that specified by the \nSPIN_MILLIS\n attribute so that it does not cause a busy wait when called repeatedly by the platform. Also, the method should not block and return in a reasonable amount of time that is less than the streaming window size (which is 500ms by default).\n\n\nOften operators have customizable parameters such as information about locations of external systems or parameters that modify the behavior of the operator. Users should be able to specify these easily without having to change source code. This can be done by making them properties of the operator because they can then be initialized from external properties files.\n\n\nWhere possible default values should be provided for the properties in the source code.\n\n\nValidation rules should be specified for the properties using javax constraint validations that check whether the values specified for the properties are in the correct format, range or other operator requirements. Required properties should have at least a \n@NotNull\n validation specifying that they have to be specified by the user.\n\n\n\n\n\n\n\n\nCheckpointing\n\n\nCheckpointing is a process of snapshotting the state of an operator and saving it so that in case of failure the state can be used to restore the operator to a prior state and continue processing. It is automatically performed by the platform at a configurable interval. All operators in the application are checkpointed in a distributed fashion, thus allowing the entire state of the application to be saved and available for recovery if needed. Here are some things to remember when it comes to checkpointing:\n\n\n\n\nThe process of checkpointing involves snapshotting the state by serializing the operator and saving it to a store. This is done using a \nStorageAgent\n. By default a \nStorageAgent\n is already provided by the platform and it is called \nAsyncFSStorageAgent\n. It serializes the operator using Kryo and saves the serialized state asynchronously to a filesystem such as HDFS. There are other implementations of \nStorageAgent\n available such as \nGeodeKeyValueStorageAgent\n that stores the serialized state in Geode which is an in-memory replicated data grid.\n\n\nAll variables in the operator marked neither transient nor final are saved so any variables in the operator that are not part of the state should be marked transient. Specifically any variables like connection objects, i/o streams, ports are transient, because they need to be setup again on failure recovery.\n\n\nIf the operator does not keep any state between windows, mark it with the \n@Stateless\n annotation. This results in efficiencies during checkpointing and recovery. The operator will not be checkpointed and is always restored to the initial state\n\n\nThe checkpoint interval can be set using the \nCHECKPOINT_WINDOW_COUNT\n attribute which specifies the interval in terms of number of streaming windows.\n\n\nIf the correct functioning of the operator requires the \nendWindow\n method be called before checkpointing can happen, then the checkpoint interval should align with application window interval i.e., it should be a multiple of application window interval. In this case the operator should be marked with \nOperatorAnnotation\n and \ncheckpointableWithinAppWindow\n set to false. If the window intervals are configured by the user and they don\u2019t align, it will result in a DAG validation error and application won\u2019t launch.\n\n\nIn some cases the operator state related to a piece of data needs to be purged once that data is no longer required by the application, otherwise the state will continue to build up indefinitely. The platform provides a way to let the operator know about this using a callback listener called \nCheckpointNotificationListener\n. This listener has a callback method called \ncommitted\n, which is called by the platform from time to time with a window id that has been processed successfully by all the operators in the DAG and hence is no longer needed. The operator can delete all the state corresponding to window ids less than or equal to the provided window id.\n\n\nSometimes operators need to perform some tasks just before checkpointing. For example, filesystem operators may want to flush the files just before checkpoint so they can be sure that all pending data is written to disk and no data is lost if there is an operator failure just after the checkpoint and the operator restarts from the checkpoint. To do this the operator would implement the same \nCheckpointNotificationListener\n interface and implement the \nbeforeCheckpoint\n method where it can do these tasks.\n\n\nIf the operator is going to have a large state, checkpointing the entire state each time becomes unviable. Furthermore, the amount of memory needed to hold the state could be larger than the amount of physical memory available. In these cases the operator should checkpoint the state incrementally and also manage the memory for the state more efficiently. The platform provides a utiltiy called \nManagedState\n that uses a combination of in memory and disk cache to efficiently store and retrieve data in a performant, fault tolerant way and also checkpoint it in an incremental fashion. There are operators in the platform that use \nManagedState\n and can be used as a reference on how to use this utility such as Dedup or Join operators.\n\n\n\n\nInput Operators\n\n\nInput operators have additional requirements:\n\n\n\n\nThe \nemitTuples\n method implemented by the operator, is called by the platform, to give the operator an opportunity to emit some data. This method is always called within a window boundary but can be called multiple times within the same window. There are some important guidelines on how to implement this method:\n\n\nThis should not be a blocking method and should return in a reasonable time that is less than the streaming window size (which is 500ms by default). This also applies to other callback methods called by the platform such as \nbeginWindow\n, \nendWindow\n etc., but is more important here since this method will be called continuously by the platform.\n\n\nIf the operator needs to interact with external systems to obtain data and this can potentially take a long time, then this should be performed asynchronously in a different thread. Refer to the threading section below for the guidelines when using threading.\n\n\nIn each invocation, the method can emit any number of data tuples.\n\n\n\n\n\n\n\n\nIdempotence\n\n\nMany applications write data to external systems using output operators. To ensure that data is present exactly once in the external system even in a failure recovery scenario, the output operators expect the replayed windows during recovery contain the same data as before the failure. This is called idempotency. Since operators within the DAG are merely responding to input data provided to them by the upstream operators and the input operator has no upstream operator, the responsibility of idempotent replay falls on the input operators.\n\n\n\n\nFor idempotent replay of data, the operator needs to store some meta-information for every window that would allow it to identify what data was sent in that window. This is called the idempotent state.\n\n\nIf the external source of the input operator allows replayability, this could be information such as offset of last piece of data in the window, an identifier of the last piece of data itself or number of data tuples sent.\n\n\nHowever if the external source does not allow replayability from an operator specified point, then the entire data sent within the window may need to be persisted by the operator.\n\n\n\n\n\n\nThe platform provides a utility called \nWindowDataManager\n to allow operators to save and retrieve idempotent state every window. Operators should use this to implement idempotency.\n\n\n\n\nOutput Operators\n\n\nOutput operators typically connect to external storage systems such as filesystems, databases or key value stores to store data.\n\n\n\n\nIn some situations, the external systems may not be functioning in a reliable fashion. They may be having prolonged outages or performance problems. If the operator is being designed to work in such environments, it needs to be able to to handle these problems gracefully and not block the DAG or fail. In these scenarios the operator should cache the data into a local store such as HDFS and interact with external systems in a separate thread so as to not have problems in the operator lifecycle thread. This pattern is called the \nReconciler\n pattern and there are operators that implement this pattern available in the library for reference.\n\n\n\n\nEnd-to-End Exactly Once\n\n\nWhen output operators store data in external systems, it is important that they do not lose data or write duplicate data when there is a failure event and the DAG recovers from that failure. In failure recovery, the windows from the previous checkpoint are replayed and the operator receives this data again. The operator should ensure that it does not write this data again. Operator developers should figure out how to do this specifically for the operators they are developing depending on the logic of the operators. Below are examples of how a couple of existing output operators do this for reference.\n\n\n\n\nFile output operator that writes data to files keeps track of the file lengths in the state. These lengths are checkpointed and restored on failure recovery. On restart, the operator truncates the file to the length equal to the length in the recovered state. This makes the data in the file same as it was at the time of checkpoint before the failure. The operator now writes the replayed data from the checkpoint in regular fashion as any other data. This ensures no data is lost or duplicated in the file.\n\n\nThe JDBC output operator that writes data to a database table writes the data in a window in a single transaction. It also writes the current window id into a meta table along with the data as part of the same transaction. It commits the transaction at the end of the window. When there is an operator failure before the final commit, the state of the database is that it contains the data from the previous fully processed window and its window id since the current window transaction isn\u2019t yet committed. On recovery, the operator reads this window id back from the meta table. It ignores all the replayed windows whose window id is less than or equal to the recovered window id and thus ensures that it does not duplicate data already present in the database. It starts writing data normally again when window id of data becomes greater than recovered window thus ensuring no data is lost.\n\n\n\n\nPartitioning\n\n\nPartitioning allows an operation to be scaled to handle more pieces of data than before but with a similar SLA. This is done by creating multiple instances of an operator and distributing the data among them. Input operators can also be partitioned to stream more pieces of data into the application. The platform provides a lot of flexibility and options for partitioning. Partitioning can happen once at startup or can be dynamically changed anytime while the application is running, and it can be done in a stateless or stateful way by distributing state from the old partitions to new partitions.\n\n\nIn the platform, the responsibility for partitioning is shared among different entities. These are:\n\n\n\n\nA \npartitioner\n that specifies \nhow\n to partition the operator, specifically it takes an old set of partitions and creates a new set of partitions. At the start of the application the old set has one partition and the partitioner can return more than one partitions to start the application with multiple partitions. The partitioner can have any custom JAVA logic to determine the number of new partitions, set their initial state as a brand new state or derive it from the state of the old partitions. It also specifies how the data gets distributed among the new partitions. The new set doesn't have to contain only new partitions, it can carry over some old partitions if desired.\n\n\nAn optional \nstatistics (stats) listener\n that specifies \nwhen\n to partition. The reason it is optional is that it is needed only when dynamic partitioning is needed. With the stats listener, the stats can be used to determine when to partition.\n\n\nIn some cases the \noperator\n itself should be aware of partitioning and would need to provide supporting code.\n\n\nIn case of input operators each partition should have a property or a set of properties that allow it to distinguish itself from the other partitions and fetch unique data.\n\n\n\n\n\n\nWhen an operator that was originally a single instance is split into multiple partitions with each partition working on a subset of data, the results of the partitions may need to be combined together to compute the final result. The combining logic would depend on the logic of the operator. This would be specified by the developer using a \nUnifier\n, which is deployed as another operator by the platform. If no \nUnifier\n is specified, the platform inserts a \ndefault unifier\n that merges the results of the multiple partition streams into a single stream. Each output port can have a different \nUnifier\n and this is specified by returning the corresponding \nUnifier\n in the \ngetUnifier\n method of the output port. The operator developer should provide a custom \nUnifier\n wherever applicable.\n\n\nThe Apex \nengine\n that brings everything together and effects the partitioning.\n\n\n\n\nSince partitioning is critical for scalability of applications, operators must support it. There should be a strong reason for an operator to not support partitioning, such as, the logic performed by the operator not lending itself to parallelism. In order to support partitioning, an operator developer, apart from developing the functionality of the operator, may also need to provide a partitioner, stats listener and supporting code in the operator as described in the steps above. The next sections delve into this. \n\n\nOut of the box partitioning\n\n\nThe platform comes with some built-in partitioning utilities that can be used in certain scenarios.\n\n\n\n\n\n\nStatelessPartitioner\n provides a default partitioner, that can be used for an operator in certain conditions. If the operator satisfies these conditions, the partitioner can be specified for the operator with a simple setting and no other partitioning code is needed. The conditions are:\n\n\n\n\nNo dynamic partitioning is needed, see next point about dynamic partitioning. \n\n\nThere is no distinct initial state for the partitions, i.e., all partitions start with the same initial state submitted during application launch.\n\n\n\n\nTypically input or output operators do not fall into this category, although there are some exceptions. This partitioner is mainly used with operators that are in the middle of the DAG, after the input and before the output operators. When used with non-input operators, only the data for the first declared input port is distributed among the different partitions. All other input ports are treated as broadcast and all partitions receive all the data for that port.\n\n\n\n\n\n\nStatelessThroughputBasedPartitioner\n in Malhar provides a dynamic partitioner based on throughput thresholds. Similarly \nStatelessLatencyBasedPartitioner\n provides a latency based dynamic partitioner in RTS. If these partitioners can be used, then separate partitioning related code is not needed. The conditions under which these can be used are:\n\n\n\n\nThere is no distinct initial state for the partitions.\n\n\nThere is no state being carried over by the operator from one window to the next i.e., operator is stateless.\n\n\n\n\n\n\n\n\nCustom partitioning\n\n\nIn many cases, operators don\u2019t satisfy the above conditions and a built-in partitioner cannot be used. Custom partitioning code needs to be written by the operator developer. Below are guidelines for it.\n\n\n\n\nSince the operator developer is providing a \npartitioner\n for the operator, the partitioning code should be added to the operator itself by making the operator implement the Partitioner interface and implementing the required methods, rather than creating a separate partitioner. The advantage is the user of the operator does not have to explicitly figure out the partitioner and set it for the operator but still has the option to override this built-in partitioner with a different one.\n\n\nThe \npartitioner\n is responsible for setting the initial state of the new partitions, whether it is at the start of the application or when partitioning is happening while the application is running as in the dynamic partitioning case. In the dynamic partitioning scenario, the partitioner needs to take the state from the old partitions and distribute it among the new partitions. It is important to note that apart from the checkpointed state the partitioner also needs to distribute idempotent state.\n\n\nThe \npartitioner\n interface has two methods, \ndefinePartitions\n and \npartitioned\n. The method \ndefinePartitons\n is first called to determine the new partitions, and if enough resources are available on the cluster, the \npartitioned\n method is called passing in the new partitions. This happens both during initial partitioning and dynamic partitioning. If resources are not available, partitioning is abandoned and existing partitions continue to run untouched. This means that any processing intensive operations should be deferred to the \npartitioned\n call instead of doing them in \ndefinePartitions\n, as they may not be needed if there are not enough resources available in the cluster.\n\n\nThe \npartitioner\n, along with creating the new partitions, should also specify how the data gets distributed across the new partitions. It should do this by specifying a mapping called \nPartitionKeys\n for each partition that maps the data to that partition. This mapping needs to be specified for every input port in the operator. If the \npartitioner\n wants to use the standard mapping it can use a utility method called \nDefaultPartition.assignPartitionKeys\n.\n\n\nWhen the partitioner is scaling the operator up to more partitions, try to reuse the existing partitions and create new partitions to augment the current set. The reuse can be achieved by the partitioner returning the current partitions unchanged. This will result in the current partitions continuing to run untouched.\n\n\nIn case of dynamic partitioning, as mentioned earlier, a stats listener is also needed to determine when to re-partition. Like the \nPartitioner\n interface, the operator can also implement the \nStatsListener\n interface to provide a stats listener implementation that will be automatically used.\n\n\nThe \nStatsListener\n has access to all operator statistics to make its decision on partitioning. Apart from the statistics that the platform computes for the operators such as throughput, latency etc, operator developers can include their own business metrics by using the AutoMetric feature.\n\n\nIf the operator is not partitionable, mark it so with \nOperatorAnnotation\n and \npartitionable\n element set to false.\n\n\n\n\nStreamCodecs\n\n\nA \nStreamCodec\n is used in partitioning to distribute the data tuples among the partitions. The \nStreamCodec\n computes an integer hashcode for a data tuple and this is used along with \nPartitionKeys\n mapping to determine which partition or partitions receive the data tuple. If a \nStreamCodec\n is not specified, then a default one is used by the platform which returns the JAVA hashcode of the tuple. \n\n\nStreamCodec\n is also useful in another aspect of the application. It is used to serialize and deserialize the tuple to transfer it between operators. The default \nStreamCodec\n uses Kryo library for serialization. \n\n\nThe following guidelines are useful when considering a custom \nStreamCodec\n\n\n\n\nA custom \nStreamCodec\n is needed if the tuples need to be distributed based on a criteria different from the hashcode of the tuple. If the correct working of an operator depends on the data from the upstream operator being distributed using a custom criteria such as being sticky on a \u201ckey\u201d field within the tuple, then a custom \nStreamCodec\n should be provided by the operator developer. This codec can implement the custom criteria. The operator should also return this custom codec in the \ngetStreamCodec\n method of the input port.\n\n\nWhen implementing a custom \nStreamCodec\n for the purpose of using a different criteria to distribute the tuples, the codec can extend an existing \nStreamCodec\n and implement the hashcode method, so that the codec does not have to worry about the serialization and deserialization functionality. The Apex platform provides two pre-built \nStreamCodec\n implementations for this purpose, one is \nKryoSerializableStreamCodec\n that uses Kryo for serialization and another one \nJavaSerializationStreamCodec\n that uses JAVA serialization.\n\n\nDifferent \nStreamCodec\n implementations can be used for different inputs in a stream with multiple inputs when different criteria of distributing the tuples is desired between the multiple inputs. \n\n\n\n\nThreads\n\n\nThe operator lifecycle methods such as \nsetup\n, \nbeginWindow\n, \nendWindow\n, \nprocess\n in \nInputPorts\n are all called from a single operator lifecycle thread, by the platform, unbeknownst to the user. So the user does not have to worry about dealing with the issues arising from multi-threaded code. Use of separate threads in an operator is discouraged because in most cases the motivation for this is parallelism, but parallelism can already be achieved by using multiple partitions and furthermore mistakes can be made easily when writing multi-threaded code. When dealing with high volume and velocity data, the corner cases with incorrectly written multi-threaded code are encountered more easily and exposed. However, there are times when separate threads are needed, for example, when interacting with external systems the delay in retrieving or sending data can be large at times, blocking the operator and other DAG processing such as committed windows. In these cases the following guidelines must be followed strictly.\n\n\n\n\nThreads should be started in \nactivate\n and stopped in \ndeactivate\n. In \ndeactivate\n the operator should wait till any threads it launched, have finished execution. It can do so by calling \njoin\n on the threads or if using \nExecutorService\n, calling \nawaitTermination\n on the service.\n\n\nThreads should not call any methods on the ports directly as this can cause concurrency exceptions and also result in invalid states.\n\n\nThreads can share state with the lifecycle methods using data structures that are either explicitly protected by synchronization or are inherently thread safe such as thread safe queues.\n\n\nIf this shared state needs to be protected against failure then it needs to be persisted during checkpoint. To have a consistent checkpoint, the state should not be modified by the thread when it is being serialized and saved by the operator lifecycle thread during checkpoint. Since the checkpoint process happens outside the window boundary the thread should be quiesced between \nendWindow\n and \nbeginWindow\n or more efficiently between pre-checkpoint and checkpointed callbacks.", 
            "title": "Best Practices"
        }, 
        {
            "location": "/development_best_practices/#development-best-practices", 
            "text": "This document describes the best practices to follow when developing operators and other application components such as partitoners, stream codecs etc on the Apache Apex platform.", 
            "title": "Development Best Practices"
        }, 
        {
            "location": "/development_best_practices/#operators", 
            "text": "These are general guidelines for all operators that are covered in the current section. The subsequent sections talk about special considerations for input and output operators.   When writing a new operator to be used in an application, consider breaking it down into  An abstract operator that encompasses the core functionality but leaves application specific schemas and logic to the implementation.  An optional concrete operator also in the library that extends the abstract operator and provides commonly used schema types such as strings, byte[] or POJOs.    Follow these conventions for the life cycle methods:  Do one time initialization of entities that apply for the entire lifetime of the operator in the  setup  method, e.g., factory initializations. Initializations in  setup  are done in the container where the operator is deployed. Allocating memory for fields in the constructor is not efficient as it would lead to extra garbage in memory for the following reason. The operator is instantiated on the client from where the application is launched, serialized and started one of the Hadoop nodes in a container. So the constructor is first called on the client and if it were to initialize any of the fields, that state would be saved during serialization. In the Hadoop container the operator is deserialized and started. This would invoke the constructor again, which will initialize the fields but their state will get overwritten by the serialized state and the initial values would become garbage in memory.  Do one time initialization for live entities in  activate  method, e.g., opening connections to a database server or starting a thread for asynchronous operations. The  activate  method is called right before processing starts so it is a better place for these initializations than at  setup  which can lead to a delay before processing data from the live entity.    Perform periodic tasks based on processing time in application window boundaries.  Perform initializations needed for each application window in  beginWindow .  Perform aggregations needed for each application window  in  endWindow .  Teardown of live entities (inverse of tasks performed during activate) should be in the  deactivate  method.  Teardown of lifetime entities (those initialized in setup method) should happen in the  teardown  method.  If the operator implementation is not finalized mark it with the  @Evolving  annotation.    If the operator needs to perform operations based on event time of the individual tuples and not the processing time, extend and use the  WindowedOperator . Refer to documentation of that operator for details on how to use it.  If an operator needs to do some work when it is not receiving any input, it should implement  IdleTimeHandler  interface. This interface contains  handleIdleTime  method which will be called whenever the platform isn\u2019t doing anything else and the operator can do the work in this method. If for any reason the operator does not have any work to do when this method is called, it should sleep for a small amount of time such as that specified by the  SPIN_MILLIS  attribute so that it does not cause a busy wait when called repeatedly by the platform. Also, the method should not block and return in a reasonable amount of time that is less than the streaming window size (which is 500ms by default).  Often operators have customizable parameters such as information about locations of external systems or parameters that modify the behavior of the operator. Users should be able to specify these easily without having to change source code. This can be done by making them properties of the operator because they can then be initialized from external properties files.  Where possible default values should be provided for the properties in the source code.  Validation rules should be specified for the properties using javax constraint validations that check whether the values specified for the properties are in the correct format, range or other operator requirements. Required properties should have at least a  @NotNull  validation specifying that they have to be specified by the user.", 
            "title": "Operators"
        }, 
        {
            "location": "/development_best_practices/#checkpointing", 
            "text": "Checkpointing is a process of snapshotting the state of an operator and saving it so that in case of failure the state can be used to restore the operator to a prior state and continue processing. It is automatically performed by the platform at a configurable interval. All operators in the application are checkpointed in a distributed fashion, thus allowing the entire state of the application to be saved and available for recovery if needed. Here are some things to remember when it comes to checkpointing:   The process of checkpointing involves snapshotting the state by serializing the operator and saving it to a store. This is done using a  StorageAgent . By default a  StorageAgent  is already provided by the platform and it is called  AsyncFSStorageAgent . It serializes the operator using Kryo and saves the serialized state asynchronously to a filesystem such as HDFS. There are other implementations of  StorageAgent  available such as  GeodeKeyValueStorageAgent  that stores the serialized state in Geode which is an in-memory replicated data grid.  All variables in the operator marked neither transient nor final are saved so any variables in the operator that are not part of the state should be marked transient. Specifically any variables like connection objects, i/o streams, ports are transient, because they need to be setup again on failure recovery.  If the operator does not keep any state between windows, mark it with the  @Stateless  annotation. This results in efficiencies during checkpointing and recovery. The operator will not be checkpointed and is always restored to the initial state  The checkpoint interval can be set using the  CHECKPOINT_WINDOW_COUNT  attribute which specifies the interval in terms of number of streaming windows.  If the correct functioning of the operator requires the  endWindow  method be called before checkpointing can happen, then the checkpoint interval should align with application window interval i.e., it should be a multiple of application window interval. In this case the operator should be marked with  OperatorAnnotation  and  checkpointableWithinAppWindow  set to false. If the window intervals are configured by the user and they don\u2019t align, it will result in a DAG validation error and application won\u2019t launch.  In some cases the operator state related to a piece of data needs to be purged once that data is no longer required by the application, otherwise the state will continue to build up indefinitely. The platform provides a way to let the operator know about this using a callback listener called  CheckpointNotificationListener . This listener has a callback method called  committed , which is called by the platform from time to time with a window id that has been processed successfully by all the operators in the DAG and hence is no longer needed. The operator can delete all the state corresponding to window ids less than or equal to the provided window id.  Sometimes operators need to perform some tasks just before checkpointing. For example, filesystem operators may want to flush the files just before checkpoint so they can be sure that all pending data is written to disk and no data is lost if there is an operator failure just after the checkpoint and the operator restarts from the checkpoint. To do this the operator would implement the same  CheckpointNotificationListener  interface and implement the  beforeCheckpoint  method where it can do these tasks.  If the operator is going to have a large state, checkpointing the entire state each time becomes unviable. Furthermore, the amount of memory needed to hold the state could be larger than the amount of physical memory available. In these cases the operator should checkpoint the state incrementally and also manage the memory for the state more efficiently. The platform provides a utiltiy called  ManagedState  that uses a combination of in memory and disk cache to efficiently store and retrieve data in a performant, fault tolerant way and also checkpoint it in an incremental fashion. There are operators in the platform that use  ManagedState  and can be used as a reference on how to use this utility such as Dedup or Join operators.", 
            "title": "Checkpointing"
        }, 
        {
            "location": "/development_best_practices/#input-operators", 
            "text": "Input operators have additional requirements:   The  emitTuples  method implemented by the operator, is called by the platform, to give the operator an opportunity to emit some data. This method is always called within a window boundary but can be called multiple times within the same window. There are some important guidelines on how to implement this method:  This should not be a blocking method and should return in a reasonable time that is less than the streaming window size (which is 500ms by default). This also applies to other callback methods called by the platform such as  beginWindow ,  endWindow  etc., but is more important here since this method will be called continuously by the platform.  If the operator needs to interact with external systems to obtain data and this can potentially take a long time, then this should be performed asynchronously in a different thread. Refer to the threading section below for the guidelines when using threading.  In each invocation, the method can emit any number of data tuples.", 
            "title": "Input Operators"
        }, 
        {
            "location": "/development_best_practices/#idempotence", 
            "text": "Many applications write data to external systems using output operators. To ensure that data is present exactly once in the external system even in a failure recovery scenario, the output operators expect the replayed windows during recovery contain the same data as before the failure. This is called idempotency. Since operators within the DAG are merely responding to input data provided to them by the upstream operators and the input operator has no upstream operator, the responsibility of idempotent replay falls on the input operators.   For idempotent replay of data, the operator needs to store some meta-information for every window that would allow it to identify what data was sent in that window. This is called the idempotent state.  If the external source of the input operator allows replayability, this could be information such as offset of last piece of data in the window, an identifier of the last piece of data itself or number of data tuples sent.  However if the external source does not allow replayability from an operator specified point, then the entire data sent within the window may need to be persisted by the operator.    The platform provides a utility called  WindowDataManager  to allow operators to save and retrieve idempotent state every window. Operators should use this to implement idempotency.", 
            "title": "Idempotence"
        }, 
        {
            "location": "/development_best_practices/#output-operators", 
            "text": "Output operators typically connect to external storage systems such as filesystems, databases or key value stores to store data.   In some situations, the external systems may not be functioning in a reliable fashion. They may be having prolonged outages or performance problems. If the operator is being designed to work in such environments, it needs to be able to to handle these problems gracefully and not block the DAG or fail. In these scenarios the operator should cache the data into a local store such as HDFS and interact with external systems in a separate thread so as to not have problems in the operator lifecycle thread. This pattern is called the  Reconciler  pattern and there are operators that implement this pattern available in the library for reference.", 
            "title": "Output Operators"
        }, 
        {
            "location": "/development_best_practices/#end-to-end-exactly-once", 
            "text": "When output operators store data in external systems, it is important that they do not lose data or write duplicate data when there is a failure event and the DAG recovers from that failure. In failure recovery, the windows from the previous checkpoint are replayed and the operator receives this data again. The operator should ensure that it does not write this data again. Operator developers should figure out how to do this specifically for the operators they are developing depending on the logic of the operators. Below are examples of how a couple of existing output operators do this for reference.   File output operator that writes data to files keeps track of the file lengths in the state. These lengths are checkpointed and restored on failure recovery. On restart, the operator truncates the file to the length equal to the length in the recovered state. This makes the data in the file same as it was at the time of checkpoint before the failure. The operator now writes the replayed data from the checkpoint in regular fashion as any other data. This ensures no data is lost or duplicated in the file.  The JDBC output operator that writes data to a database table writes the data in a window in a single transaction. It also writes the current window id into a meta table along with the data as part of the same transaction. It commits the transaction at the end of the window. When there is an operator failure before the final commit, the state of the database is that it contains the data from the previous fully processed window and its window id since the current window transaction isn\u2019t yet committed. On recovery, the operator reads this window id back from the meta table. It ignores all the replayed windows whose window id is less than or equal to the recovered window id and thus ensures that it does not duplicate data already present in the database. It starts writing data normally again when window id of data becomes greater than recovered window thus ensuring no data is lost.", 
            "title": "End-to-End Exactly Once"
        }, 
        {
            "location": "/development_best_practices/#partitioning", 
            "text": "Partitioning allows an operation to be scaled to handle more pieces of data than before but with a similar SLA. This is done by creating multiple instances of an operator and distributing the data among them. Input operators can also be partitioned to stream more pieces of data into the application. The platform provides a lot of flexibility and options for partitioning. Partitioning can happen once at startup or can be dynamically changed anytime while the application is running, and it can be done in a stateless or stateful way by distributing state from the old partitions to new partitions.  In the platform, the responsibility for partitioning is shared among different entities. These are:   A  partitioner  that specifies  how  to partition the operator, specifically it takes an old set of partitions and creates a new set of partitions. At the start of the application the old set has one partition and the partitioner can return more than one partitions to start the application with multiple partitions. The partitioner can have any custom JAVA logic to determine the number of new partitions, set their initial state as a brand new state or derive it from the state of the old partitions. It also specifies how the data gets distributed among the new partitions. The new set doesn't have to contain only new partitions, it can carry over some old partitions if desired.  An optional  statistics (stats) listener  that specifies  when  to partition. The reason it is optional is that it is needed only when dynamic partitioning is needed. With the stats listener, the stats can be used to determine when to partition.  In some cases the  operator  itself should be aware of partitioning and would need to provide supporting code.  In case of input operators each partition should have a property or a set of properties that allow it to distinguish itself from the other partitions and fetch unique data.    When an operator that was originally a single instance is split into multiple partitions with each partition working on a subset of data, the results of the partitions may need to be combined together to compute the final result. The combining logic would depend on the logic of the operator. This would be specified by the developer using a  Unifier , which is deployed as another operator by the platform. If no  Unifier  is specified, the platform inserts a  default unifier  that merges the results of the multiple partition streams into a single stream. Each output port can have a different  Unifier  and this is specified by returning the corresponding  Unifier  in the  getUnifier  method of the output port. The operator developer should provide a custom  Unifier  wherever applicable.  The Apex  engine  that brings everything together and effects the partitioning.   Since partitioning is critical for scalability of applications, operators must support it. There should be a strong reason for an operator to not support partitioning, such as, the logic performed by the operator not lending itself to parallelism. In order to support partitioning, an operator developer, apart from developing the functionality of the operator, may also need to provide a partitioner, stats listener and supporting code in the operator as described in the steps above. The next sections delve into this.", 
            "title": "Partitioning"
        }, 
        {
            "location": "/development_best_practices/#out-of-the-box-partitioning", 
            "text": "The platform comes with some built-in partitioning utilities that can be used in certain scenarios.    StatelessPartitioner  provides a default partitioner, that can be used for an operator in certain conditions. If the operator satisfies these conditions, the partitioner can be specified for the operator with a simple setting and no other partitioning code is needed. The conditions are:   No dynamic partitioning is needed, see next point about dynamic partitioning.   There is no distinct initial state for the partitions, i.e., all partitions start with the same initial state submitted during application launch.   Typically input or output operators do not fall into this category, although there are some exceptions. This partitioner is mainly used with operators that are in the middle of the DAG, after the input and before the output operators. When used with non-input operators, only the data for the first declared input port is distributed among the different partitions. All other input ports are treated as broadcast and all partitions receive all the data for that port.    StatelessThroughputBasedPartitioner  in Malhar provides a dynamic partitioner based on throughput thresholds. Similarly  StatelessLatencyBasedPartitioner  provides a latency based dynamic partitioner in RTS. If these partitioners can be used, then separate partitioning related code is not needed. The conditions under which these can be used are:   There is no distinct initial state for the partitions.  There is no state being carried over by the operator from one window to the next i.e., operator is stateless.", 
            "title": "Out of the box partitioning"
        }, 
        {
            "location": "/development_best_practices/#custom-partitioning", 
            "text": "In many cases, operators don\u2019t satisfy the above conditions and a built-in partitioner cannot be used. Custom partitioning code needs to be written by the operator developer. Below are guidelines for it.   Since the operator developer is providing a  partitioner  for the operator, the partitioning code should be added to the operator itself by making the operator implement the Partitioner interface and implementing the required methods, rather than creating a separate partitioner. The advantage is the user of the operator does not have to explicitly figure out the partitioner and set it for the operator but still has the option to override this built-in partitioner with a different one.  The  partitioner  is responsible for setting the initial state of the new partitions, whether it is at the start of the application or when partitioning is happening while the application is running as in the dynamic partitioning case. In the dynamic partitioning scenario, the partitioner needs to take the state from the old partitions and distribute it among the new partitions. It is important to note that apart from the checkpointed state the partitioner also needs to distribute idempotent state.  The  partitioner  interface has two methods,  definePartitions  and  partitioned . The method  definePartitons  is first called to determine the new partitions, and if enough resources are available on the cluster, the  partitioned  method is called passing in the new partitions. This happens both during initial partitioning and dynamic partitioning. If resources are not available, partitioning is abandoned and existing partitions continue to run untouched. This means that any processing intensive operations should be deferred to the  partitioned  call instead of doing them in  definePartitions , as they may not be needed if there are not enough resources available in the cluster.  The  partitioner , along with creating the new partitions, should also specify how the data gets distributed across the new partitions. It should do this by specifying a mapping called  PartitionKeys  for each partition that maps the data to that partition. This mapping needs to be specified for every input port in the operator. If the  partitioner  wants to use the standard mapping it can use a utility method called  DefaultPartition.assignPartitionKeys .  When the partitioner is scaling the operator up to more partitions, try to reuse the existing partitions and create new partitions to augment the current set. The reuse can be achieved by the partitioner returning the current partitions unchanged. This will result in the current partitions continuing to run untouched.  In case of dynamic partitioning, as mentioned earlier, a stats listener is also needed to determine when to re-partition. Like the  Partitioner  interface, the operator can also implement the  StatsListener  interface to provide a stats listener implementation that will be automatically used.  The  StatsListener  has access to all operator statistics to make its decision on partitioning. Apart from the statistics that the platform computes for the operators such as throughput, latency etc, operator developers can include their own business metrics by using the AutoMetric feature.  If the operator is not partitionable, mark it so with  OperatorAnnotation  and  partitionable  element set to false.", 
            "title": "Custom partitioning"
        }, 
        {
            "location": "/development_best_practices/#streamcodecs", 
            "text": "A  StreamCodec  is used in partitioning to distribute the data tuples among the partitions. The  StreamCodec  computes an integer hashcode for a data tuple and this is used along with  PartitionKeys  mapping to determine which partition or partitions receive the data tuple. If a  StreamCodec  is not specified, then a default one is used by the platform which returns the JAVA hashcode of the tuple.   StreamCodec  is also useful in another aspect of the application. It is used to serialize and deserialize the tuple to transfer it between operators. The default  StreamCodec  uses Kryo library for serialization.   The following guidelines are useful when considering a custom  StreamCodec   A custom  StreamCodec  is needed if the tuples need to be distributed based on a criteria different from the hashcode of the tuple. If the correct working of an operator depends on the data from the upstream operator being distributed using a custom criteria such as being sticky on a \u201ckey\u201d field within the tuple, then a custom  StreamCodec  should be provided by the operator developer. This codec can implement the custom criteria. The operator should also return this custom codec in the  getStreamCodec  method of the input port.  When implementing a custom  StreamCodec  for the purpose of using a different criteria to distribute the tuples, the codec can extend an existing  StreamCodec  and implement the hashcode method, so that the codec does not have to worry about the serialization and deserialization functionality. The Apex platform provides two pre-built  StreamCodec  implementations for this purpose, one is  KryoSerializableStreamCodec  that uses Kryo for serialization and another one  JavaSerializationStreamCodec  that uses JAVA serialization.  Different  StreamCodec  implementations can be used for different inputs in a stream with multiple inputs when different criteria of distributing the tuples is desired between the multiple inputs.", 
            "title": "StreamCodecs"
        }, 
        {
            "location": "/development_best_practices/#threads", 
            "text": "The operator lifecycle methods such as  setup ,  beginWindow ,  endWindow ,  process  in  InputPorts  are all called from a single operator lifecycle thread, by the platform, unbeknownst to the user. So the user does not have to worry about dealing with the issues arising from multi-threaded code. Use of separate threads in an operator is discouraged because in most cases the motivation for this is parallelism, but parallelism can already be achieved by using multiple partitions and furthermore mistakes can be made easily when writing multi-threaded code. When dealing with high volume and velocity data, the corner cases with incorrectly written multi-threaded code are encountered more easily and exposed. However, there are times when separate threads are needed, for example, when interacting with external systems the delay in retrieving or sending data can be large at times, blocking the operator and other DAG processing such as committed windows. In these cases the following guidelines must be followed strictly.   Threads should be started in  activate  and stopped in  deactivate . In  deactivate  the operator should wait till any threads it launched, have finished execution. It can do so by calling  join  on the threads or if using  ExecutorService , calling  awaitTermination  on the service.  Threads should not call any methods on the ports directly as this can cause concurrency exceptions and also result in invalid states.  Threads can share state with the lifecycle methods using data structures that are either explicitly protected by synchronization or are inherently thread safe such as thread safe queues.  If this shared state needs to be protected against failure then it needs to be persisted during checkpoint. To have a consistent checkpoint, the state should not be modified by the thread when it is being serialized and saved by the operator lifecycle thread during checkpoint. Since the checkpoint process happens outside the window boundary the thread should be quiesced between  endWindow  and  beginWindow  or more efficiently between pre-checkpoint and checkpointed callbacks.", 
            "title": "Threads"
        }, 
        {
            "location": "/apex_cli/", 
            "text": "Apache Apex Command Line Interface\n\n\nApex CLI, the Apache Apex command line interface, can be used to launch, monitor, and manage Apache Apex applications.  It provides a developer friendly way of interacting with Apache Apex platform.  Another advantage of Apex CLI is to provide scope, by connecting and executing commands in a context of specific application.  Apex CLI enables easy integration with existing enterprise toolset for automated application monitoring and management.  Currently the following high level tasks are supported.\n\n\n\n\nLaunch or kill applications\n\n\nView system metrics including load, throughput, latency, etc.\n\n\nStart or stop tuple recording\n\n\nRead operator, stream, port properties and attributes\n\n\nWrite to operator properties\n\n\nDynamically change the application logical plan\n\n\nCreate custom macros\n\n\n\n\nApex CLI Commands\n\n\nApex CLI can be launched by running following command\n\n\napex\n\n\n\nHelp on all commands is available via \u201chelp\u201d command in the CLI\n\n\nGlobal Commands\n\n\nGLOBAL COMMANDS EXCEPT WHEN CHANGING LOGICAL PLAN:\n\nalias alias-name command\n    Create a command alias\n\nbegin-macro name\n    Begin Macro Definition ($1...$9 to access parameters and type 'end' to end the definition)\n\nconnect app-id\n    Connect to an app\n\ndump-properties-file out-file jar-file class-name\n    Dump the properties file of an app class\n\necho [arg ...]\n    Echo the arguments\n\nexit\n    Exit the CLI\n\nget-app-info app-id\n    Get the information of an app\n\nget-app-package-info app-package-file\n    Get info on the app package file\n\nget-app-package-operator-properties app-package-file operator-class\n    Get operator properties within the given app package\n\nget-app-package-operators [options] app-package-file [search-term]\n    Get operators within the given app package\n    Options:\n            -parent    Specify the parent class for the operators\n\nget-config-parameter [parameter-name]\n    Get the configuration parameter\n\nget-jar-operator-classes [options] jar-files-comma-separated [search-term]\n    List operators in a jar list\n    Options:\n            -parent    Specify the parent class for the operators\n\nget-jar-operator-properties jar-files-comma-separated operator-class-name\n    List properties in specified operator\n\nhelp [command]\n    Show help\n\nkill-app app-id/app-name [app-id/app-name ...]\n    Kill an app\n\n  launch [options] jar-file/json-file/properties-file/app-package-file [matching-app-name]\n    Launch an app\n    Options:\n            -apconf \napp package configuration file\n        Specify an application\n                                                            configuration file\n                                                            within the app\n                                                            package if launching\n                                                            an app package.\n            -archives \ncomma separated list of archives\n    Specify comma\n                                                            separated archives\n                                                            to be unarchived on\n                                                            the compute machines.\n            -conf \nconfiguration file\n                      Specify an\n                                                            application\n                                                            configuration file.\n            -D \nproperty=value\n                             Use value for given\n                                                            property.\n            -exactMatch                                     Only consider\n                                                            applications with\n                                                            exact app name\n            -files \ncomma separated list of files\n          Specify comma\n                                                            separated files to\n                                                            be copied on the\n                                                            compute machines.\n            -ignorepom                                      Do not run maven to\n                                                            find the dependency\n            -libjars \ncomma separated list of libjars\n      Specify comma\n                                                            separated jar files\n                                                            or other resource\n                                                            files to include in\n                                                            the classpath.\n            -local                                          Run application in\n                                                            local mode.\n            -originalAppId \napplication id\n                 Specify original\n                                                            application\n                                                            identifier for restart.\n            -queue \nqueue name\n                             Specify the queue to\n                                                            launch the application\n\nlist-application-attributes\n    Lists the application attributes\nlist-apps [pattern]\n    List applications\nlist-operator-attributes\n    Lists the operator attributes\nlist-port-attributes\n    Lists the port attributes\nset-pager on/off\n    Set the pager program for output\nshow-logical-plan [options] jar-file/app-package-file [class-name]\n    List apps in a jar or show logical plan of an app class\n    Options:\n            -exactMatch                                Only consider exact match\n                                                       for app name\n            -ignorepom                                 Do not run maven to find\n                                                       the dependency\n            -libjars \ncomma separated list of jars\n    Specify comma separated\n                                                       jar/resource files to\n                                                       include in the classpath.\nshutdown-app app-id [app-id ...]\n    Shutdown an app\nsource file\n    Execute the commands in a file\n\n\n\n\nCommands after connecting to an application\n\n\nCOMMANDS WHEN CONNECTED TO AN APP (via connect \nappid\n) EXCEPT WHEN CHANGING LOGICAL PLAN:\n\nbegin-logical-plan-change\n    Begin Logical Plan Change\ndump-properties-file out-file [jar-file] [class-name]\n    Dump the properties file of an app class\nget-app-attributes [attribute-name]\n    Get attributes of the connected app\nget-app-info [app-id]\n    Get the information of an app\nget-operator-attributes operator-name [attribute-name]\n    Get attributes of an operator\nget-operator-properties operator-name [property-name]\n    Get properties of a logical operator\nget-physical-operator-properties [options] operator-id\n    Get properties of a physical operator\n    Options:\n            -propertyName \nproperty name\n    The name of the property whose\n                                             value needs to be retrieved\n            -waitTime \nwait time\n            How long to wait to get the result\nget-port-attributes operator-name port-name [attribute-name]\n    Get attributes of a port\nget-recording-info [operator-id] [start-time]\n    Get tuple recording info\nkill-app [\napp-id/app-name\n ...]\n    Kill an app\nkill-container container-id [container-id ...]\n    Kill a container\nlist-containers\n    List containers\nlist-operators [pattern]\n    List operators\nset-operator-property operator-name property-name property-value\n    Set a property of an operator\nset-physical-operator-property operator-id property-name property-value\n    Set a property of an operator\nshow-logical-plan [options] [jar-file/app-package-file] [class-name]\n    Show logical plan of an app class\n    Options:\n            -exactMatch                                Only consider exact match\n                                                       for app name\n            -ignorepom                                 Do not run maven to find\n                                                       the dependency\n            -libjars \ncomma separated list of jars\n    Specify comma separated\n                                                       jar/resource files to\n                                                       include in the classpath.\nshow-physical-plan\n    Show physical plan\nshutdown-app [app-id ...]\n    Shutdown an app\nstart-recording operator-id [port-name] [num-windows]\n    Start recording\nstop-recording operator-id [port-name]\n    Stop recording\nwait timeout\n    Wait for completion of current application\n\n\n\n\nCommands when changing the logical plan\n\n\nCOMMANDS WHEN CHANGING LOGICAL PLAN (via begin-logical-plan-change):\n\nabort\n    Abort the plan change\nadd-stream-sink stream-name to-operator-name to-port-name\n    Add a sink to an existing stream\ncreate-operator operator-name class-name\n    Create an operator\ncreate-stream stream-name from-operator-name from-port-name to-operator-name to-port-name\n    Create a stream\nhelp [command]\n    Show help\nremove-operator operator-name\n    Remove an operator\nremove-stream stream-name\n    Remove a stream\nset-operator-attribute operator-name attr-name attr-value\n    Set an attribute of an operator\nset-operator-property operator-name property-name property-value\n    Set a property of an operator\nset-port-attribute operator-name port-name attr-name attr-value\n    Set an attribute of a port\nset-stream-attribute stream-name attr-name attr-value\n    Set an attribute of a stream\nshow-queue\n    Show the queue of the plan change\nsubmit\n    Submit the plan change\n\n\n\n\nExamples\n\n\nAn example of defining a custom macro.  The macro updates a running application by inserting a new operator.  It takes three parameters and executes a logical plan changes.\n\n\napex\n begin-macro add-console-output\nmacro\n begin-logical-plan-change\nmacro\n create-operator $1 com.datatorrent.lib.io.ConsoleOutputOperator\nmacro\n create-stream stream_$1 $2 $3 $1 in\nmacro\n submit\n\n\n\n\nThen execute the \nadd-console-output\n macro like this\n\n\napex\n add-console-output xyz opername portname\n\n\n\n\nThis macro then expands to run the following command\n\n\nbegin-logical-plan-change\ncreate-operator xyz com.datatorrent.lib.io.ConsoleOutputOperator\ncreate-stream stream_xyz opername portname xyz in\nsubmit\n\n\n\n\nNote\n:  To perform runtime logical plan changes, like ability to add new operators,\nthey must be part of the jar files that were deployed at application launch time.\n\n\nkill-app\n terminates operators in undefined order, causing some operators to not process\ndata emitted by input operators. Such application can be restarted from last checkpointed\nstate using \nlaunch -originalAppId\n command. As application is relaunched from checkpointed\nstate there will be no data loss.\n\n\nshutdown-app\n This command is to terminate the application while making sure that all data\nemitted by input operators are processed throughout the DAG. Application terminated with\n\nshutdown-app\n command can not be restarted, as application is considered to be completed\nsuccessfully.", 
            "title": "Apex CLI"
        }, 
        {
            "location": "/apex_cli/#apache-apex-command-line-interface", 
            "text": "Apex CLI, the Apache Apex command line interface, can be used to launch, monitor, and manage Apache Apex applications.  It provides a developer friendly way of interacting with Apache Apex platform.  Another advantage of Apex CLI is to provide scope, by connecting and executing commands in a context of specific application.  Apex CLI enables easy integration with existing enterprise toolset for automated application monitoring and management.  Currently the following high level tasks are supported.   Launch or kill applications  View system metrics including load, throughput, latency, etc.  Start or stop tuple recording  Read operator, stream, port properties and attributes  Write to operator properties  Dynamically change the application logical plan  Create custom macros", 
            "title": "Apache Apex Command Line Interface"
        }, 
        {
            "location": "/apex_cli/#apex-cli-commands", 
            "text": "Apex CLI can be launched by running following command  apex  Help on all commands is available via \u201chelp\u201d command in the CLI", 
            "title": "Apex CLI Commands"
        }, 
        {
            "location": "/apex_cli/#global-commands", 
            "text": "GLOBAL COMMANDS EXCEPT WHEN CHANGING LOGICAL PLAN:\n\nalias alias-name command\n    Create a command alias\n\nbegin-macro name\n    Begin Macro Definition ($1...$9 to access parameters and type 'end' to end the definition)\n\nconnect app-id\n    Connect to an app\n\ndump-properties-file out-file jar-file class-name\n    Dump the properties file of an app class\n\necho [arg ...]\n    Echo the arguments\n\nexit\n    Exit the CLI\n\nget-app-info app-id\n    Get the information of an app\n\nget-app-package-info app-package-file\n    Get info on the app package file\n\nget-app-package-operator-properties app-package-file operator-class\n    Get operator properties within the given app package\n\nget-app-package-operators [options] app-package-file [search-term]\n    Get operators within the given app package\n    Options:\n            -parent    Specify the parent class for the operators\n\nget-config-parameter [parameter-name]\n    Get the configuration parameter\n\nget-jar-operator-classes [options] jar-files-comma-separated [search-term]\n    List operators in a jar list\n    Options:\n            -parent    Specify the parent class for the operators\n\nget-jar-operator-properties jar-files-comma-separated operator-class-name\n    List properties in specified operator\n\nhelp [command]\n    Show help\n\nkill-app app-id/app-name [app-id/app-name ...]\n    Kill an app\n\n  launch [options] jar-file/json-file/properties-file/app-package-file [matching-app-name]\n    Launch an app\n    Options:\n            -apconf  app package configuration file         Specify an application\n                                                            configuration file\n                                                            within the app\n                                                            package if launching\n                                                            an app package.\n            -archives  comma separated list of archives     Specify comma\n                                                            separated archives\n                                                            to be unarchived on\n                                                            the compute machines.\n            -conf  configuration file                       Specify an\n                                                            application\n                                                            configuration file.\n            -D  property=value                              Use value for given\n                                                            property.\n            -exactMatch                                     Only consider\n                                                            applications with\n                                                            exact app name\n            -files  comma separated list of files           Specify comma\n                                                            separated files to\n                                                            be copied on the\n                                                            compute machines.\n            -ignorepom                                      Do not run maven to\n                                                            find the dependency\n            -libjars  comma separated list of libjars       Specify comma\n                                                            separated jar files\n                                                            or other resource\n                                                            files to include in\n                                                            the classpath.\n            -local                                          Run application in\n                                                            local mode.\n            -originalAppId  application id                  Specify original\n                                                            application\n                                                            identifier for restart.\n            -queue  queue name                              Specify the queue to\n                                                            launch the application\n\nlist-application-attributes\n    Lists the application attributes\nlist-apps [pattern]\n    List applications\nlist-operator-attributes\n    Lists the operator attributes\nlist-port-attributes\n    Lists the port attributes\nset-pager on/off\n    Set the pager program for output\nshow-logical-plan [options] jar-file/app-package-file [class-name]\n    List apps in a jar or show logical plan of an app class\n    Options:\n            -exactMatch                                Only consider exact match\n                                                       for app name\n            -ignorepom                                 Do not run maven to find\n                                                       the dependency\n            -libjars  comma separated list of jars     Specify comma separated\n                                                       jar/resource files to\n                                                       include in the classpath.\nshutdown-app app-id [app-id ...]\n    Shutdown an app\nsource file\n    Execute the commands in a file", 
            "title": "Global Commands"
        }, 
        {
            "location": "/apex_cli/#commands-after-connecting-to-an-application", 
            "text": "COMMANDS WHEN CONNECTED TO AN APP (via connect  appid ) EXCEPT WHEN CHANGING LOGICAL PLAN:\n\nbegin-logical-plan-change\n    Begin Logical Plan Change\ndump-properties-file out-file [jar-file] [class-name]\n    Dump the properties file of an app class\nget-app-attributes [attribute-name]\n    Get attributes of the connected app\nget-app-info [app-id]\n    Get the information of an app\nget-operator-attributes operator-name [attribute-name]\n    Get attributes of an operator\nget-operator-properties operator-name [property-name]\n    Get properties of a logical operator\nget-physical-operator-properties [options] operator-id\n    Get properties of a physical operator\n    Options:\n            -propertyName  property name     The name of the property whose\n                                             value needs to be retrieved\n            -waitTime  wait time             How long to wait to get the result\nget-port-attributes operator-name port-name [attribute-name]\n    Get attributes of a port\nget-recording-info [operator-id] [start-time]\n    Get tuple recording info\nkill-app [ app-id/app-name  ...]\n    Kill an app\nkill-container container-id [container-id ...]\n    Kill a container\nlist-containers\n    List containers\nlist-operators [pattern]\n    List operators\nset-operator-property operator-name property-name property-value\n    Set a property of an operator\nset-physical-operator-property operator-id property-name property-value\n    Set a property of an operator\nshow-logical-plan [options] [jar-file/app-package-file] [class-name]\n    Show logical plan of an app class\n    Options:\n            -exactMatch                                Only consider exact match\n                                                       for app name\n            -ignorepom                                 Do not run maven to find\n                                                       the dependency\n            -libjars  comma separated list of jars     Specify comma separated\n                                                       jar/resource files to\n                                                       include in the classpath.\nshow-physical-plan\n    Show physical plan\nshutdown-app [app-id ...]\n    Shutdown an app\nstart-recording operator-id [port-name] [num-windows]\n    Start recording\nstop-recording operator-id [port-name]\n    Stop recording\nwait timeout\n    Wait for completion of current application", 
            "title": "Commands after connecting to an application"
        }, 
        {
            "location": "/apex_cli/#commands-when-changing-the-logical-plan", 
            "text": "COMMANDS WHEN CHANGING LOGICAL PLAN (via begin-logical-plan-change):\n\nabort\n    Abort the plan change\nadd-stream-sink stream-name to-operator-name to-port-name\n    Add a sink to an existing stream\ncreate-operator operator-name class-name\n    Create an operator\ncreate-stream stream-name from-operator-name from-port-name to-operator-name to-port-name\n    Create a stream\nhelp [command]\n    Show help\nremove-operator operator-name\n    Remove an operator\nremove-stream stream-name\n    Remove a stream\nset-operator-attribute operator-name attr-name attr-value\n    Set an attribute of an operator\nset-operator-property operator-name property-name property-value\n    Set a property of an operator\nset-port-attribute operator-name port-name attr-name attr-value\n    Set an attribute of a port\nset-stream-attribute stream-name attr-name attr-value\n    Set an attribute of a stream\nshow-queue\n    Show the queue of the plan change\nsubmit\n    Submit the plan change", 
            "title": "Commands when changing the logical plan"
        }, 
        {
            "location": "/apex_cli/#examples", 
            "text": "An example of defining a custom macro.  The macro updates a running application by inserting a new operator.  It takes three parameters and executes a logical plan changes.  apex  begin-macro add-console-output\nmacro  begin-logical-plan-change\nmacro  create-operator $1 com.datatorrent.lib.io.ConsoleOutputOperator\nmacro  create-stream stream_$1 $2 $3 $1 in\nmacro  submit  Then execute the  add-console-output  macro like this  apex  add-console-output xyz opername portname  This macro then expands to run the following command  begin-logical-plan-change\ncreate-operator xyz com.datatorrent.lib.io.ConsoleOutputOperator\ncreate-stream stream_xyz opername portname xyz in\nsubmit  Note :  To perform runtime logical plan changes, like ability to add new operators,\nthey must be part of the jar files that were deployed at application launch time.  kill-app  terminates operators in undefined order, causing some operators to not process\ndata emitted by input operators. Such application can be restarted from last checkpointed\nstate using  launch -originalAppId  command. As application is relaunched from checkpointed\nstate there will be no data loss.  shutdown-app  This command is to terminate the application while making sure that all data\nemitted by input operators are processed throughout the DAG. Application terminated with shutdown-app  command can not be restarted, as application is considered to be completed\nsuccessfully.", 
            "title": "Examples"
        }, 
        {
            "location": "/security/", 
            "text": "Security\n\n\nApplications built on Apex run as native YARN applications on Hadoop. The security framework and apparatus in Hadoop apply to the applications. Both authentication and SSL aspects of the security framework are covered here.\n\n\nAuthentication\n\n\nThe default authentication mechanism in Hadoop is Kerberos.\n\n\nKerberos Authentication\n\n\nKerberos is a ticket based authentication system that provides authentication in a distributed environment where authentication is needed between multiple users, hosts and services. It is the de-facto authentication mechanism supported in Hadoop. To use Kerberos authentication, the Hadoop installation must first be configured for secure mode with Kerberos. Please refer to the administration guide of your Hadoop distribution on how to do that. Once Hadoop is configured, some configuration is needed on the Apex side as well.\n\n\nConfiguring Kerberos\n\n\nThe Apex command line interface (CLI) program, \napex\n, is used to launch applications on the Hadoop cluster along with performing various other operations and administrative tasks on the applications. In a secure cluster additional configuration is needed for the CLI program \napex\n.\n\n\nCLI Configuration\n\n\nWhen Kerberos security is enabled in Hadoop, a Kerberos ticket granting ticket (TGT) or the Kerberos credentials of the user are needed by the CLI program \napex\n to authenticate with Hadoop for any operation. Kerberos credentials are composed of a principal and either a \nkeytab\n or a password. For security and operational reasons only keytabs are supported in Hadoop and by extension in Apex platform. When user credentials are specified, all operations including launching application are performed as that user.\n\n\nUsing kinit\n\n\nA Kerberos ticket granting ticket (TGT) can be obtained by using the Kerberos command \nkinit\n. Detailed documentation for the command can be found online or in man pages. An sample usage of this command is\n\n\nkinit -k -t path-tokeytab-file kerberos-principal\n\n\n\nIf this command is successful, the TGT is obtained, cached and available for other programs. The CLI program \napex\n can then be started to launch applications and perform other operations.\n\n\nUsing Kerberos credentials\n\n\nThe CLI program \napex\n can also use the Kerberos credentials directly without requiring a TGT to be obtained separately. This can be useful in batch mode where \napex\n is not launched manually and also in scenarios where running another program like \nkinit\n is not feasible.\n\n\nThe credentials can be specified in the \ndt-site.xml\n configuration file. If only a single user is launching applications, the global \ndt-site.xml\n configuration file in the installation folder can be used. In a multi-user environment the users can use the \ndt-site.xml\n file in their\nhome directory. The location of this file will be \n$HOME/.dt/dt-site.xml\n. If this file does not exist, the user can create a new one.\n\n\nThe snippet below shows the how the credentials can be specified in the configuration file as properties.\n\n\nproperty\n\n        \nname\ndt.authentication.principal\n/name\n\n        \nvalue\nkerberos-principal-of-user\n/value\n\n\n/property\n\n\nproperty\n\n        \nname\ndt.authentication.keytab\n/name\n\n        \nvalue\nabsolute-path-to-keytab-file\n/value\n\n\n/property\n\n\n\n\n\nThe property \ndt.authentication.principal\n specifies the Kerberos user principal and \ndt.authentication.keytab\n specifies the absolute path to the keytab file for the user.\n\n\nWeb Services security\n\n\nAlongside every Apex application, there is an application master process called Streaming Container Manager (STRAM) running. STRAM manages the application by handling the various control aspects of the application such as orchestrating the execution of the application on the cluster, playing a key role in scalability and fault tolerance, providing application insight by collecting statistics among other functionality.\n\n\nSTRAM provides a web service interface to introspect the state of the application and its various components and to make dynamic changes to the applications. Some examples of supported functionality are getting resource usage and partition information of various operators, getting operator statistics and changing properties of running operators.\n\n\nAccess to the web services can be secured to prevent unauthorized access. By default it is automatically enabled in Hadoop secure mode environments and not enabled in non-secure environments. How the security actually works is described in \nSecurity architecture\n section below.\n\n\nThere are additional options available for finer grained control on enabling it. This can be configured on a per-application basis using an application attribute. It can also be enabled or disabled based on Hadoop security configuration. The following security options are available\n\n\n\n\nEnable - Enable Authentication\n\n\nFollow Hadoop Authentication - Enable authentication if secure mode is enabled in Hadoop, the default\n\n\nFollow Hadoop HTTP Authentication - Enable authentication only if HTTP authentication is enabled in Hadoop and not just secure mode.\n\n\nDisable - Disable Authentication\n\n\n\n\nTo specify the security option for an application the following configuration can be specified in the \ndt-site.xml\n file\n\n\nproperty\n\n        \nname\ndt.application.name.attr.STRAM_HTTP_AUTHENTICATION\n/name\n\n        \nvalue\nsecurity-option\n/value\n\n\n/property\n\n\n\n\n\nThe security option value can be \nENABLED\n, \nFOLLOW_HADOOP_AUTH\n, \nFOLLOW_HADOOP_HTTP_AUTH\n or \nDISABLE\n for the four options above respectively.\n\n\nThe subsequent sections talk about how security works in Apex. This information is not needed by users but is intended for the inquisitive techical audience who want to know how security works.\n\n\nCLI setup\n\n\nThe CLI program \napex\n connects to the web service endpoint of the STRAM for a running application to query for information or to make changes to it. In order to do that, it has to first connect to the YARN proxy web service and get the necessary connection information and credentials to connect to STRAM. The proxy web service may have security enabled and in that case, the CLI program \napex\n would first need to authenticate with the service before it can get any information.\n\n\nHadoop allows a lot of flexibility in the kind of security to use for the proxy. It allows the user to plug-in their own authentication provider. The authentication provider is specified as a JAVA class name. It also comes bundled with a provider for Kerberos SPNEGO authentication. Some distributions also include a provider for BASIC authentication via SASL.\n\n\nThe CLI \napex\n, has built-in functionality for Kerberos SPNEGO, BASIC and DIGEST authentication mechanisms. Because of the way the authentication provider is configured for the proxy on the Hadoop side, there is no reliable way to determine before hand what kind of authentication is being used. Only at runtime, when the CLI connects to the proxy web service will it know the type of authentication that the service is using. For this reason, \napex\n allows the user to configure credentials for multiple authentication mechanisms it supports and will pick the one that matches what the service expects.\n\n\nIf the authentication mechanism is Kerberos SPNEGO, the properties listed in the \nUsing Kerberos credentials\n section for general communication with Hadoop above are sufficient. No additional properties are needed.\n\n\nFor BASIC authentication, the credentials can be specified using the following properties\n\n\nproperty\n\n        \nname\ndt.authentication.basic.username\n/name\n\n        \nvalue\nusername\n/value\n\n\n/property\n\n\nproperty\n\n        \nname\ndt.authentication.basic.password\n/name\n\n        \nvalue\npassword\n/value\n\n\n/property\n\n\n\n\n\nFor DIGEST authentication, the credentials can be specified using the following properties\n\n\nproperty\n\n        \nname\ndt.authentication.digest.username\n/name\n\n        \nvalue\nusername\n/value\n\n\n/property\n\n\nproperty\n\n        \nname\ndt.authentication.digest.password\n/name\n\n        \nvalue\npassword\n/value\n\n\n/property\n\n\n\n\n\nToken Refresh\n\n\nApex applications, at runtime, use delegation tokens to authenticate with Hadoop services when communicating with them as described in the security architecture section below. The delegation tokens are originally issued by these Hadoop services and have an expiry time period which is typically 7 days. The tokens become invalid beyond this time and the applications will no longer be able to communicate with the Hadoop services. For long running applications this presents a problem.\n\n\nTo solve this problem one of the two approaches can be used. The first approach is to change the Hadoop configuration itself to extend the token expiry time period. This may not be possible in all environments as it requires a change in the security policy as the tokens will now be valid for a longer period of time and the change also requires administrator privileges to Hadoop. The second approach is to use a feature available in apex to auto-refresh the tokens before they expire. Both the approaches are detailed below and the users can choose the one that works best for them.\n\n\nHadoop configuration approach\n\n\nAn Apex application uses delegation tokens to authenticate with Hadoop services, Resource Manager (YARN) and Name Node (HDFS), and these tokens are issued by those services respectively. Since the application is long-running, the tokens can expire while the application is still running. Hadoop uses configuration settings for the maximum lifetime of these tokens. \n\n\nThere are separate settings for ResourceManager and NameNode delegation tokens. In this approach the user increases the values of these settings to cover the lifetime of the application. Once these settings are changed, the YARN and HDFS services would have to be restarted. The values in these settings are of type \nlong\n and has an upper limit so applications cannot run forever. This limitation is not present with the next approach described below.\n\n\nThe Resource Manager delegation token max lifetime is specified in \nyarn-site.xml\n and can be specified as follows for a lifetime of 1 year as an example\n\n\nproperty\n\n  \nname\nyarn.resourcemanager.delegation.token.max-lifetime\n/name\n\n  \nvalue\n31536000000\n/value\n\n\n/property\n\n\n\n\n\nThe Name Node delegation token max lifetime is specified in\nhdfs-site.xml and can be specified as follows for a lifetime of 1 year as an example\n\n\nproperty\n\n   \nname\ndfs.namenode.delegation.token.max-lifetime\n/name\n\n   \nvalue\n31536000000\n/value\n\n \n/property\n\n\n\n\n\nAuto-refresh approach\n\n\nIn this approach the application, in anticipation of a token expiring, obtains a new token to replace the current one. It keeps repeating the process whenever a token is close to expiry so that the application can continue to run indefinitely.\n\n\nThis requires the application having access to a keytab file at runtime because obtaining a new token requires a keytab. The keytab file should be present in HDFS so that the application can access it at runtime. The user can provide a HDFS location for the keytab file using a setting otherwise the keytab file specified for the \napex\n CLI program above will be copied from the local filesystem into HDFS before the application is started and made available to the application. There are other optional settings available to configure the behavior of this feature. All the settings are described below.\n\n\nThe location of the keytab can be specified by using the following setting in \ndt-site.xml\n. If it is not specified then the file specified in \ndt.authentication.keytab\n is copied into HDFS and used.\n\n\nproperty\n\n        \nname\ndt.authentication.store.keytab\n/name\n\n        \nvalue\nhdfs-path-to-keytab-file\n/value\n\n\n/property\n\n\n\n\n\nThe expiry period of the Resource Manager and Name Node tokens needs to be known so that the application can renew them before they expire. These are automatically obtained using the \nyarn.resourcemanager.delegation.token.max-lifetime\n and \ndfs.namenode.delegation.token.max-lifetime\n properties from the hadoop configuration files. Sometimes however these properties are not available or kept up-to-date on the nodes running the applications. If that is the case then the following properties can be used to specify the expiry period, the values are in milliseconds. The example below shows how to specify these with values of 7 days.\n\n\nproperty\n\n        \nname\ndt.resourcemanager.delegation.token.max-lifetime\n/name\n\n        \nvalue\n604800000\n/value\n\n\n/property\n\n\n\nproperty\n\n        \nname\ndt.namenode.delegation.token.max-lifetime\n/name\n\n        \nvalue\n604800000\n/value\n\n\n/property\n\n\n\n\n\nAs explained earlier new tokens are obtained before the old ones expire. How early the new tokens are obtained before expiry is controlled by a setting. This setting is specified as a factor of the token expiration with a value between 0.0 and 1.0. The default value is \n0.7\n. This factor is multipled with the expiration time to determine when to refresh the tokens. This setting can be changed by the user and the following example shows how this can be done\n\n\nproperty\n\n        \nname\ndt.authentication.token.refresh.factor\n/name\n\n        \nvalue\n0.7\n/value\n\n\n/property\n\n\n\n\n\nImpersonation\n\n\nThe CLI program \napex\n supports Hadoop proxy user impersonation, in allowing applications to be launched and other operations to be performed as a different user than the one specified by the Kerberos credentials. The Kerberos credentials are still used for authentication. This is useful in scenarios where a system using \napex\n has to support multiple users but only has a single set of Kerberos credentials, those of a system user.\n\n\nUsage\n\n\nTo use this feature, the following environment variable should be set to the user name of the user being impersonated, before running \napex\n and the operations will be performed as that user. For example, if launching an application, the application will run as the specified user and not as the user specified by the Kerberos credentials.\n\n\nHADOOP_USER_NAME=\nusername\n\n\n\n\n\nHadoop Configuration\n\n\nFor this feature to work, additional configuration settings are needed in Hadoop. These settings would allow a specified user, such as a system user, to impersonate other users. The example snippet below shows these settings. In this example, the specified user can impersonate users belonging to any group and can do so running from any host. Note that the user specified here is different from the user specified above in usage, there it is the user that is being impersonated and here it is the impersonating user such as a system user.\n\n\nproperty\n\n  \nname\nhadoop.proxyuser.\nusername\n.groups\n/name\n\n  \nvalue\n*\n/value\n\n\n/property\n\n\n\nproperty\n\n  \nname\nhadoop.proxyuser.\nusername\n.hosts\n/name\n\n  \nvalue\n*\n/value\n\n\n/property\n\n\n\n\n\nApplication Root Directory under HDFS\n\n\nA running Apex application uses a 'root' directory under HDFS where runtime artifacts are saved or read from. For example, with default configuration an Apex application \nwould use the HDFS path \n/user/dtadmin/datatorrent\n as the application root directory. If the application's Hadoop assigned application ID is \napplication_1487803614053_10222\n \nthen the full HDFS path for saving and reading application package contents,  events, checkpoint and recovery data is \n\n/user/dtadmin/datatorrent/apps/application_1487803614053_10222\n.\n\n\nThis root directory determination involves using the 'current' user, and in case of impersonation Apex treats either the impersonating user or the impersonated \nuser as the 'current' user depending on the value of the configuration propery \ndt.authentication.impersonation.path.enable\n. You should set its value to \ntrue\n to \ntreat the impersonated user as the current user, or \nfalse\n (the default value) to treat the impersonating user as the current user.\n\n\nApex also uses the configuration property \napex.app.dfsRootDirectory\n to determine the application root directory when \ndt.authentication.impersonation.path.enable\n\nis set to \ntrue\n i.e. treating the impersonated user as the 'current' user. When \ndt.authentication.impersonation.path.enable\n is set to \nfalse\n, Apex will use\nthe configuration property \ndt.dfsRootDirectory\n to determine the application root directory.\n\n\nAs an example, let's assume the following values:\n\n\n\n\n\n\napex.app.dfsRootDirectory\n is set to \n/user/%USER_NAME%/apex\n.\n\n\n\n\n\n\ndt.dfsRootDirectory\n is set to \n/user/%USER_NAME%/datatorrent\n.\n\n\n\n\n\n\nAlso assume the impersonating user is \napexadmin\n and the impersonated user is \npeter\n when the application is launched. In this scenario, when\n\ndt.authentication.impersonation.path.enable\n is set to \ntrue\n, the application root directory is \n/user/peter/apex\n. When it is set to \nfalse\n, the application\nroot directory is \n/user/apexadmin/datatorrent\n. As you can see, Apex replaces the string \n%USER_NAME%\n with the 'current' user as described above. If you do not use\nan absolute path (i.e. path starting with \n/\n) as the value of \napex.app.dfsRootDirectory\n then the evaluated path is prepended with the current user's home directory.\nFor example, when the value of \napex.app.dfsRootDirectory\n is set to \napex/%USER_NAME%/myDir\n and \ndt.authentication.impersonation.path.enable\n is set to \ntrue\n then\nthe application root directory in the above example is \n/user/peter/apex/peter/myDir\n.\n\n\nAll of the configuration properties mentioned above can be set using any one of the methods mentioned \nhere\n.\nFor example, you can use the \ndt-site.xml\n file, or use the \n-D\n option in the Apex CLI launch command as described \nhere\n.\n\n\nSecurity architecture\n\n\nIn this section we will see how security works for applications built on Apex. We will look at the different methodologies involved in running the applications and in each case we will look into the different components that are involved. We will go into the architecture of these components and look at the different security mechanisms that are in play.\n\n\nApplication Launch\n\n\nTo launch applications in Apache Apex the command line client \napex\n can be used. The application artifacts such as binaries and properties are supplied as an application package. The client, during the various steps involved to launch the application needs to communicate with both the Resource Manager and the Name Node. The Resource Manager communication involves the client asking for new resources to run the application master and start the application launch process. The steps along with sample Java code are described in Writing YARN Applications. The Name Node communication includes the application artifacts being copied to HDFS so that they are available across the cluster for launching the different application containers.\n\n\nIn secure mode, the communications with both Resource Manager and Name Node requires authentication and the mechanism is Kerberos. Below is an illustration showing this.\n\n\n\n\nThe client \napex\n supports Kerberos authentication and will automatically enable it in a secure environment. To authenticate, some Kerberos configuration namely the Kerberos credentials, are needed by the client. There are two parameters, the Kerberos principal and keytab to use for the client. These can be specified in the dt-site.xml configuration file. The properties are shown below\n\n\n    \nproperty\n\n            \nname\ndt.authentication.principal\n/name\n\n            \nvalue\nkerberos-principal-of-user\n/value\n\n    \n/property\n\n    \nproperty\n\n            \nname\ndt.authentication.keytab\n/name\n\n            \nvalue\nabsolute-path-to-keytab-file\n/value\n\n    \n/property\n\n\n\n\nRefer to document Operation and Installation Guide section Multi Tenancy and Security subsection CLI Configuration in the documentation for more information. The document can also be accessed here client configuration\n\n\nThere is another important functionality that is performed by the client and that is to retrieve what are called delegation tokens from the Resource Manager and Name Node to seed the application master container that is to be launched. This is detailed in the next section. \n\n\nRuntime Security\n\n\nWhen the application is completely up and running, there are different components of the application running as separate processes possibly on different nodes in the cluster as it is a distributed application. These components interactwould be interacting with each other and the Hadoop services. In secure mode, all these interactions have to be authenticated before they can be successfully processed. The interactions are illustrated below in a diagram to give a complete overview. Each of them is explained in subsequent sections.\n\n\n\n\nSTRAM and Hadoop\n\n\nEvery Apache Apex application has a master process akin to any YARN application. In our case it is called STRAM (Streaming Application Master). It is a master process that runs in its own container and manages the different distributed components of the application. Among other tasks it requests Resource Manager for new resources as they are needed and gives back resources that are no longer needed. STRAM also needs to communicate with Name Node from time-to-time to access the persistent HDFS file system. \n\n\nIn secure mode, STRAM has to authenticate with both Resource Manager and Name Node before it can send any requests and this is achieved using Delegation Tokens. Since STRAM runs as a managed application master, it runs in a Hadoop container. This container could have been allocated on any node based on what resources were available. Since there is no fixed node where STRAM runs, it does not have Kerberos credentials. Unlike launch client \napex\n, it cannot authenticate with Hadoop services Resource Manager and Name Node using Kerberos. Instead, Delegation Tokens are used for authentication.\n\n\nDelegation Tokens\n\n\nDelegation tokens are tokens that are dynamically issued by the source and clients use them to authenticate with the source. The source stores the delegation tokens it has issued in a cache and checks the delegation token sent by a client against the cache. If a match is found, the authentication is successful else it fails. This is the second mode of authentication in secure Hadoop after Kerberos. More details can be found in the Hadoop security design document. In this case the delegation tokens are issued by Resource Manager and Name Node. STRAM would use these tokens to authenticate with them. But how does it get them in the first place? This is where the launch client \napex\n comes in.\n\n\nThe client \napex\n, since it possesses Kerberos credentials as explained in the Application Launch section, is able to authenticate with Resource Manager and Name Node using Kerberos. It then requests for delegation tokens over the Kerberos authenticated connection. The servers return the delegation tokens in the response payload. The client in requesting the resource manager for the start of the application master container for STRAM seeds it with these tokens so that when STRAM starts it has these tokens. It can then use these tokens to authenticate with the Hadoop services.\n\n\nStreaming Container\n\n\nA streaming container is a process that runs a part of the application business logic. It is a container deployed on a node in the cluster. The part of business logic is implemented in what we call an operator. Multiple operators connected together make up the complete application and hence there are multiple streaming containers in an application. The streaming containers have different types of communications going on as illustrated in the diagram above. They are described below.\n\n\nSTRAM Delegation Token\n\n\nThe streaming containers periodically communicate with the application master STRAM. In the communication they send what are called heartbeats with information such as statistics and receive commands from STRAM such as deployment or un-deployment of operators, changing properties of operators etc. In secure mode, this communication cannot just occur without any authentication. To facilitate this authentication special tokens called STRAM Delegation Tokens are used. These tokens are created and managed by STRAM. When a new streaming container is being started, since STRAM is the one negotiating resources from Resource Manager for the container and requesting to start the container, it seeds the container with the STRAM delegation token necessary to communicate with it. Thus, a streaming container has the STRAM delegation token to successfully authenticate and communicate with STRAM.\n\n\nBuffer Server Token\n\n\nAs mentioned earlier an operator implements a piece of the business logic of the application and multiple operators together complete the application. In creating the application the operators are assembled together in a direct acyclic graph, a pipeline, with output of operators becoming the input for other operators. At runtime the stream containers hosting the operators are connected to each other and sending data to each other. In secure mode these connections should be authenticated too, more importantly than others, as they are involved in transferring application data.\n\n\nWhen operators are running there will be effective processing rate differences between them due to intrinsic reasons such as operator logic or external reasons such as different resource availability of CPU, memory, network bandwidth etc. as the operators are running in different containers. To maximize performance and utilization the data flow is handled asynchronous to the regular operator function and a buffer is used to intermediately store the data that is being produced by the operator. This buffered data is served by a buffer server over the network connection to the downstream streaming container containing the operator that is supposed to receive the data from this operator. This connection is secured by a token called the buffer server token. These tokens are also generated and seeded by STRAM when the streaming containers are deployed and started and it uses different tokens for different buffer servers to have better security.\n\n\nNameNode Delegation Token\n\n\nLike STRAM, streaming containers also need to communicate with NameNode to use HDFS persistence for reasons such as saving the state of the operators. In secure mode they also use NameNode delegation tokens for authentication. These tokens are also seeded by STRAM for the streaming containers.\n\n\nStram Web Services\n\n\nClients connect to STRAM and make web service requests to obtain operational information about running applications. When security is enabled we want this connection to also be authenticated. In this mode the client passes a web service token in the request and STRAM checks this token. If the token is valid, then the request is processed else it is denied.\n\n\nHow does the client get the web service token in the first place? The client will have to first connect to STRAM via the Resource Manager Web Services Proxy which is a service run by Hadoop to proxy requests to application web services. This connection is authenticated by the proxy service using a protocol called SPNEGO when secure mode is enabled. SPNEGO is Kerberos over HTTP and the client also needs to support it. If the authentication is successful the proxy forwards the request to STRAM. STRAM in processing the request generates and sends back a web service token similar to a delegation token. This token is then used by the client in subsequent requests it makes directly to STRAM and STRAM is able to validate it since it generated the token in the first place.\n\n\n\n\nSSL Configuration\n\n\nThe STRAM Web services component described above can be configured with SSL to enable HTTPS. To achieve this you need to enable SSL in YARN as described \nhere\n, specifically set \nyarn.http.policy\n to \nHTTPS_ONLY\n in \nyarn-site.xml\n. You also need to make keystore and other SSL material available to the Web services component for HTTPS to work. There are 3 approaches to achieve this:\n\n\nApproach 1: Using Default Hadoop-YARN SSL\n\n\nIf the default Hadoop-YARN SSL material is uniformly available in the local file system of each node in your Hadoop cluster, STRAM can use it provided the STRAM process has access to the files. Use your Hadoop/YARN configuration procedures to enable SSL in YARN to use this approach, so all YARN applications including Apex applications will be SSL-enabled. However if the SSL material is not present, or not accessible to the STRAM process (for example, if STRAM is not running as \nroot\n), then you will have to use one of the following two approaches.\n\n\nApproach 2: Pre-installing SSL Files on Hadoop Cluster Nodes\n\n\nWith this approach, your own SSL files are pre-installed on each node in the Hadoop cluster, so the STRAM can access them regardless of the node it runs on. Each node will have 2 files: \nssl-server.xml\n is the master SSL configuration file and the SSL keystore file (a JKS file) whose name and location are specified in \nssl-server.xml\n. Decide on a location for each of these files and follow the steps below.\n\n\n\n\n\n\nCreate an SSL keystore as described \nhere\n. Assume this file is named \nmyapex-keystore.jks\n and will reside at \n/opt/apex/keystore/\n on each Hadoop node and the keystore password is \nstorepass1\n and the keystore key-password is \nkeypass2\n. \n\n\n\n\n\n\nCreate the SSL configuration file with the following content. Typically the file is called \nssl-server.xml\n but you can use any other name.\n\n\n\n\n\n\nconfiguration\n\n  \nproperty\n\n    \nname\nssl.server.keystore.location\n/name\n\n    \nvalue\n/opt/apex/keystore/myapex-keystore.jks\n/value\n\n    \ndescription\n/description\n\n  \n/property\n\n  \nproperty\n\n    \nname\nssl.server.keystore.keypassword\n/name\n\n    \nvalue\nkeypass2\n/value\n\n    \ndescription\n/description\n\n  \n/property\n\n  \nproperty\n\n    \nname\nssl.server.keystore.password\n/name\n\n    \nvalue\nstorepass1\n/value\n\n    \ndescription\n/description\n\n  \n/property\n\n\n/configuration\n\n\n\n\n\nLet's assume this file will reside on each node as \n/opt/apex/sslConfig/my-apex-ssl-server.xml\n .\n\n\n\n\n\n\nCopy the keystore file and the SSL configuration file on each Hadoop node to their designated locations and make sure they are readable by your Apex application.\n\n\n\n\n\n\nUsing the Apex CLI, pass the SSL configuration file location to your Apex application using the SSL_CONFIG attribute as follows:\n\n\n\n\n\n\nlaunch -Dapex.attr.SSL_CONFIG=\n{\\\nconfigPath\\\n:\\\n/opt/apex/sslConfig/my-apex-ssl-server.xml\\\n}\n  \napa file\n\n\n\n\n\n\n\nAlternatively you can use configuration files to supply the value of \napex.attr.SSL_CONFIG\n attribute as described \nhere\n\n\n\n\nApproach 3: Using Apex CLI to Copy the Keystore\n\n\nIf you cannot pre-install the SSL files on all your cluster nodes, you can use the SSL_CONFIG attribute in such a way that Apex CLI copies the keystore file from the client node to the server node and also passes on the keystore password values to the STRAM. The keystore file needs to be accessible on the Apex CLI machine so Apex CLI can copy it. Let's assume this location is \n/opt/apexCli/ssl/myapex-keystore.jks\n. Use the Apex CLI launch command as follows:\n\n\nlaunch -Dapex.attr.SSL_CONFIG=\n{\\\nkeyStorePath\\\n:\\\n/opt/apexCli/ssl/myapex-keystore.jks\\\n,\\\nkeyStorePassword\\\n:\\\nstorepass1\\\n,\\\nkeyStoreKeyPassword\\\n:\\\nkeypass2\\\n}\n  \napa file\n\n\n\n\n\nApec CLI will copy the keystore file \n/opt/apexCli/ssl/myapex-keystore.jks\n to the destination STRAM node and also pass on the keystore password values to the STRAM. As mentioned above, you can also use configuration files to supply the value of \napex.attr.SSL_CONFIG\n.\n\n\nUpdating Trust-store for the App Proxy\n\n\nYou need to ensure that all end-points connecting to the STRAM Web service trust the SSL certificate when SSL is enabled.\nThis is especially true for the \nWeb Application Proxy\n which connects to the STRAM Web service whenever you access the service through the App Master Proxy HTTPS URL. \nIf you use a self-signed or untrusted certificate, you will need to add that certificate to the trust-store used by the RM Web Application Proxy as described \nhere\n and update \nssl-client.xml\n to use the trust-store as described \nhere\n\n\nDependencies\n\n\nThe use of the attribute \napex.attr.SSL_CONFIG\n described in the last 2 approaches is dependent on an \nenhancement\n made in YARN, which is available in the following versions: \n2.9.0, 2.7.4, 3.0.0-alpha4, 2.8.2.", 
            "title": "Security"
        }, 
        {
            "location": "/security/#security", 
            "text": "Applications built on Apex run as native YARN applications on Hadoop. The security framework and apparatus in Hadoop apply to the applications. Both authentication and SSL aspects of the security framework are covered here.", 
            "title": "Security"
        }, 
        {
            "location": "/security/#authentication", 
            "text": "The default authentication mechanism in Hadoop is Kerberos.", 
            "title": "Authentication"
        }, 
        {
            "location": "/security/#kerberos-authentication", 
            "text": "Kerberos is a ticket based authentication system that provides authentication in a distributed environment where authentication is needed between multiple users, hosts and services. It is the de-facto authentication mechanism supported in Hadoop. To use Kerberos authentication, the Hadoop installation must first be configured for secure mode with Kerberos. Please refer to the administration guide of your Hadoop distribution on how to do that. Once Hadoop is configured, some configuration is needed on the Apex side as well.", 
            "title": "Kerberos Authentication"
        }, 
        {
            "location": "/security/#configuring-kerberos", 
            "text": "The Apex command line interface (CLI) program,  apex , is used to launch applications on the Hadoop cluster along with performing various other operations and administrative tasks on the applications. In a secure cluster additional configuration is needed for the CLI program  apex .", 
            "title": "Configuring Kerberos"
        }, 
        {
            "location": "/security/#cli-configuration", 
            "text": "When Kerberos security is enabled in Hadoop, a Kerberos ticket granting ticket (TGT) or the Kerberos credentials of the user are needed by the CLI program  apex  to authenticate with Hadoop for any operation. Kerberos credentials are composed of a principal and either a  keytab  or a password. For security and operational reasons only keytabs are supported in Hadoop and by extension in Apex platform. When user credentials are specified, all operations including launching application are performed as that user.", 
            "title": "CLI Configuration"
        }, 
        {
            "location": "/security/#using-kinit", 
            "text": "A Kerberos ticket granting ticket (TGT) can be obtained by using the Kerberos command  kinit . Detailed documentation for the command can be found online or in man pages. An sample usage of this command is  kinit -k -t path-tokeytab-file kerberos-principal  If this command is successful, the TGT is obtained, cached and available for other programs. The CLI program  apex  can then be started to launch applications and perform other operations.", 
            "title": "Using kinit"
        }, 
        {
            "location": "/security/#using-kerberos-credentials", 
            "text": "The CLI program  apex  can also use the Kerberos credentials directly without requiring a TGT to be obtained separately. This can be useful in batch mode where  apex  is not launched manually and also in scenarios where running another program like  kinit  is not feasible.  The credentials can be specified in the  dt-site.xml  configuration file. If only a single user is launching applications, the global  dt-site.xml  configuration file in the installation folder can be used. In a multi-user environment the users can use the  dt-site.xml  file in their\nhome directory. The location of this file will be  $HOME/.dt/dt-site.xml . If this file does not exist, the user can create a new one.  The snippet below shows the how the credentials can be specified in the configuration file as properties.  property \n         name dt.authentication.principal /name \n         value kerberos-principal-of-user /value  /property  property \n         name dt.authentication.keytab /name \n         value absolute-path-to-keytab-file /value  /property   The property  dt.authentication.principal  specifies the Kerberos user principal and  dt.authentication.keytab  specifies the absolute path to the keytab file for the user.", 
            "title": "Using Kerberos credentials"
        }, 
        {
            "location": "/security/#web-services-security", 
            "text": "Alongside every Apex application, there is an application master process called Streaming Container Manager (STRAM) running. STRAM manages the application by handling the various control aspects of the application such as orchestrating the execution of the application on the cluster, playing a key role in scalability and fault tolerance, providing application insight by collecting statistics among other functionality.  STRAM provides a web service interface to introspect the state of the application and its various components and to make dynamic changes to the applications. Some examples of supported functionality are getting resource usage and partition information of various operators, getting operator statistics and changing properties of running operators.  Access to the web services can be secured to prevent unauthorized access. By default it is automatically enabled in Hadoop secure mode environments and not enabled in non-secure environments. How the security actually works is described in  Security architecture  section below.  There are additional options available for finer grained control on enabling it. This can be configured on a per-application basis using an application attribute. It can also be enabled or disabled based on Hadoop security configuration. The following security options are available   Enable - Enable Authentication  Follow Hadoop Authentication - Enable authentication if secure mode is enabled in Hadoop, the default  Follow Hadoop HTTP Authentication - Enable authentication only if HTTP authentication is enabled in Hadoop and not just secure mode.  Disable - Disable Authentication   To specify the security option for an application the following configuration can be specified in the  dt-site.xml  file  property \n         name dt.application.name.attr.STRAM_HTTP_AUTHENTICATION /name \n         value security-option /value  /property   The security option value can be  ENABLED ,  FOLLOW_HADOOP_AUTH ,  FOLLOW_HADOOP_HTTP_AUTH  or  DISABLE  for the four options above respectively.  The subsequent sections talk about how security works in Apex. This information is not needed by users but is intended for the inquisitive techical audience who want to know how security works.", 
            "title": "Web Services security"
        }, 
        {
            "location": "/security/#cli-setup", 
            "text": "The CLI program  apex  connects to the web service endpoint of the STRAM for a running application to query for information or to make changes to it. In order to do that, it has to first connect to the YARN proxy web service and get the necessary connection information and credentials to connect to STRAM. The proxy web service may have security enabled and in that case, the CLI program  apex  would first need to authenticate with the service before it can get any information.  Hadoop allows a lot of flexibility in the kind of security to use for the proxy. It allows the user to plug-in their own authentication provider. The authentication provider is specified as a JAVA class name. It also comes bundled with a provider for Kerberos SPNEGO authentication. Some distributions also include a provider for BASIC authentication via SASL.  The CLI  apex , has built-in functionality for Kerberos SPNEGO, BASIC and DIGEST authentication mechanisms. Because of the way the authentication provider is configured for the proxy on the Hadoop side, there is no reliable way to determine before hand what kind of authentication is being used. Only at runtime, when the CLI connects to the proxy web service will it know the type of authentication that the service is using. For this reason,  apex  allows the user to configure credentials for multiple authentication mechanisms it supports and will pick the one that matches what the service expects.  If the authentication mechanism is Kerberos SPNEGO, the properties listed in the  Using Kerberos credentials  section for general communication with Hadoop above are sufficient. No additional properties are needed.  For BASIC authentication, the credentials can be specified using the following properties  property \n         name dt.authentication.basic.username /name \n         value username /value  /property  property \n         name dt.authentication.basic.password /name \n         value password /value  /property   For DIGEST authentication, the credentials can be specified using the following properties  property \n         name dt.authentication.digest.username /name \n         value username /value  /property  property \n         name dt.authentication.digest.password /name \n         value password /value  /property", 
            "title": "CLI setup"
        }, 
        {
            "location": "/security/#token-refresh", 
            "text": "Apex applications, at runtime, use delegation tokens to authenticate with Hadoop services when communicating with them as described in the security architecture section below. The delegation tokens are originally issued by these Hadoop services and have an expiry time period which is typically 7 days. The tokens become invalid beyond this time and the applications will no longer be able to communicate with the Hadoop services. For long running applications this presents a problem.  To solve this problem one of the two approaches can be used. The first approach is to change the Hadoop configuration itself to extend the token expiry time period. This may not be possible in all environments as it requires a change in the security policy as the tokens will now be valid for a longer period of time and the change also requires administrator privileges to Hadoop. The second approach is to use a feature available in apex to auto-refresh the tokens before they expire. Both the approaches are detailed below and the users can choose the one that works best for them.", 
            "title": "Token Refresh"
        }, 
        {
            "location": "/security/#hadoop-configuration-approach", 
            "text": "An Apex application uses delegation tokens to authenticate with Hadoop services, Resource Manager (YARN) and Name Node (HDFS), and these tokens are issued by those services respectively. Since the application is long-running, the tokens can expire while the application is still running. Hadoop uses configuration settings for the maximum lifetime of these tokens.   There are separate settings for ResourceManager and NameNode delegation tokens. In this approach the user increases the values of these settings to cover the lifetime of the application. Once these settings are changed, the YARN and HDFS services would have to be restarted. The values in these settings are of type  long  and has an upper limit so applications cannot run forever. This limitation is not present with the next approach described below.  The Resource Manager delegation token max lifetime is specified in  yarn-site.xml  and can be specified as follows for a lifetime of 1 year as an example  property \n   name yarn.resourcemanager.delegation.token.max-lifetime /name \n   value 31536000000 /value  /property   The Name Node delegation token max lifetime is specified in\nhdfs-site.xml and can be specified as follows for a lifetime of 1 year as an example  property \n    name dfs.namenode.delegation.token.max-lifetime /name \n    value 31536000000 /value \n  /property", 
            "title": "Hadoop configuration approach"
        }, 
        {
            "location": "/security/#auto-refresh-approach", 
            "text": "In this approach the application, in anticipation of a token expiring, obtains a new token to replace the current one. It keeps repeating the process whenever a token is close to expiry so that the application can continue to run indefinitely.  This requires the application having access to a keytab file at runtime because obtaining a new token requires a keytab. The keytab file should be present in HDFS so that the application can access it at runtime. The user can provide a HDFS location for the keytab file using a setting otherwise the keytab file specified for the  apex  CLI program above will be copied from the local filesystem into HDFS before the application is started and made available to the application. There are other optional settings available to configure the behavior of this feature. All the settings are described below.  The location of the keytab can be specified by using the following setting in  dt-site.xml . If it is not specified then the file specified in  dt.authentication.keytab  is copied into HDFS and used.  property \n         name dt.authentication.store.keytab /name \n         value hdfs-path-to-keytab-file /value  /property   The expiry period of the Resource Manager and Name Node tokens needs to be known so that the application can renew them before they expire. These are automatically obtained using the  yarn.resourcemanager.delegation.token.max-lifetime  and  dfs.namenode.delegation.token.max-lifetime  properties from the hadoop configuration files. Sometimes however these properties are not available or kept up-to-date on the nodes running the applications. If that is the case then the following properties can be used to specify the expiry period, the values are in milliseconds. The example below shows how to specify these with values of 7 days.  property \n         name dt.resourcemanager.delegation.token.max-lifetime /name \n         value 604800000 /value  /property  property \n         name dt.namenode.delegation.token.max-lifetime /name \n         value 604800000 /value  /property   As explained earlier new tokens are obtained before the old ones expire. How early the new tokens are obtained before expiry is controlled by a setting. This setting is specified as a factor of the token expiration with a value between 0.0 and 1.0. The default value is  0.7 . This factor is multipled with the expiration time to determine when to refresh the tokens. This setting can be changed by the user and the following example shows how this can be done  property \n         name dt.authentication.token.refresh.factor /name \n         value 0.7 /value  /property", 
            "title": "Auto-refresh approach"
        }, 
        {
            "location": "/security/#impersonation", 
            "text": "The CLI program  apex  supports Hadoop proxy user impersonation, in allowing applications to be launched and other operations to be performed as a different user than the one specified by the Kerberos credentials. The Kerberos credentials are still used for authentication. This is useful in scenarios where a system using  apex  has to support multiple users but only has a single set of Kerberos credentials, those of a system user.", 
            "title": "Impersonation"
        }, 
        {
            "location": "/security/#usage", 
            "text": "To use this feature, the following environment variable should be set to the user name of the user being impersonated, before running  apex  and the operations will be performed as that user. For example, if launching an application, the application will run as the specified user and not as the user specified by the Kerberos credentials.  HADOOP_USER_NAME= username", 
            "title": "Usage"
        }, 
        {
            "location": "/security/#hadoop-configuration", 
            "text": "For this feature to work, additional configuration settings are needed in Hadoop. These settings would allow a specified user, such as a system user, to impersonate other users. The example snippet below shows these settings. In this example, the specified user can impersonate users belonging to any group and can do so running from any host. Note that the user specified here is different from the user specified above in usage, there it is the user that is being impersonated and here it is the impersonating user such as a system user.  property \n   name hadoop.proxyuser. username .groups /name \n   value * /value  /property  property \n   name hadoop.proxyuser. username .hosts /name \n   value * /value  /property", 
            "title": "Hadoop Configuration"
        }, 
        {
            "location": "/security/#application-root-directory-under-hdfs", 
            "text": "A running Apex application uses a 'root' directory under HDFS where runtime artifacts are saved or read from. For example, with default configuration an Apex application \nwould use the HDFS path  /user/dtadmin/datatorrent  as the application root directory. If the application's Hadoop assigned application ID is  application_1487803614053_10222  \nthen the full HDFS path for saving and reading application package contents,  events, checkpoint and recovery data is  /user/dtadmin/datatorrent/apps/application_1487803614053_10222 .  This root directory determination involves using the 'current' user, and in case of impersonation Apex treats either the impersonating user or the impersonated \nuser as the 'current' user depending on the value of the configuration propery  dt.authentication.impersonation.path.enable . You should set its value to  true  to \ntreat the impersonated user as the current user, or  false  (the default value) to treat the impersonating user as the current user.  Apex also uses the configuration property  apex.app.dfsRootDirectory  to determine the application root directory when  dt.authentication.impersonation.path.enable \nis set to  true  i.e. treating the impersonated user as the 'current' user. When  dt.authentication.impersonation.path.enable  is set to  false , Apex will use\nthe configuration property  dt.dfsRootDirectory  to determine the application root directory.  As an example, let's assume the following values:    apex.app.dfsRootDirectory  is set to  /user/%USER_NAME%/apex .    dt.dfsRootDirectory  is set to  /user/%USER_NAME%/datatorrent .    Also assume the impersonating user is  apexadmin  and the impersonated user is  peter  when the application is launched. In this scenario, when dt.authentication.impersonation.path.enable  is set to  true , the application root directory is  /user/peter/apex . When it is set to  false , the application\nroot directory is  /user/apexadmin/datatorrent . As you can see, Apex replaces the string  %USER_NAME%  with the 'current' user as described above. If you do not use\nan absolute path (i.e. path starting with  / ) as the value of  apex.app.dfsRootDirectory  then the evaluated path is prepended with the current user's home directory.\nFor example, when the value of  apex.app.dfsRootDirectory  is set to  apex/%USER_NAME%/myDir  and  dt.authentication.impersonation.path.enable  is set to  true  then\nthe application root directory in the above example is  /user/peter/apex/peter/myDir .  All of the configuration properties mentioned above can be set using any one of the methods mentioned  here .\nFor example, you can use the  dt-site.xml  file, or use the  -D  option in the Apex CLI launch command as described  here .", 
            "title": "Application Root Directory under HDFS"
        }, 
        {
            "location": "/security/#security-architecture", 
            "text": "In this section we will see how security works for applications built on Apex. We will look at the different methodologies involved in running the applications and in each case we will look into the different components that are involved. We will go into the architecture of these components and look at the different security mechanisms that are in play.", 
            "title": "Security architecture"
        }, 
        {
            "location": "/security/#application-launch", 
            "text": "To launch applications in Apache Apex the command line client  apex  can be used. The application artifacts such as binaries and properties are supplied as an application package. The client, during the various steps involved to launch the application needs to communicate with both the Resource Manager and the Name Node. The Resource Manager communication involves the client asking for new resources to run the application master and start the application launch process. The steps along with sample Java code are described in Writing YARN Applications. The Name Node communication includes the application artifacts being copied to HDFS so that they are available across the cluster for launching the different application containers.  In secure mode, the communications with both Resource Manager and Name Node requires authentication and the mechanism is Kerberos. Below is an illustration showing this.   The client  apex  supports Kerberos authentication and will automatically enable it in a secure environment. To authenticate, some Kerberos configuration namely the Kerberos credentials, are needed by the client. There are two parameters, the Kerberos principal and keytab to use for the client. These can be specified in the dt-site.xml configuration file. The properties are shown below       property \n             name dt.authentication.principal /name \n             value kerberos-principal-of-user /value \n     /property \n     property \n             name dt.authentication.keytab /name \n             value absolute-path-to-keytab-file /value \n     /property   Refer to document Operation and Installation Guide section Multi Tenancy and Security subsection CLI Configuration in the documentation for more information. The document can also be accessed here client configuration  There is another important functionality that is performed by the client and that is to retrieve what are called delegation tokens from the Resource Manager and Name Node to seed the application master container that is to be launched. This is detailed in the next section.", 
            "title": "Application Launch"
        }, 
        {
            "location": "/security/#runtime-security", 
            "text": "When the application is completely up and running, there are different components of the application running as separate processes possibly on different nodes in the cluster as it is a distributed application. These components interactwould be interacting with each other and the Hadoop services. In secure mode, all these interactions have to be authenticated before they can be successfully processed. The interactions are illustrated below in a diagram to give a complete overview. Each of them is explained in subsequent sections.", 
            "title": "Runtime Security"
        }, 
        {
            "location": "/security/#stram-and-hadoop", 
            "text": "Every Apache Apex application has a master process akin to any YARN application. In our case it is called STRAM (Streaming Application Master). It is a master process that runs in its own container and manages the different distributed components of the application. Among other tasks it requests Resource Manager for new resources as they are needed and gives back resources that are no longer needed. STRAM also needs to communicate with Name Node from time-to-time to access the persistent HDFS file system.   In secure mode, STRAM has to authenticate with both Resource Manager and Name Node before it can send any requests and this is achieved using Delegation Tokens. Since STRAM runs as a managed application master, it runs in a Hadoop container. This container could have been allocated on any node based on what resources were available. Since there is no fixed node where STRAM runs, it does not have Kerberos credentials. Unlike launch client  apex , it cannot authenticate with Hadoop services Resource Manager and Name Node using Kerberos. Instead, Delegation Tokens are used for authentication.", 
            "title": "STRAM and Hadoop"
        }, 
        {
            "location": "/security/#delegation-tokens", 
            "text": "Delegation tokens are tokens that are dynamically issued by the source and clients use them to authenticate with the source. The source stores the delegation tokens it has issued in a cache and checks the delegation token sent by a client against the cache. If a match is found, the authentication is successful else it fails. This is the second mode of authentication in secure Hadoop after Kerberos. More details can be found in the Hadoop security design document. In this case the delegation tokens are issued by Resource Manager and Name Node. STRAM would use these tokens to authenticate with them. But how does it get them in the first place? This is where the launch client  apex  comes in.  The client  apex , since it possesses Kerberos credentials as explained in the Application Launch section, is able to authenticate with Resource Manager and Name Node using Kerberos. It then requests for delegation tokens over the Kerberos authenticated connection. The servers return the delegation tokens in the response payload. The client in requesting the resource manager for the start of the application master container for STRAM seeds it with these tokens so that when STRAM starts it has these tokens. It can then use these tokens to authenticate with the Hadoop services.", 
            "title": "Delegation Tokens"
        }, 
        {
            "location": "/security/#streaming-container", 
            "text": "A streaming container is a process that runs a part of the application business logic. It is a container deployed on a node in the cluster. The part of business logic is implemented in what we call an operator. Multiple operators connected together make up the complete application and hence there are multiple streaming containers in an application. The streaming containers have different types of communications going on as illustrated in the diagram above. They are described below.", 
            "title": "Streaming Container"
        }, 
        {
            "location": "/security/#stram-delegation-token", 
            "text": "The streaming containers periodically communicate with the application master STRAM. In the communication they send what are called heartbeats with information such as statistics and receive commands from STRAM such as deployment or un-deployment of operators, changing properties of operators etc. In secure mode, this communication cannot just occur without any authentication. To facilitate this authentication special tokens called STRAM Delegation Tokens are used. These tokens are created and managed by STRAM. When a new streaming container is being started, since STRAM is the one negotiating resources from Resource Manager for the container and requesting to start the container, it seeds the container with the STRAM delegation token necessary to communicate with it. Thus, a streaming container has the STRAM delegation token to successfully authenticate and communicate with STRAM.", 
            "title": "STRAM Delegation Token"
        }, 
        {
            "location": "/security/#buffer-server-token", 
            "text": "As mentioned earlier an operator implements a piece of the business logic of the application and multiple operators together complete the application. In creating the application the operators are assembled together in a direct acyclic graph, a pipeline, with output of operators becoming the input for other operators. At runtime the stream containers hosting the operators are connected to each other and sending data to each other. In secure mode these connections should be authenticated too, more importantly than others, as they are involved in transferring application data.  When operators are running there will be effective processing rate differences between them due to intrinsic reasons such as operator logic or external reasons such as different resource availability of CPU, memory, network bandwidth etc. as the operators are running in different containers. To maximize performance and utilization the data flow is handled asynchronous to the regular operator function and a buffer is used to intermediately store the data that is being produced by the operator. This buffered data is served by a buffer server over the network connection to the downstream streaming container containing the operator that is supposed to receive the data from this operator. This connection is secured by a token called the buffer server token. These tokens are also generated and seeded by STRAM when the streaming containers are deployed and started and it uses different tokens for different buffer servers to have better security.", 
            "title": "Buffer Server Token"
        }, 
        {
            "location": "/security/#namenode-delegation-token", 
            "text": "Like STRAM, streaming containers also need to communicate with NameNode to use HDFS persistence for reasons such as saving the state of the operators. In secure mode they also use NameNode delegation tokens for authentication. These tokens are also seeded by STRAM for the streaming containers.", 
            "title": "NameNode Delegation Token"
        }, 
        {
            "location": "/security/#stram-web-services", 
            "text": "Clients connect to STRAM and make web service requests to obtain operational information about running applications. When security is enabled we want this connection to also be authenticated. In this mode the client passes a web service token in the request and STRAM checks this token. If the token is valid, then the request is processed else it is denied.  How does the client get the web service token in the first place? The client will have to first connect to STRAM via the Resource Manager Web Services Proxy which is a service run by Hadoop to proxy requests to application web services. This connection is authenticated by the proxy service using a protocol called SPNEGO when secure mode is enabled. SPNEGO is Kerberos over HTTP and the client also needs to support it. If the authentication is successful the proxy forwards the request to STRAM. STRAM in processing the request generates and sends back a web service token similar to a delegation token. This token is then used by the client in subsequent requests it makes directly to STRAM and STRAM is able to validate it since it generated the token in the first place.", 
            "title": "Stram Web Services"
        }, 
        {
            "location": "/security/#ssl-configuration", 
            "text": "The STRAM Web services component described above can be configured with SSL to enable HTTPS. To achieve this you need to enable SSL in YARN as described  here , specifically set  yarn.http.policy  to  HTTPS_ONLY  in  yarn-site.xml . You also need to make keystore and other SSL material available to the Web services component for HTTPS to work. There are 3 approaches to achieve this:", 
            "title": "SSL Configuration"
        }, 
        {
            "location": "/security/#approach-1-using-default-hadoop-yarn-ssl", 
            "text": "If the default Hadoop-YARN SSL material is uniformly available in the local file system of each node in your Hadoop cluster, STRAM can use it provided the STRAM process has access to the files. Use your Hadoop/YARN configuration procedures to enable SSL in YARN to use this approach, so all YARN applications including Apex applications will be SSL-enabled. However if the SSL material is not present, or not accessible to the STRAM process (for example, if STRAM is not running as  root ), then you will have to use one of the following two approaches.", 
            "title": "Approach 1: Using Default Hadoop-YARN SSL"
        }, 
        {
            "location": "/security/#approach-2-pre-installing-ssl-files-on-hadoop-cluster-nodes", 
            "text": "With this approach, your own SSL files are pre-installed on each node in the Hadoop cluster, so the STRAM can access them regardless of the node it runs on. Each node will have 2 files:  ssl-server.xml  is the master SSL configuration file and the SSL keystore file (a JKS file) whose name and location are specified in  ssl-server.xml . Decide on a location for each of these files and follow the steps below.    Create an SSL keystore as described  here . Assume this file is named  myapex-keystore.jks  and will reside at  /opt/apex/keystore/  on each Hadoop node and the keystore password is  storepass1  and the keystore key-password is  keypass2 .     Create the SSL configuration file with the following content. Typically the file is called  ssl-server.xml  but you can use any other name.    configuration \n   property \n     name ssl.server.keystore.location /name \n     value /opt/apex/keystore/myapex-keystore.jks /value \n     description /description \n   /property \n   property \n     name ssl.server.keystore.keypassword /name \n     value keypass2 /value \n     description /description \n   /property \n   property \n     name ssl.server.keystore.password /name \n     value storepass1 /value \n     description /description \n   /property  /configuration   Let's assume this file will reside on each node as  /opt/apex/sslConfig/my-apex-ssl-server.xml  .    Copy the keystore file and the SSL configuration file on each Hadoop node to their designated locations and make sure they are readable by your Apex application.    Using the Apex CLI, pass the SSL configuration file location to your Apex application using the SSL_CONFIG attribute as follows:    launch -Dapex.attr.SSL_CONFIG= {\\ configPath\\ :\\ /opt/apex/sslConfig/my-apex-ssl-server.xml\\ }    apa file    Alternatively you can use configuration files to supply the value of  apex.attr.SSL_CONFIG  attribute as described  here", 
            "title": "Approach 2: Pre-installing SSL Files on Hadoop Cluster Nodes"
        }, 
        {
            "location": "/security/#approach-3-using-apex-cli-to-copy-the-keystore", 
            "text": "If you cannot pre-install the SSL files on all your cluster nodes, you can use the SSL_CONFIG attribute in such a way that Apex CLI copies the keystore file from the client node to the server node and also passes on the keystore password values to the STRAM. The keystore file needs to be accessible on the Apex CLI machine so Apex CLI can copy it. Let's assume this location is  /opt/apexCli/ssl/myapex-keystore.jks . Use the Apex CLI launch command as follows:  launch -Dapex.attr.SSL_CONFIG= {\\ keyStorePath\\ :\\ /opt/apexCli/ssl/myapex-keystore.jks\\ ,\\ keyStorePassword\\ :\\ storepass1\\ ,\\ keyStoreKeyPassword\\ :\\ keypass2\\ }    apa file   Apec CLI will copy the keystore file  /opt/apexCli/ssl/myapex-keystore.jks  to the destination STRAM node and also pass on the keystore password values to the STRAM. As mentioned above, you can also use configuration files to supply the value of  apex.attr.SSL_CONFIG .", 
            "title": "Approach 3: Using Apex CLI to Copy the Keystore"
        }, 
        {
            "location": "/security/#updating-trust-store-for-the-app-proxy", 
            "text": "You need to ensure that all end-points connecting to the STRAM Web service trust the SSL certificate when SSL is enabled.\nThis is especially true for the  Web Application Proxy  which connects to the STRAM Web service whenever you access the service through the App Master Proxy HTTPS URL. \nIf you use a self-signed or untrusted certificate, you will need to add that certificate to the trust-store used by the RM Web Application Proxy as described  here  and update  ssl-client.xml  to use the trust-store as described  here", 
            "title": "Updating Trust-store for the App Proxy"
        }, 
        {
            "location": "/security/#dependencies", 
            "text": "The use of the attribute  apex.attr.SSL_CONFIG  described in the last 2 approaches is dependent on an  enhancement  made in YARN, which is available in the following versions: \n2.9.0, 2.7.4, 3.0.0-alpha4, 2.8.2.", 
            "title": "Dependencies"
        }, 
        {
            "location": "/compatibility/", 
            "text": "Apache Apex Compatibility\n\n\nPurpose\n\n\nThis document captures the compatibility goals of the Apache Apex project. The different types of compatibility between Apex releases that affect contributors, downstream projects, and end-users are enumerated. For each type of compatibility we:\n\n\n\n\ndescribe the impact on downstream projects or end-users\n\n\nwhere applicable, call out the policy adopted when incompatible changes are permitted.\n\n\n\n\nApache Apex follows \nsemantic versioning\n. Depending on the compatibility type, there may be different tools or mechanisms to ensure compatibility, for example by comparing artifacts during the build process.\n\n\nThe type of change will inform the required target version number. Given a version number MAJOR.MINOR.PATCH, increment the:\n\n\n\n\nMAJOR version when you make incompatible API changes,\n\n\nMINOR version when you add functionality in a backward-compatible manner, and\n\n\nPATCH version when you make backward-compatible bug fixes.\n\n\n\n\nAdditional labels for pre-release and build metadata are available as extensions to the MAJOR.MINOR.PATCH format.\n\n\nThe overall goal is to avoid backward incompatible changes and major release upgrades. Accordingly we attempt to release new features with minor versions that are incremental to the prior release and offer our users a frictionless upgrade path. When planning contributions, please consider compatibility and release road map upfront. Specifically, certain changes that conflict with the versioning may need to be documented in JIRA and deferred until a future major release. \n\n\nCompatibility types\n\n\nJava API\n\n\nPublic API compatibility is required to ensure end-user programs and downstream projects continue to work without modification.\nThe public API consists of:\n\n\n\n\napex-core: all interfaces and classes in \napi\n and \ncommon\n modules\n\n\napex-malhar: all interfaces and classes in all modules except \ndemos\n, \nsamples\n, \nbenchmark\n \n\n\n\n\nInterfaces and classes that are part of the public API and are annotated with \ninterface stability\n are treated according to the rules defined by the annotation.  \n\n\nPolicy\n\n\nChanges to the public API must follow semantic versioning. \nPublic APIs must be deprecated for at least one minor release prior to their removal in a major release.\nThe \njapicmp Maven plugin\n is used to enforce compatibility as part of the Travis pre-commit builds.\n\n\nSemantic compatibility\n\n\nThe behavior of APIs needs to remain consistent over versions, though changes for correctness may result in changes in behavior. Tests and javadocs specify the behavior. Over time, test suites should be expanded to verify compliance with the specification, effectively creating a formal specification for the subset of behaviors that can be easily tested.\n\n\nPolicy\n\n\nThe behavior of existing API cannot be modified as it would break existing user code. There are exceptional circumstances that may justify such changes, in which cases they should be discussed on the mailing list before implementation. Examples are bug fixes related to security issues, data corruption/consistency or to correct an unintended change from previous release that violated semantic compatibility. Such changes should be accompanied by test coverage for the exact behavior.\n\n\nREST API\n\n\nREST API compatibility corresponds to both the URLs and request/response content over the wire. REST APIs are specifically meant for stable use by clients across releases, even major releases. \n\n\nPolicy\n\n\nThe REST API is separately versioned. This is to allow for co-existence of old and new API should there be a need for backward incompatible changes in the future.\n\n\nCommand Line Interface (CLI)\n\n\nThe CLI may be used either directly via the system shell or via shell scripts. Changing the path, removing or renaming command line options, the order of arguments, or the command return code and output break compatibility and may adversely affect users.\n\n\nPolicy\n\n\nCLI commands are to be deprecated (warning when used) in a prior minor release before they are removed or incompatibly modified in a subsequent major release.\n\n\nConfiguration Files\n\n\nConfiguration files are used for engine or application settings. Changes to keys and default values directly affect users and are hard to diagnose (compared to a compile error, for example).\n\n\nPolicy\n\n\nName, location, format, keys of configuration files should be deprecated in a prior minor release and can only be changed in major release. Best effort should be made to support the deprecated behavior for one more major release (not guaranteed). It is also desirable to provide the user with a migration tool.\n\n\nInternal Wire compatibility\n\n\nApex containers internally use RPC communication and netlet for the data flow. The protocols are private and user components are not exposed to it. Apex is a YARN application and automatically deployed. There is currently no situation where containers of different Apex engine versions need to be interoperable. Should such a scenario become relevant in the future, wire compatibility needs to be specified.\n\n\nPolicy\n\n\nN/A\n\n\nInternal File formats\n\n\nApex engine stores data in the file system for recovery and the data is typically obtained from serialization (from Kryo, Java etc.). Changes to internal classes may affect the ability to relaunch an application with upgraded engine code from previous state. This is currently not supported. In the future, the serialization mechanism should guarantee backward compatibility.\n\n\nPolicy\n\n\nCurrently no compatibility guarantee. User to cold-restart application on engine upgrade.\n\n\nJava Classpath\n\n\nApex applications should not bundle Hadoop dependencies or Apex engine dependencies but use the dependencies provided in the target environment to avoid conflicts. The Apex application archetype can be used to generate a compliant project.  \n\n\nPolicy\n\n\nApex engine dependencies can change as per semantic versioning. Following above guidelines automatically maintains the backward compatibility based on semantic versioning of Apex.\n\n\nMaven Build Artifacts\n\n\nDownstream projects reference the Apex engine dependencies and Malhar operator libraries for application development etc. Changes to the packaging (which classes are in which jar), the groupId, artifactId and which artifacts are deployed to Maven central impact upgrades.\n\n\nPolicy\n\n\nThe artifacts that contain the classes that form the public API as specified above cannot change in patch releases and should stay compatible within a major release. Patch releases can change dependencies, but only at the patch level and following semantic versioning.\n\n\nHardware/Software Requirements\n\n\nApex depends on Apache Hadoop. The community intends to support all major Hadoop distros and current versions. Apex currently supports Hadoop 2.6.0 and higher and Java 7 and higher. Apex is written in Java and has been tested on Linux based Hadoop clusters. There are no additional restrictions on the hardware architecture.\n\n\nTo keep up with the latest advances in hardware, operating systems, JVMs, Hadoop and other software, new Apex releases may require higher versions. Upgrading Apex may require upgrading other dependent software components.\n\n\nPolicy\n\n\nThe JVM and Hadoop minimum version requirements are not expected to change outside major releases.", 
            "title": "Compatibility"
        }, 
        {
            "location": "/compatibility/#apache-apex-compatibility", 
            "text": "", 
            "title": "Apache Apex Compatibility"
        }, 
        {
            "location": "/compatibility/#purpose", 
            "text": "This document captures the compatibility goals of the Apache Apex project. The different types of compatibility between Apex releases that affect contributors, downstream projects, and end-users are enumerated. For each type of compatibility we:   describe the impact on downstream projects or end-users  where applicable, call out the policy adopted when incompatible changes are permitted.   Apache Apex follows  semantic versioning . Depending on the compatibility type, there may be different tools or mechanisms to ensure compatibility, for example by comparing artifacts during the build process.  The type of change will inform the required target version number. Given a version number MAJOR.MINOR.PATCH, increment the:   MAJOR version when you make incompatible API changes,  MINOR version when you add functionality in a backward-compatible manner, and  PATCH version when you make backward-compatible bug fixes.   Additional labels for pre-release and build metadata are available as extensions to the MAJOR.MINOR.PATCH format.  The overall goal is to avoid backward incompatible changes and major release upgrades. Accordingly we attempt to release new features with minor versions that are incremental to the prior release and offer our users a frictionless upgrade path. When planning contributions, please consider compatibility and release road map upfront. Specifically, certain changes that conflict with the versioning may need to be documented in JIRA and deferred until a future major release.", 
            "title": "Purpose"
        }, 
        {
            "location": "/compatibility/#compatibility-types", 
            "text": "", 
            "title": "Compatibility types"
        }, 
        {
            "location": "/compatibility/#java-api", 
            "text": "Public API compatibility is required to ensure end-user programs and downstream projects continue to work without modification.\nThe public API consists of:   apex-core: all interfaces and classes in  api  and  common  modules  apex-malhar: all interfaces and classes in all modules except  demos ,  samples ,  benchmark     Interfaces and classes that are part of the public API and are annotated with  interface stability  are treated according to the rules defined by the annotation.    Policy  Changes to the public API must follow semantic versioning. \nPublic APIs must be deprecated for at least one minor release prior to their removal in a major release.\nThe  japicmp Maven plugin  is used to enforce compatibility as part of the Travis pre-commit builds.", 
            "title": "Java API"
        }, 
        {
            "location": "/compatibility/#semantic-compatibility", 
            "text": "The behavior of APIs needs to remain consistent over versions, though changes for correctness may result in changes in behavior. Tests and javadocs specify the behavior. Over time, test suites should be expanded to verify compliance with the specification, effectively creating a formal specification for the subset of behaviors that can be easily tested.  Policy  The behavior of existing API cannot be modified as it would break existing user code. There are exceptional circumstances that may justify such changes, in which cases they should be discussed on the mailing list before implementation. Examples are bug fixes related to security issues, data corruption/consistency or to correct an unintended change from previous release that violated semantic compatibility. Such changes should be accompanied by test coverage for the exact behavior.", 
            "title": "Semantic compatibility"
        }, 
        {
            "location": "/compatibility/#rest-api", 
            "text": "REST API compatibility corresponds to both the URLs and request/response content over the wire. REST APIs are specifically meant for stable use by clients across releases, even major releases.   Policy  The REST API is separately versioned. This is to allow for co-existence of old and new API should there be a need for backward incompatible changes in the future.", 
            "title": "REST API"
        }, 
        {
            "location": "/compatibility/#command-line-interface-cli", 
            "text": "The CLI may be used either directly via the system shell or via shell scripts. Changing the path, removing or renaming command line options, the order of arguments, or the command return code and output break compatibility and may adversely affect users.  Policy  CLI commands are to be deprecated (warning when used) in a prior minor release before they are removed or incompatibly modified in a subsequent major release.", 
            "title": "Command Line Interface (CLI)"
        }, 
        {
            "location": "/compatibility/#configuration-files", 
            "text": "Configuration files are used for engine or application settings. Changes to keys and default values directly affect users and are hard to diagnose (compared to a compile error, for example).  Policy  Name, location, format, keys of configuration files should be deprecated in a prior minor release and can only be changed in major release. Best effort should be made to support the deprecated behavior for one more major release (not guaranteed). It is also desirable to provide the user with a migration tool.", 
            "title": "Configuration Files"
        }, 
        {
            "location": "/compatibility/#internal-wire-compatibility", 
            "text": "Apex containers internally use RPC communication and netlet for the data flow. The protocols are private and user components are not exposed to it. Apex is a YARN application and automatically deployed. There is currently no situation where containers of different Apex engine versions need to be interoperable. Should such a scenario become relevant in the future, wire compatibility needs to be specified.  Policy  N/A", 
            "title": "Internal Wire compatibility"
        }, 
        {
            "location": "/compatibility/#internal-file-formats", 
            "text": "Apex engine stores data in the file system for recovery and the data is typically obtained from serialization (from Kryo, Java etc.). Changes to internal classes may affect the ability to relaunch an application with upgraded engine code from previous state. This is currently not supported. In the future, the serialization mechanism should guarantee backward compatibility.  Policy  Currently no compatibility guarantee. User to cold-restart application on engine upgrade.", 
            "title": "Internal File formats"
        }, 
        {
            "location": "/compatibility/#java-classpath", 
            "text": "Apex applications should not bundle Hadoop dependencies or Apex engine dependencies but use the dependencies provided in the target environment to avoid conflicts. The Apex application archetype can be used to generate a compliant project.    Policy  Apex engine dependencies can change as per semantic versioning. Following above guidelines automatically maintains the backward compatibility based on semantic versioning of Apex.", 
            "title": "Java Classpath"
        }, 
        {
            "location": "/compatibility/#maven-build-artifacts", 
            "text": "Downstream projects reference the Apex engine dependencies and Malhar operator libraries for application development etc. Changes to the packaging (which classes are in which jar), the groupId, artifactId and which artifacts are deployed to Maven central impact upgrades.  Policy  The artifacts that contain the classes that form the public API as specified above cannot change in patch releases and should stay compatible within a major release. Patch releases can change dependencies, but only at the patch level and following semantic versioning.", 
            "title": "Maven Build Artifacts"
        }, 
        {
            "location": "/compatibility/#hardwaresoftware-requirements", 
            "text": "Apex depends on Apache Hadoop. The community intends to support all major Hadoop distros and current versions. Apex currently supports Hadoop 2.6.0 and higher and Java 7 and higher. Apex is written in Java and has been tested on Linux based Hadoop clusters. There are no additional restrictions on the hardware architecture.  To keep up with the latest advances in hardware, operating systems, JVMs, Hadoop and other software, new Apex releases may require higher versions. Upgrading Apex may require upgrading other dependent software components.  Policy  The JVM and Hadoop minimum version requirements are not expected to change outside major releases.", 
            "title": "Hardware/Software Requirements"
        }
    ]
}